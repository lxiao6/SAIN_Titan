<!--
RSS generated by JIRA (7.6.3#76005-sha1:8a4e38d34af948780dbf52044e7aafb13a7cae58) at Mon Jan 21 19:15:27 UTC 2019

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<!-- If you wish to do custom client-side styling of RSS, uncomment this:
<?xml-stylesheet href="https://issues.apache.org/jira/styles/jiraxml2html.xsl" type="text/xsl"?>
-->
<rss version="0.92">
    <channel>
        <title>ASF JIRA</title>
        <link>https://issues.apache.org/jira/issues/?jql=project+%3D+AVRO+AND+created+%3E%3D+2011-4-13+AND+created+%3C%3D+2011-4-20+ORDER+BY+key+ASC</link>
        <description>An XML representation of a search request</description>
                <language>en-uk</language>
                        <issue start="0" end="4" total="4"/>
                <build-info>
            <version>7.6.3</version>
            <build-number>76005</build-number>
            <build-date>09-01-2018</build-date>
        </build-info>

<item>
            <title>[AVRO-803] Java generated Avro classes make using Avro painful and surprising</title>
                <link>https://issues.apache.org/jira/browse/AVRO-803</link>
                <project id="12310911" key="AVRO">Apache Avro</project>
                    <description>&lt;p&gt;Currently the Avro generated Java classes expose CharSequence in their API. However, you cannot use any old CharSequence when interacting with them. In fact, you have to use the Utf8 class if you want to get consistent results. I think that Avro should work with any CharSequence if that is the API. Here is an example where this happens:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/spullara/avro-generated-code/blob/master/src/test/java/AnnoyingTest.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/spullara/avro-generated-code/blob/master/src/test/java/AnnoyingTest.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;That prints out &apos;false&apos; three times unexpectedly. If you can&apos;t get it to print &apos;true&apos; three times then you should probably change it back to Utf8.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Any&lt;/p&gt;</environment>
        <key id="12504206">AVRO-803</key>
            <summary>Java generated Avro classes make using Avro painful and surprising</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21140&amp;avatarType=issuetype">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jadler">Joseph Adler</assignee>
                                    <reporter username="spullara">Sam Pullara</reporter>
                        <labels>
                    </labels>
                <created>Wed, 13 Apr 2011 19:54:53 +0000</created>
                <updated>Wed, 19 Jun 2013 21:56:24 +0000</updated>
                            <resolved>Tue, 11 Oct 2011 23:27:12 +0000</resolved>
                                    <version>1.5.0</version>
                                    <fixVersion>1.6.0</fixVersion>
                                    <component>java</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>8</watches>
                                                                                                            <comments>
                            <comment id="13019525" author="scott_carey" created="Wed, 13 Apr 2011 20:04:43 +0000"  >&lt;p&gt;A co-worker highlighted this and brought this to my attention very recently.  CharSequence&apos;s contract explicity states that hashCode() and equals() are not consistent across implementations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://download.oracle.com/javase/6/docs/api/java/lang/CharSequence.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://download.oracle.com/javase/6/docs/api/java/lang/CharSequence.html&lt;/a&gt;&lt;br/&gt;
And thus a CharSequence can&apos;t be used for a key in a map.&lt;/p&gt;

&lt;p&gt;In Java, we&apos;ll have to use a specific implementation of CharSequence inside any map keys, and can&apos;t generalize on CharSequence in a few places.  The low level Encoder / Decoder can happily use CharSequence in many places, but generated classes and interfaces on the higher layer APIs have to be more strict.&lt;/p&gt;</comment>
                            <comment id="13058031" author="cutting" created="Thu, 30 Jun 2011 19:37:54 +0000"  >&lt;p&gt;I don&apos;t think this would be a compatible change, so should go into 1.6.&lt;/p&gt;</comment>
                            <comment id="13105358" author="gffletch" created="Thu, 15 Sep 2011 13:37:13 +0000"  >&lt;p&gt;We&apos;ve run into this as well and for now, I wrote my own velocity template that makes the data elements private and generates getters/setters (similar to what&apos;s in 1.6.0). In these getters/setters I convert the CharSequence to a String and then do copies for arrays and maps to convert the Utf8 type to a String. This is an ugly work around and has a big performance impact. Are there any plans to fix this in 1.6.0?&lt;/p&gt;</comment>
                            <comment id="13105697" author="cutting" created="Thu, 15 Sep 2011 21:28:19 +0000"  >&lt;p&gt;What&apos;s a concrete proposal?  Should we just switch generated code back to Utf8?&lt;/p&gt;

&lt;p&gt;Or we might:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Use Utf8 for field values, permitting efficient reuse in loops that read data;&lt;/li&gt;
	&lt;li&gt;Provide getter methods that return String.  A Utf8 memoizes its string conversion, so repeated calls to the getter would only allocate a single String.  Applications that wanted to avoid that could use the field directly.&lt;/li&gt;
	&lt;li&gt;Provide setter methods that accept CharSequence.  This would check the runtime type and convert String to Utf8.&lt;/li&gt;
	&lt;li&gt;For lists and maps, use wrappers to adapt w/o copying the entire list.&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;Foo {

  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Utf8 x;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; getX() {
     &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; x == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; ? &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; : x.toString();
  }
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void setX(CharSequence x) {
     &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.x = x &lt;span class=&quot;code-keyword&quot;&gt;instanceof&lt;/span&gt; Utf8 ? (Utf8)x : &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Utf8(x.toString()); 
  }

  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; List&amp;lt;Utf8&amp;gt; values;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; List&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; getValues() {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; AbstractList&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; get(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i) {
         Utf8 value = values.get(i);
         &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; value == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; ? &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; : value.toString();
      }
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; size() { &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; values.size(); }
    };
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void setValues(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; List&amp;lt;? &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; CharSequence&amp;gt; values) {
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.values = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; AbstractList&amp;lt;Utf8&amp;gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Utf8 get(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i) {
         CharSequence value = values.get(i);
         &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (value &lt;span class=&quot;code-keyword&quot;&gt;instanceof&lt;/span&gt; Utf8)
           &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; (Utf8)value;
         &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (value == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;)
           &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
         &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;
           &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Utf8(value.toString());
      }
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; size() { &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; values.size(); }
    };
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="13105816" author="spullara" created="Fri, 16 Sep 2011 02:09:27 +0000"  >&lt;p&gt;This is what I did in my generated classes. I think it is a pretty good solution to the problem and gives developers the option. Will it also work as easily with Maps?&lt;/p&gt;</comment>
                            <comment id="13106814" author="cutting" created="Fri, 16 Sep 2011 21:06:07 +0000"  >&lt;p&gt;A couple more observations:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I&apos;d rather keep the raw fields using CharSequence, so that we can avoid any conversions on write.  The low-level output routines can handle either Utf8 or String, so we should pass along whichever the application prefers to create.&lt;/li&gt;
	&lt;li&gt;Map keys and values are not currently reused, so there&apos;s little point to using Utf8 there.  So we might just always use String in maps, especially as keys to avoid comparison problems.&lt;/li&gt;
	&lt;li&gt;It is possible to wrap a map and convert keys and/or values.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;ve attached code that illustrates how this might look.  This assumes that map keys are now always String but that values can be any CharSequence.&lt;/p&gt;</comment>
                            <comment id="13108077" author="scott_carey" created="Mon, 19 Sep 2011 19:21:54 +0000"  >&lt;p&gt;I like the idea of forcing String for map keys by default.  There is less value to using Utf8 there since Utf8 is mutable.  Plus, Strings in the JVM are heavily optimized for use as keys in maps.  &lt;/p&gt;

&lt;p&gt;I believe that users will sometimes want to force their value types to any of the three choices:  String, Utf8, and CharSequence.  We should have reasonable defaults, but make it easier for users to choose what they want for their use case.  For many use cases, direct conversion to Strings is better.  For others, using Utf8 to be as lazy as possible in the expensive Utf8 &amp;lt;-&amp;gt; Utf16 conversion is.&lt;/p&gt;


&lt;p&gt;Some extra laziness in Utf8 can help in a few places as well, by making the cost of creating a Utf8 from a String cheaper.&lt;br/&gt;
Right now, Utf8 when called with a String parameter in the constructor is not lazy, and generates the utf8 byte[].  It could leave this null, and only lazily create the byte[] if needed, just like it lazily creates the String only if needed.&lt;/p&gt;</comment>
                            <comment id="13122154" author="cutting" created="Thu, 6 Oct 2011 18:49:20 +0000"  >&lt;p&gt;Here&apos;s a new proposal:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;add a new Decoder method, &apos;String readString()&apos; implemented to avoid allocating new intermediate byte arrays for each call as is currently done when Utf8&apos;s are not reused.&lt;/li&gt;
	&lt;li&gt;change generated specific code to optionally use String everywhere instead of CharSequence.  (We could also add an option to emit Utf8 everywhere.)  When String is used we add a property to the string schemas in the generated code so they become 
{&quot;type&quot;:&quot;string&quot;, &quot;java&quot;:&quot;String&quot;}
&lt;p&gt;.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;GenericData#readString() would call the new Decoder method when &quot;java&quot;:&quot;String&quot; is present in the String&apos;s schema.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is totally back-compatible.&lt;/p&gt;</comment>
                            <comment id="13122222" author="scott_carey" created="Thu, 6 Oct 2011 20:11:25 +0000"  >&lt;p&gt;Sounds good to me.&lt;/p&gt;

&lt;p&gt;One thing though:  &quot;java&quot;:&quot;String&quot;&lt;/p&gt;

&lt;p&gt;I think we may want to use the avro reserved namespace on this property to avoid collisions.  Perhaps  &quot;avro.java.stringImpl&quot;:&quot;String&quot; ?&lt;/p&gt;</comment>
                            <comment id="13122281" author="cutting" created="Thu, 6 Oct 2011 20:50:01 +0000"  >&lt;p&gt;&amp;gt; Perhaps &quot;avro.java.stringImpl&quot;:&quot;String&quot;?&lt;/p&gt;

&lt;p&gt;It should really be &quot;avro.java.string.class.name&quot;:&quot;java.lang.String&quot;, but that seems way too verbose.  Maybe I went too far in my brevity, though.  How about &quot;avro.java.string&quot;:&quot;String&quot;?&lt;/p&gt;</comment>
                            <comment id="13122978" author="cutting" created="Fri, 7 Oct 2011 17:33:07 +0000"  >&lt;p&gt;Here&apos;s a patch that implements the above proposal.&lt;/p&gt;

&lt;p&gt;Applications need only add &amp;lt;stringType&amp;gt;String&amp;lt;/stringType&amp;gt; to the configuration of avro-maven-plugin in their pom.xml to switch to using String in their generated code.&lt;/p&gt;</comment>
                            <comment id="13123098" author="cutting" created="Fri, 7 Oct 2011 19:26:49 +0000"  >&lt;p&gt;New version of patch with some cleanups.  Also updates mapred, protobuf &amp;amp; thrift code to take advantage of these changes.&lt;/p&gt;</comment>
                            <comment id="13123099" author="cutting" created="Fri, 7 Oct 2011 19:27:31 +0000"  >&lt;p&gt;I&apos;ll commit this soon unless there are objections.&lt;/p&gt;</comment>
                            <comment id="13123893" author="scott_carey" created="Mon, 10 Oct 2011 04:51:21 +0000"  >&lt;p&gt;We broke back-compatibility in Avro 1.5 &amp;#8211; formerly we used Utf8 and made maps with Utf8 keys.  I think we can break back compatibility in 1.6.0 again.  And we probably should for Maps since it is not appropriate to ever have a map with a CharSequence key; hashCode and equals are not compatible between implementations.&lt;/p&gt;
</comment>
                            <comment id="13124299" author="cutting" created="Mon, 10 Oct 2011 17:06:37 +0000"  >&lt;p&gt;I&apos;d prefer not to break back-compatibility this time.  It makes it impossible for folks to upgrade one project without making source code changes to other projects.  If you specify &amp;lt;stringType&amp;gt;String&amp;lt;/stringType&amp;gt; in your pom.xml then all your Map keys become java.lang.String.&lt;/p&gt;</comment>
                            <comment id="13125477" author="cutting" created="Tue, 11 Oct 2011 23:27:12 +0000"  >&lt;p&gt;I committed this.&lt;/p&gt;

&lt;p&gt;Scott, I hope you&apos;re okay with the default.  If you and/or others feel strongly that we should break compatibility for this, we might consider changing that default, but I wanted to get this committed before it became stale, as it touches a lot of files.&lt;/p&gt;</comment>
                            <comment id="13125569" author="scott_carey" created="Wed, 12 Oct 2011 03:38:08 +0000"  >&lt;p&gt;I&apos;m fine with this.  Any discussion about changing the default can wait and would likely require more user input now that we have many more users.  Keeping it the way it is now, with a work-around and some documentation / examples for best practices is a lot better than where we are in 1.5.x.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12524783">AVRO-897</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310051">
                    <name>Supercedes</name>
                                                                <inwardlinks description="is superceded by">
                                        <issuelink>
            <issuekey id="12525984">AVRO-911</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12498230" name="AVRO-803.patch" size="55475" author="cutting" created="Fri, 7 Oct 2011 19:26:49 +0000"/>
                            <attachment id="12498194" name="AVRO-803.patch" size="46202" author="cutting" created="Fri, 7 Oct 2011 17:33:07 +0000"/>
                            <attachment id="12494865" name="Foo.java" size="4019" author="cutting" created="Fri, 16 Sep 2011 21:06:07 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12525967">AVRO-909</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 13 Apr 2011 20:04:43 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>49314</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 15 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0e2k7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>80177</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[AVRO-804] Java: add ThriftDatumWriter and ThriftDatumReader</title>
                <link>https://issues.apache.org/jira/browse/AVRO-804</link>
                <project id="12310911" key="AVRO">Apache Avro</project>
                    <description>&lt;p&gt;Add a ThriftDatumWriter that accepts Thrift-generated classes and writes them in Avro format, and a ThriftDatumReader that reads Avro data into a Thrift-generated class.  This would permit storage of Thrift-generated data structures in Avro data files.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12504729">AVRO-804</key>
            <summary>Java: add ThriftDatumWriter and ThriftDatumReader</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21141&amp;avatarType=issuetype">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cutting">Doug Cutting</assignee>
                                    <reporter username="cutting">Doug Cutting</reporter>
                        <labels>
                    </labels>
                <created>Tue, 19 Apr 2011 23:02:50 +0000</created>
                <updated>Tue, 1 Nov 2011 18:36:05 +0000</updated>
                            <resolved>Tue, 4 Oct 2011 21:54:56 +0000</resolved>
                                                    <fixVersion>1.6.0</fixVersion>
                                    <component>java</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                <comments>
                            <comment id="13021849" author="cutting" created="Tue, 19 Apr 2011 23:04:58 +0000"  >&lt;p&gt;Thrift-generated classes extend TBase, which permits efficient getting and setting of field values.  The list of Thrift fields can be obtained from static variables in the generated class. &lt;/p&gt;</comment>
                            <comment id="13022189" author="rangadi" created="Wed, 20 Apr 2011 16:18:33 +0000"  >&lt;p&gt;elephant-bird does Thrift &amp;lt;-&amp;gt; PIG tuple/schema (ThriftToPig.java etc in &lt;a href=&quot;https://github.com/kevinweil/elephant-bird&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/kevinweil/elephant-bird&lt;/a&gt;). We can reuse quite a bit of code here. I can implement it if it seems like a good idea. at the same time, I will get more familiar with Avro.   &lt;/p&gt;</comment>
                            <comment id="13022366" author="cutting" created="Wed, 20 Apr 2011 20:54:24 +0000"  >&lt;p&gt;Raghu, that&apos;d be great if you wanted to take a stab at this.  I&apos;d be happy to collaborate.&lt;/p&gt;

&lt;p&gt;Some initial thoughts:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;We can cache the Class&amp;lt;extends TBase&amp;gt; -&amp;gt; Schema mapping in a static ConcurrentHashMap.&lt;/li&gt;
	&lt;li&gt;We might use schema properties for types that don&apos;t map exactly, e.g., Thrift&apos;s i16 type could be mapped to the Avro schema 
{&quot;type&quot;:&quot;int&quot;, &quot;thrift&quot;:&quot;i16&quot;}
&lt;p&gt;.  Dunno if that&apos;ll be required, though.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;Thrift field numbers can be saved as Avro schema field properties, so that, when reading into a Thrift-generated class we&apos;d set values by number, not name, since that&apos;s Thrift semantics.&lt;/li&gt;
	&lt;li&gt;The implementation should subclass GenericData, GenericDatumReader &amp;amp; Writer, overriding methods like getField &amp;amp; setField.  Look at the specific and reflect subclasses of these for examples.&lt;/li&gt;
	&lt;li&gt;ThriftDatumReader should probably have an Schema -&amp;gt; int[] cache for record schemas, where these arrays map from Avro field positions (as passed to setField) to Thrift field numbers, passed to TBase#setFieldValue().&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13022585" author="rangadi" created="Thu, 21 Apr 2011 00:54:41 +0000"  >&lt;p&gt;Thanks Doug. &lt;br/&gt;
I will work on an implementation. Please feel free to assign this to me.&lt;/p&gt;</comment>
                            <comment id="13023269" author="cutting" created="Fri, 22 Apr 2011 16:37:28 +0000"  >&lt;p&gt;Thanks, Raghu, for taking this on.&lt;/p&gt;</comment>
                            <comment id="13031003" author="rangadi" created="Tue, 10 May 2011 04:17:28 +0000"  >&lt;p&gt;sorry for the delay. will work on the jira this week.&lt;/p&gt;</comment>
                            <comment id="13052518" author="cutting" created="Tue, 21 Jun 2011 12:35:29 +0000"  >&lt;p&gt;Raghu, are you going to have time to work on this?  If not, I may have some soon.  Or I could work on &lt;a href=&quot;https://issues.apache.org/jira/browse/AVRO-805&quot; title=&quot;Java: add ProtobufDatumReader and ProtobufDatumWriter&quot; class=&quot;issue-link&quot; data-issue-key=&quot;AVRO-805&quot;&gt;&lt;del&gt;AVRO-805&lt;/del&gt;&lt;/a&gt;...&lt;/p&gt;</comment>
                            <comment id="13052795" author="rangadi" created="Tue, 21 Jun 2011 20:06:14 +0000"  >&lt;p&gt;surely going to make time over next couple of days. I will update here soon. Thanks Doug. &lt;/p&gt;</comment>
                            <comment id="13117710" author="cutting" created="Thu, 29 Sep 2011 22:58:07 +0000"  >&lt;p&gt;Raghu, I had some time, so I started working on this.  Here&apos;s an early version.&lt;/p&gt;

&lt;p&gt;I include the generated Thrift test files in svn so that tests can still be run by folks who don&apos;t have the Thrift compiler installed.&lt;/p&gt;

&lt;p&gt;This compiles but does not yet have tests.  It&apos;s a work in progress.  I&apos;ll add some tests and start debugging it over the next few days, hopefully committing it early next week.&lt;/p&gt;</comment>
                            <comment id="13119651" author="rangadi" created="Mon, 3 Oct 2011 21:48:24 +0000"  >&lt;p&gt;Thanks Doug. ThriftData implementation looks simpler than I thought. I am going to play with this code and write tests similar to Protobuf.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for Thrift 0.5 support we might need to handle couple of quirks (absence of isBuffer() etc). I can add those if required.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&amp;gt; return (Map&amp;lt;TFieldIdEnum,FieldMetaData&amp;gt;)c.getField(&quot;metaDataMap&quot;).get(null);&lt;br/&gt;
  FieldMetaData.getStructMetaDataMap(c) returns this map.&lt;/p&gt;</comment>
                            <comment id="13119682" author="cutting" created="Mon, 3 Oct 2011 22:13:37 +0000"  >&lt;p&gt;Here&apos;s a version with tests that pass.&lt;/p&gt;

&lt;p&gt;I&apos;ll commit this soon if there are no objections.&lt;/p&gt;</comment>
                            <comment id="13119686" author="cutting" created="Mon, 3 Oct 2011 22:15:53 +0000"  >&lt;p&gt;Sorry, Raghu, I missed your comment above.&lt;/p&gt;

&lt;p&gt;Yes, please test this against earlier versions of Thrift if you can, as I have only tested it against 0.7.0 and may rely on features of that version.&lt;/p&gt;</comment>
                            <comment id="13119689" author="cutting" created="Mon, 3 Oct 2011 22:21:27 +0000"  >&lt;p&gt;Updated to use FieldMetaData.getStructMetaDataMap as suggested by Raghu.  Thanks!&lt;/p&gt;

&lt;p&gt;The reflection stuff in getFieldIds() is ugly too and could probably be replaced by something that calls this too.&lt;/p&gt;</comment>
                            <comment id="13120314" author="cutting" created="Tue, 4 Oct 2011 17:44:13 +0000"  >&lt;p&gt;Cleaned up the code in getFieldIds to no longer use reflection.&lt;/p&gt;</comment>
                            <comment id="13120355" author="cutting" created="Tue, 4 Oct 2011 18:21:47 +0000"  >&lt;p&gt;Here&apos;s a version that includes a package.html for the javadoc.&lt;/p&gt;</comment>
                            <comment id="13120398" author="rangadi" created="Tue, 4 Oct 2011 19:22:45 +0000"  >&lt;p&gt;+1. The test directory has only thrift and thrift generated files. Not sure if you missed a test file.&lt;/p&gt;

&lt;p&gt;I haven&apos;t managed to my OS X build going yet. I am going to try on ubuntu. will file a follow up jira for Thrift 5, i think it will be a minor patch.&lt;/p&gt;

&lt;p&gt;Thanks Doug.&lt;/p&gt;</comment>
                            <comment id="13120426" author="cutting" created="Tue, 4 Oct 2011 20:04:17 +0000"  >&lt;p&gt;Oops.  I forgot to include TestThrift.java.  Thanks for catching that!  Here&apos;s a new version of the patch that includes the test.&lt;/p&gt;</comment>
                            <comment id="13120478" author="rangadi" created="Tue, 4 Oct 2011 21:11:33 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="13120510" author="cutting" created="Tue, 4 Oct 2011 21:54:56 +0000"  >&lt;p&gt;I committed this.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12504730">AVRO-805</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12497690" name="AVRO-804.patch" size="147444" author="cutting" created="Tue, 4 Oct 2011 20:04:16 +0000"/>
                            <attachment id="12497675" name="AVRO-804.patch" size="144595" author="cutting" created="Tue, 4 Oct 2011 18:21:47 +0000"/>
                            <attachment id="12497667" name="AVRO-804.patch" size="142409" author="cutting" created="Tue, 4 Oct 2011 17:44:13 +0000"/>
                            <attachment id="12497556" name="AVRO-804.patch" size="142483" author="cutting" created="Mon, 3 Oct 2011 22:21:27 +0000"/>
                            <attachment id="12497553" name="AVRO-804.patch" size="142712" author="cutting" created="Mon, 3 Oct 2011 22:13:37 +0000"/>
                            <attachment id="12497085" name="AVRO-804.patch" size="136355" author="cutting" created="Thu, 29 Sep 2011 22:58:07 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 20 Apr 2011 16:18:33 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>40611</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 16 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0e2kf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>80178</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[AVRO-805] Java: add ProtobufDatumReader and ProtobufDatumWriter</title>
                <link>https://issues.apache.org/jira/browse/AVRO-805</link>
                <project id="12310911" key="AVRO">Apache Avro</project>
                    <description>&lt;p&gt;Add a ProtobufDatumWriter that accepts ProtocolBuffer-generated classes and writes them in Avro format, and a ProtobufDatumReader that reads Avro data into a Protobuf-generated class. This would permit storage of Protobuf-generated data structures in Avro data files.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12504730">AVRO-805</key>
            <summary>Java: add ProtobufDatumReader and ProtobufDatumWriter</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21141&amp;avatarType=issuetype">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cutting">Doug Cutting</assignee>
                                    <reporter username="cutting">Doug Cutting</reporter>
                        <labels>
                    </labels>
                <created>Tue, 19 Apr 2011 23:09:40 +0000</created>
                <updated>Tue, 1 Nov 2011 18:36:00 +0000</updated>
                            <resolved>Mon, 12 Sep 2011 20:24:25 +0000</resolved>
                                                    <fixVersion>1.6.0</fixVersion>
                                    <component>java</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                <comments>
                            <comment id="13021852" author="cutting" created="Tue, 19 Apr 2011 23:13:39 +0000"  >&lt;p&gt;Protobuf-generated classes extend Message, which permits efficient access to field values.  The list of Protobuf fields can also be obtained through Protobuf&apos;s Message interface, permitting construction of an equivalent Avro schema.&lt;/p&gt;</comment>
                            <comment id="13060881" author="cutting" created="Wed, 6 Jul 2011 22:26:29 +0000"  >&lt;p&gt;Here&apos;s an initial implementation of this.  It still doesn&apos;t have full support &amp;amp; tests for enums, repeated fields, etc.&lt;/p&gt;

&lt;p&gt;Probably all protobuf &apos;optional&apos; fields should be interpreted as unions with null.&lt;/p&gt;</comment>
                            <comment id="13071425" author="cutting" created="Tue, 26 Jul 2011 23:25:50 +0000"  >&lt;p&gt;Here&apos;s an updated version that handles default values and enums.  Still need to add support for repeated fields, plus probably cache a few things for decent performance.&lt;/p&gt;</comment>
                            <comment id="13101615" author="cutting" created="Fri, 9 Sep 2011 22:45:18 +0000"  >&lt;p&gt;Here&apos;s a version that handles repeated fields and that caches some expensive operations.&lt;/p&gt;</comment>
                            <comment id="13101617" author="cutting" created="Fri, 9 Sep 2011 22:46:01 +0000"  >&lt;p&gt;I think this is ready to commit.&lt;/p&gt;</comment>
                            <comment id="13103002" author="cutting" created="Mon, 12 Sep 2011 20:24:25 +0000"  >&lt;p&gt;I committed this.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12504729">AVRO-804</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12493863" name="AVRO-805.patch" size="29821" author="cutting" created="Fri, 9 Sep 2011 22:45:18 +0000"/>
                            <attachment id="12487914" name="AVRO-805.patch" size="28769" author="cutting" created="Tue, 26 Jul 2011 23:25:50 +0000"/>
                            <attachment id="12485497" name="AVRO-805.patch" size="25706" author="cutting" created="Wed, 6 Jul 2011 22:26:29 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>19192</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 19 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0e2kn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>80179</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[AVRO-806] add a column-major file format</title>
                <link>https://issues.apache.org/jira/browse/AVRO-806</link>
                <project id="12310911" key="AVRO">Apache Avro</project>
                    <description>&lt;p&gt;Define a column-major file format.  This would permit better compression and also permit efficient skipping of fields that are not of interest.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12504731">AVRO-806</key>
            <summary>add a column-major file format</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21141&amp;avatarType=issuetype">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cutting">Doug Cutting</assignee>
                                    <reporter username="cutting">Doug Cutting</reporter>
                        <labels>
                    </labels>
                <created>Tue, 19 Apr 2011 23:24:21 +0000</created>
                <updated>Thu, 2 May 2013 02:30:56 +0000</updated>
                            <resolved>Tue, 11 Sep 2012 21:41:11 +0000</resolved>
                                                    <fixVersion>1.7.2</fixVersion>
                                    <component>java</component>
                    <component>spec</component>
                        <due></due>
                            <votes>6</votes>
                                    <watches>33</watches>
                                                                <comments>
                            <comment id="13021858" author="cutting" created="Tue, 19 Apr 2011 23:45:26 +0000"  >&lt;p&gt;The writer would keep a buffer per field.  As records are added, each field would be encoded to its respective buffer.  When the total amount of buffered data reaches the desired block size, all column buffers would be flushed, preceded by an index listing the (compressed) sizes of each of column buffers.&lt;/p&gt;

&lt;p&gt;Each buffer would be compressed prior to writing, probably with Snappy.&lt;/p&gt;

&lt;p&gt;The reader would have a decoder for each field.  To skip fields not of interest, the application would specify a subset of the schema written.  The reader would only decompress and process fields present in the schema provided by the application.&lt;/p&gt;</comment>
                            <comment id="13022217" author="scott_carey" created="Wed, 20 Apr 2011 16:54:06 +0000"  >&lt;p&gt;A useful reference is the Data model and Nested columar storage section in Google&apos;s Dremel paper:  &lt;a href=&quot;http://www.google.com/research/pubs/pub36632.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.google.com/research/pubs/pub36632.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With Avro, one challenge of columnar storage is going to be Unions.&lt;/p&gt;

&lt;p&gt;If a record is &lt;/p&gt;

&lt;p&gt;{&lt;br/&gt;
int,&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;float, string&amp;#93;&lt;/span&gt;,&lt;br/&gt;
long&lt;br/&gt;
}&lt;/p&gt;


&lt;p&gt;How do you deal with the unions?   Do you put the whole union in the same columnar chunk? Do you split the union into chunks for each branch, and store the union index in its own chunk?&lt;/p&gt;

&lt;p&gt;How this plays with records nested within unions or unions inside of arrays can be complicated too.&lt;/p&gt;
</comment>
                            <comment id="13022253" author="dcreager" created="Wed, 20 Apr 2011 17:44:34 +0000"  >&lt;p&gt;Avro&apos;s formalism is a bit different than protobuf&apos;s, so we wouldn&apos;t be able to use Dremel&apos;s data model as-is.  But it&apos;s definitely a good start.&lt;/p&gt;

&lt;p&gt;My hunch is that we&apos;d want each columnar chunk to only store primitive values, so we&apos;d have to break down a complex type into each primitive pieces.  For most schemas, there&apos;s a unique name that identifies each primitive piece.  So if you have&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;record Test {
  &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;  a;
  union {&lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt;, string}  b;
  &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;  c;
  array&amp;lt;&lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt;&amp;gt;  d;
  map&amp;lt;string&amp;gt;  e;
  record Subtest {
    &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; f;
    bytes g;
  } sub1;
  union {&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, Subtest}  sub2;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then your primitive names would be&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;a
b.&lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt;
b.string
c
d.[]
e.{}
sub1.f
sub1.g
sub2.&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
sub2.Subtest.f
sub2.Subtest.g
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are only four compound types in Avro:  For records, there&apos;s a child for each field.  For arrays, there&apos;s a single child, which I called [].  For maps, it&apos;s {}.  For unions, there&apos;s a child for each branch, with the same name as the branch&apos;s schema.&lt;/p&gt;

&lt;p&gt;The only issue is with recursive types, since for those, the nesting can be arbitrarily deep at runtime.  We can probably add a &#8220;nesting level&#8221; to handle these, but we&apos;ll have to hammer out the details.&lt;/p&gt;</comment>
                            <comment id="13022255" author="dcreager" created="Wed, 20 Apr 2011 17:46:29 +0000"  >&lt;p&gt;And we could handle the union index in the same way as we&apos;d handle array and map counts.  Each branch of the union can be seen as a container that can only have 0 or 1 elements.  Since we&apos;ll already need some kind of repetition count to keep track of how many elements are in an array type, we can use the same mechanism to keep track of how many elements are in each union branch.&lt;/p&gt;</comment>
                            <comment id="13022298" author="cutting" created="Wed, 20 Apr 2011 18:55:40 +0000"  >&lt;p&gt;I was thinking of just creating columns for the fields of the fields of the top-level record.  In this approach, a union would be written as a union, prefixed with a varint indicating the branch taken.&lt;/p&gt;

&lt;p&gt;If we stored union branches separately then we&apos;d also need a column that has the varint.  Iterators would then use this to decide when a column has a value.  For nested unions I think the iterators would need to have a list of pointers to varints.&lt;/p&gt;

&lt;p&gt;The use case is to accelerate scans of a subset of fields.  Further acceleration is possible if things are columnized more deeply, but we probably want to stop at some fixed depth in each block regardless.  So I&apos;m effectively proposing a depth of 1.  Increasing the depth increases the number of buffer pointers and the complexity of row iteration.  I don&apos;t have a clear sense of when that becomes significant.  One way to limit the depth would be to specify a maximum number of columns, and use a breadth-first walk of the schema until that number of columns are encountered.  However I wonder whether we&apos;re over-engineering this.&lt;/p&gt;</comment>
                            <comment id="13022375" author="tucu00" created="Wed, 20 Apr 2011 21:24:26 +0000"  >&lt;p&gt;Could we god simple at first? I.e., single column for unions and columns only for the elements of depth 1 as a start.&lt;/p&gt;

&lt;p&gt;We could add a &apos;column encoding&apos; version somewhere. This would allow as to change things and still being able to provide backwards compatibility.&lt;/p&gt;</comment>
                            <comment id="13022379" author="dvryaboy" created="Wed, 20 Apr 2011 21:34:42 +0000"  >&lt;p&gt;Not having elements of depth &amp;gt; 1 would basically eliminate all of our schemas from using this.&lt;/p&gt;

&lt;p&gt;Are there enough people out there who would be able to weather this constraint for the simpler version to be effectively tested before complications are added in? &lt;/p&gt;

&lt;p&gt;Seems like since in this case we know what the expected complications will be, it&apos;s better to design with them in mind from the start and avoid surprises later.&lt;/p&gt;</comment>
                            <comment id="13022395" author="cutting" created="Wed, 20 Apr 2011 22:07:57 +0000"  >&lt;p&gt;The question is not whether the elements of depth &amp;gt; 1 are included, but whether they&apos;re each stored in a distinct column.  Regardless, one will read the data file in the same way, using a schema with a subset of the fields, even if you&apos;re not using the column-major codec at all.  So if you have a query that scans only field x.y.z, then storing values for x.y in a column will still make things faster than a row-order, but perhaps not as fast as if x.y.z values were stored in their own column, especially if y has a lot of other fields.  Note that Avro&apos;s already fast at skipping string and binary values that are not desired: it reads the length and increments the buffer pointer.  So column-major will provide the biggest speedup for structures that have a lot of numeric fields that are often ignored queries. &lt;/p&gt;</comment>
                            <comment id="13022593" author="thiru_mg" created="Thu, 21 Apr 2011 02:01:38 +0000"  >&lt;p&gt;It will be nice if we can make the column datafile compatible with the current datafile standard (with the column&apos;s schema, of course). There will be a &quot;meta&quot; datafile as a catalog of its constituent column data files. A single datafile is, by definition, its own catalog. This approach, which I suppose is not incompatible with the original proposal, has some advantages:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;We can reuse almost all the current code.&lt;/li&gt;
	&lt;li&gt;Whether to split columns into further columns is completely left the writer as both are valid.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;However, this approach will make splitting unions harder. We have to cater to &quot;holes&quot; in the branch columns. With a little bit of space overhead, we can address the problem: &lt;span class=&quot;error&quot;&gt;&amp;#91;A, B, C&amp;#93;&lt;/span&gt; will be split into &lt;span class=&quot;error&quot;&gt;&amp;#91;null, A&amp;#93;&lt;/span&gt;, &lt;span class=&quot;error&quot;&gt;&amp;#91;null, B&amp;#93;&lt;/span&gt; and &lt;span class=&quot;error&quot;&gt;&amp;#91;null, C&amp;#93;&lt;/span&gt;. (A, B, or C is null, there won&apos;t be a corresponding column datafile). The &quot;holes&quot; in the column will then be encoded as null branch. We do not need a separate branch-index column.&lt;/p&gt;

&lt;p&gt;An array of record &lt;/p&gt;
{ A, B, C }
&lt;p&gt; can be decomposed as three columns: array of A, array of B and and array C. The idea can be extended to maps.&lt;/p&gt;

&lt;p&gt;The good thing about this is that this defines a recursive column splitting algorithm. And it is flexible. For example, if for a record &lt;/p&gt;
{A, B, C}
&lt;p&gt; B and C always come together, we can split it as A and &lt;/p&gt;
{B, C}
&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;On another point, while it may be a good strategy for the writer to write column blocks of data in lock-step fashion, the reader should not rely on it. This allows different columns to be written at different points in time. This will perform better because different columns have different sizes and compress differently. But with this, skipping to next sync on one column could lead to skipping to the middle of another column. That could be challenging.&lt;/p&gt;</comment>
                            <comment id="13022911" author="cutting" created="Thu, 21 Apr 2011 20:46:47 +0000"  >&lt;p&gt;Thiru, so you&apos;re proposing multiple, parallel files for columns?  I&apos;m proposing a single file whose format is as it is today except for the encoding of blocks of records, which would use a new codec: in addition to the current &quot;null&quot;, &quot;gzip&quot; and &quot;snappy&quot; we&apos;d add a &quot;column&quot; codec.  As you note, existing implementations would not be able to read this until they&apos;ve implemented this codec, while, with multiple files, they would.  However I&apos;m not sure that folks would appreciate the increase in the number of files that parallel files would create.&lt;/p&gt;

&lt;p&gt;Note that this codec could be implemented using the existing compression codec API: it could accept a buffer of serialized records, then parse the records using the file&apos;s schema, splitting their fields into separate buffers, and finally appending all of these buffers with an index at the front.  This can be optimized to avoid the extra copy of data.&lt;/p&gt;</comment>
                            <comment id="13023445" author="cutting" created="Fri, 22 Apr 2011 22:57:10 +0000"  >&lt;p&gt;This is a work in progress.&lt;/p&gt;

&lt;p&gt;I believe the output is correct and complete, but the input side is not right yet so I can&apos;t test it.  To read the column format I need ResolvingDecoder to call three new Decoder methods on the nested Decoder:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;startRecord() at the beginning of each record&lt;/li&gt;
	&lt;li&gt;startField() at the beginning of each field&lt;/li&gt;
	&lt;li&gt;endRecord() at the end of each field.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;ve made some changes to ResolvingDecoder attempting to do this, but they don&apos;t work and I don&apos;t understand it well enough to make this work.  Thiru, can you please help me here?&lt;/p&gt;

&lt;p&gt;That would get the input side to work, but it wouldn&apos;t yet be much faster when columns are elided from the schema.  To make it fast we also need to change ResolvingDecoder to take advantage of the new Decoder#skipField() method.&lt;/p&gt;</comment>
                            <comment id="13023453" author="cutting" created="Fri, 22 Apr 2011 23:09:14 +0000"  >&lt;p&gt;A breif description of my patch:&lt;/p&gt;

&lt;p&gt;I&apos;ve added methods to Codec to create the Encoder and Decoder used for the file.  Then I&apos;ve added methods to Encoder and Decoder called as we start to read each record, as we start to read each field, and as we finish reading each field.  These are used by the ColumnEncoder and ColumnDecoder implementations to track the depth and column and then switch input or output to the correct buffer as objects are written and read.&lt;/p&gt;

&lt;p&gt;I hope this makes sense! &lt;/p&gt;</comment>
                            <comment id="13050510" author="thiru_mg" created="Thu, 16 Jun 2011 15:53:07 +0000"  >&lt;p&gt;Here is a patch which implements the same idea, more formally. Please see the attached file that Raymie Stata wrote about the approach we are proposing. Some comments on the patch:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The implementation basically works. Further optimizations are possible. One pending work is the optimization to avoid looking into columns that are needed. At present the traditional schema resolution works, but if the reader&apos;s and writer&apos;s schemas are such that certain column can be completely skipped, they are not skipped.&lt;/li&gt;
	&lt;li&gt;Data file writer uses a trick that in case of exceptions, it flushes things up to the previous record. That trick won&apos;t work with columnar storage because it can be flushed just once per block. (I&apos;ve commented out the test case for that.)&lt;/li&gt;
	&lt;li&gt;Raymie and I have designed the solution in layers - one layer that is capable of writing and reading columnar store. The column to data item assignment are a separate layer. We have a default that assigns each item to a separate column up to a depth. But the users can supply their own custom column assignments. Yet another layer is the ability to put the columns in a block. In case someone wants to use file-based columnar storage, they can do so easily on top the first two layers.&lt;/li&gt;
	&lt;li&gt;For now, unions have a single column, irrespective of the number and type of branches.&lt;/li&gt;
	&lt;li&gt;I don&apos;t think extending the codec mechanism for supporting columnar store will work. Columnar store is orthogonal to codecs. Codecs are about storing blocks with compression. Decoders decide how the contents should be interpreted. I think, the way to support columnar storage is to replace binary encoder/decoder with columnar encoder/decoders. I&apos;ve demonstrated in the Data file reader and writer. In order to support both binary encoder/decoder and columnar encoder decoder, I pushes bytesBuffered() to Encoder from BinaryEncoder and isEnd() to Decoder from BinaryDecoder. I don&apos;t think these changes will break anything.&lt;/li&gt;
	&lt;li&gt;The implementation is not complete, We have to let the user of data file writer choose columnar storage instead of binarystorage. I&apos;ve not implemeted it yet.&lt;/li&gt;
	&lt;li&gt;One test in FileSpanStorage is failing possibly because, I think, it assumes something about the way data file stores using binary encoder. I&apos;m not sure.&lt;/li&gt;
	&lt;li&gt;If we make Data file writer/reader handle different encoders/decoders, then changes to columnar storage is reasonably well-isolated.&lt;/li&gt;
	&lt;li&gt;There are a couple of unrelated changes (in tools) that were required to make the tests pass on my machine. Please ignore them.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13052566" author="cutting" created="Tue, 21 Jun 2011 13:39:40 +0000"  >&lt;p&gt;Thiru, this looks great.  So some issues we need to resolve are:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;How does one specify this format to DataFileWriter?  Perhaps rather than extending the codec API we might add EncoderFactory.getEncoderNamed(String) and DecoderFactory.getDecoderNamed(String)?  Then we can add a setEncoding(String) method to DataFileWriter?&lt;/li&gt;
	&lt;li&gt;How does this integrate with compression?  I suspect we should compress each column separately, so the compression codec needs to be invoked on each buffer before it&apos;s written.  This means that the Encoder must know about the compression codec.&lt;/li&gt;
	&lt;li&gt;How is the format indicated in the file itself?  While it may not have made sense to implement this as a codec, it might make sense to use the &quot;avro.codec&quot; metadata field, as readers should already check this.  We might use, e.g., values like &quot;column+snappy&quot;.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think it would be perfectly acceptable if the initial version only supported a particular compression codec, Snappy, and that compression codec was always turned on.  A big advantage of the column representation should be improved compression, and Snappy&apos;s fast enough that using it all of the time doesn&apos;t cost much.&lt;/p&gt;</comment>
                            <comment id="13052721" author="scott_carey" created="Tue, 21 Jun 2011 18:02:08 +0000"  >&lt;blockquote&gt;
&lt;p&gt;For now, unions have a single column, irrespective of the number and type of branches.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think &quot;For now&quot; = Forever?  This is a binary format, I think we should make unions columnar as well.  Backwards compatibility will be hard and high maintenance.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;How does this integrate with compression? I suspect we should compress each column separately, so the compression codec needs to be invoked on each buffer before it&apos;s written. This means that the Encoder must know about the compression codec.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This depends on what the goal is.  If we want to avoid decompressing columns that are not accessed, we will need to do that.  Otherwise it is not necessary and compression ratios will be best if large blocks are compressed as a unit with all columns.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;and Snappy&apos;s fast enough that using it all of the time doesn&apos;t cost much.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, the CRC currently used costs more than Snappy.  &lt;/p&gt;</comment>
                            <comment id="13058098" author="cutting" created="Thu, 30 Jun 2011 21:54:44 +0000"  >&lt;p&gt;&amp;gt; I think we should make unions columnar as well.&lt;/p&gt;

&lt;p&gt;That would be nice, but I&apos;d rather we have something useful sooner than something perfect later.  We can extend it later in a backward-compatible manner.  It would not be forward compatible, but that might be acceptable as long as there&apos;s only a single implementation (Java).&lt;/p&gt;

&lt;p&gt;&amp;gt; If we want to avoid decompressing columns that are not accessed [ ... ]&lt;/p&gt;

&lt;p&gt;I think the advantage of a columnar format is to avoid touching data that&apos;s not needed, and avoiding decompression is consistent with that.&lt;/p&gt;</comment>
                            <comment id="13058166" author="hammer" created="Fri, 1 Jul 2011 00:59:40 +0000"  >&lt;p&gt;&amp;gt; I think the advantage of a columnar format is to avoid touching data that&apos;s not needed, and avoiding decompression is consistent with that.&lt;/p&gt;

&lt;p&gt;During the design of RCFile, the folks from Facebook found that packing a few columns together into a file made for better performance than putting a single column into the file. There&apos;s a trade-off between CPU consumed in deserialization and IO consumed in pulling the data off of disk. Avoiding decompressing columns that are not accessed seemed to be important for Hive performance.&lt;/p&gt;</comment>
                            <comment id="13069700" author="hammer" created="Fri, 22 Jul 2011 19:58:53 +0000"  >&lt;p&gt;In addition to RCFile, it&apos;s also worth comparing this format to the CIF format proposed by IBM Research: &lt;a href=&quot;http://pages.cs.wisc.edu/~jignesh/publ/colMR.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://pages.cs.wisc.edu/~jignesh/publ/colMR.pdf&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13069792" author="cutting" created="Fri, 22 Jul 2011 21:35:54 +0000"  >&lt;p&gt;Yes, CIF file looks promising.  It&apos;s great to see all the benchmarks!&lt;/p&gt;

&lt;p&gt;I wonder if the advantages of CIF could be had without a custom HDFS block placement strategy?  For example, one might pack the files of a split directory into a single file whose block size was set to the size of the file, forcing it into a single block.  This would guarantee locality for the columns of a split.&lt;/p&gt;

&lt;p&gt;In other words, instead of groups of column-major records within a file (&quot;block columnar&quot; in Raymie&apos;s document) on one hand or a file-per-column on the other (&quot;file columnar&quot;), we have a single group per file.  Since splits might often be bigger than RAM, creating these would probably require two steps: writing a set of temporary local files, one per column, then appending these into the final output.  The file would have an index indicating where each column lies, and each column within the file would permit efficient skipping, in the style of CIF.&lt;/p&gt;</comment>
                            <comment id="13257077" author="cutting" created="Wed, 18 Apr 2012 23:19:35 +0000"  >&lt;p&gt;I&apos;ve implemented a new column-file format at:&lt;/p&gt;

&lt;p&gt;  &lt;a href=&quot;https://github.com/cutting/trevni&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/cutting/trevni&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This supports writing Avro data.&lt;/p&gt;

&lt;p&gt;If folks find this useful then I intend to contribute it to Apache.&lt;/p&gt;</comment>
                            <comment id="13260732" author="raymie" created="Tue, 24 Apr 2012 17:36:42 +0000"  >&lt;p&gt;This is the second attempt at a column-major codec.  The whole goal of col-major formats is to optimize performance.  Thus, to drive this exercise forward it seems necessary to have some kind of benchmark to do some testing.  (I don&apos;t think a micro-benchmark is sufficient &amp;#8211; rather the right benchmark is with a query planner (Hive?) that can take advantage of these formats.)  With such a benchmark in place, we&apos;d compare the performance of the existing row-major (as a baseline) Avro formats with the various, proposed col-major formats to make sure that we&apos;re getting the kind of performance improvements (2x, 4x or more) to justify the complexity of a col-major format.&lt;/p&gt;

&lt;p&gt;Some comments more specific to this proposal: First, I&apos;d like to see the Type Mapping section for Avro filled in; this would give us a much better idea of what you&apos;re trying.  Second, at first glance, it seems like your design replicates some of the features of RCFiles that the CIF paper claims cause performance problems (but, again, maybe this issue is better addressed via some benchmarking).&lt;/p&gt;

&lt;p&gt;Regarding your implementation of this proposal, it re-implements all the lower-levels of Avro.  It seems like this double-implementation will be a maintenance problem.  &lt;/p&gt;</comment>
                            <comment id="13263424" author="cutting" created="Fri, 27 Apr 2012 07:23:41 +0000"  >&lt;p&gt;Raymie, thanks for your thoughts.&lt;/p&gt;

&lt;p&gt;I agree that benchmarks are needed.  The best benchmarks are real applications.  I provided code that folks can try now in their MapReduce applications.  I have not yet had a chance to integrate this with Hive by writing a SerDe, but that is an obvious next step.  (I&apos;ve never written a SerDe.  If someone else has perhaps they can help.)&lt;/p&gt;

&lt;p&gt;Do you have any datasets or queries that you&apos;d like to propose as benchmarks?&lt;/p&gt;

&lt;p&gt;I&apos;ll work on better documenting the type mapping for Avro, since that&apos;s been implemented.&lt;/p&gt;

&lt;p&gt;I&apos;ll be on offline next week and won&apos;t be able to work more on this (or respond) until the week after.&lt;/p&gt;</comment>
                            <comment id="13263665" author="raymie" created="Fri, 27 Apr 2012 14:02:07 +0000"  >&lt;p&gt;In about a month we will have some Hive benchmarks, but the data won&apos;t be very wide, so they won&apos;t be good for testing column-major formats.  However, maybe we should walk before we run: If someone puts Avro SerDe&apos;s in place against the regular Avro format, we could benchmark and maybe even help tune that configuration, which would provide a baseline for testing a column-major configuration.  (Unfortunately, we can&apos;t do the SerDe work itself.)&lt;/p&gt;</comment>
                            <comment id="13267017" author="jghoman" created="Wed, 2 May 2012 23:24:25 +0000"  >&lt;p&gt;The Avro Serde already exists: &lt;a href=&quot;https://github.com/jghoman/haivvreo&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/jghoman/haivvreo&lt;/a&gt; and is being ported to Hive directly.  I&apos;d love to test this format with it, but I in all likelihood won&apos;t have time in the next month.  If someone wants to do that, it&apos;d be great.  Shouldn&apos;t be too hard at all.&lt;/p&gt;</comment>
                            <comment id="13267317" author="gemini5201314" created="Thu, 3 May 2012 09:04:51 +0000"  >&lt;p&gt;1.the file header should have some extra space in case we add some column or append some value to block end.&lt;br/&gt;
2.trevni set different codec for each column,I think there is different between compression,decompression and encoding,decoding:for example,run length encoding will have high compression ratio by sort them first.we change the sequence of data within a minor block(say 64k) but still guarantee the whole block(say hdfs block 128M) will looks same as before(except that data sequence change).so,use different Codec for each column maybe not get better performance or compression ratio than a single codec for all column.I think different column need different encodec (for example run length encodec which we didn&apos;t implement yet).&lt;/p&gt;</comment>
                            <comment id="13267465" author="gemini5201314" created="Thu, 3 May 2012 14:30:13 +0000"  >&lt;p&gt;compression always treat everything as raw bytes,encoding and decoding only apply to a certain pattern.The encodec and decodec should be separated,for example for dictionary encoding(like email column or address column),we will decoding it only when we need exact value,if we just need to count the total number of row,the application program will tell how fileformat treat that.For simplicity,we always decoding it first.&lt;/p&gt;</comment>
                            <comment id="13446228" author="cutting" created="Fri, 31 Aug 2012 18:41:20 +0000"  >&lt;p&gt;I&apos;d like to bring the Trevni (&lt;a href=&quot;https://github.com/cutting/trevni&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/cutting/trevni&lt;/a&gt;) code to Apache.  How do folks think we ought to do this?&lt;/p&gt;

&lt;p&gt;Trevni&apos;s serialization, columns of scalar values, is different from Avro.  It doesn&apos;t require a schema, but implements a mapping between Avro schemas and columns.  I see three options:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;incorporate it into Avro&apos;s lang/* tree.  Currently Trevni has only a Java implementation, so its code could be merged into lang/java as new modules.  The trevni-core module is independent of Avro, but the trevni-avro module depends on other Avro modules.&lt;/li&gt;
	&lt;li&gt;have an independent code tree for Trevni but still managed by the Avro PMC.  If we expect that folks from Avro will also be the folks who work on Trevni then having the Avro project produce Trevni as a separately released product might be reasonable.&lt;/li&gt;
	&lt;li&gt;submit an incubator proposal for Trevni, aiming for an independent TLP.  I worry that Trevni&apos;s too small to sustain an independent community and that bundling it with Avro might be best.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Which do folks think is best?  I&apos;m leaning towards (1).  Any objections to that?  If not, I&apos;ll prepare a patch.&lt;/p&gt;</comment>
                            <comment id="13446273" author="scott_carey" created="Fri, 31 Aug 2012 19:17:54 +0000"  >&lt;p&gt;I think (1) is the best way to start.  We could easily transition to (2) if that made sense due to other language implementations, and (3) if it grows big enough.&lt;/p&gt;

&lt;p&gt;We may want to identify it separately as &apos;evolving&apos; or similar so that API changes in the next couple releases if needed can be managed more flexibly.&lt;/p&gt;</comment>
                            <comment id="13448159" author="jghoman" created="Tue, 4 Sep 2012 23:05:57 +0000"  >&lt;p&gt;(1) sounds good.  &lt;/p&gt;

&lt;p&gt;I&apos;m curious about the decision to go with 1 row group/block and 1 block/file.  The recommendation is to increase the block size, up to a 1 gb or more.  Any background on this design decision?  Any reason not to go with multiple, smaller row groups per file ala RCFile?&lt;/p&gt;</comment>
                            <comment id="13448220" author="cutting" created="Wed, 5 Sep 2012 00:01:37 +0000"  >&lt;p&gt;Jakob, this is discussed in the spec and also in the CIF paper cited there.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://cutting.github.com/trevni/spec.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://cutting.github.com/trevni/spec.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;RCFile, with relatively small row groups, can save CPU time when skipping un-processed columns, but isn&apos;t able to save much i/o time.  To save i/o time you need adjacent sequences of values that are larger than 10MB or so (so transfer time &amp;gt;&amp;gt; seek time).  When values in some columns are considerably smaller than others (e.g., an int or long versus a blob) then it can take a lot of records to get 10MB or more of the smaller values.  Put another way, i/o performance is optimal when there&apos;s one row group per unit of parallelism so there&apos;s just one seek per column.  So a job with 100 map tasks should ideally have 100 row groups.  And we want to localize these map tasks, so each group should ideally reside in a single HDFS datanode.  The CIF folks achieve this by having a single-block file per column in the row group, then using a custom block placement strategy to place these on the same datanodes.  Trevni instead works with stock HDFS by placing all the columns of each row group in a single file that fits in a single block.  Does that make sense?&lt;/p&gt;</comment>
                            <comment id="13448281" author="jghoman" created="Wed, 5 Sep 2012 00:52:16 +0000"  >&lt;p&gt;Yes, that&apos;s all reasonable.  My concern is just enforcing a 1:1:1 relationship between row groups, blocks and files.  RCFile&apos;s very tiny recommended row group size (4mb, I believe), certainly don&apos;t make sense from an IO perspective.  But if our only ability to increase parallelism on trevni files is to decrease the size of row groups (and correspondingly increase the number of files), this may be a problem.  It&apos;s not required to enforce a 1:1:1 relationship in the file; one could still have row groups large enough to make it worth the IO (and still split on block boundaries), but have multiple of them within a single trevni file.  This could certainly be supported as an option.&lt;/p&gt;

&lt;p&gt;Either way, this is looking good.  &lt;/p&gt;</comment>
                            <comment id="13448632" author="tomwhite" created="Wed, 5 Sep 2012 10:59:35 +0000"  >&lt;p&gt;+1 for option (1)&lt;/p&gt;</comment>
                            <comment id="13448671" author="raymie" created="Wed, 5 Sep 2012 12:31:18 +0000"  >&lt;p&gt;I&apos;m strongly against option (1).  Trevni is clearly designed to use Avro (and Thrift and Protocol Buffers).  Indeed, it&apos;s code is in org.apache.trevni, not org.apache.avro.trevni (which it should be &amp;#8211; I&apos;m not suggesting otherwise).  Further, I can imagine large number of Avro usages &amp;#8211; perhaps the majority &amp;#8211; not wanting Trevni.  So it seems like option (1) creates unnecessary coupling &amp;#8211; and just sets us up for refactoring the code base in the future.&lt;/p&gt;

&lt;p&gt;Regarding option (2), I think that&apos;s bad for Trevni.  If you want to recruit members of the Thrift and Protocol Buffers communities to the Trevni project, it seems like a mistake to put Trevni into the Avro project.  You write &quot;I worry that Trevni&apos;s too small to sustain an independent community,&quot; but this has the danger of becoming a self-fulfilling prophesy.  For example, as far as I can tell, the Thrift and Protocol Buffer communities are larger than Avro&apos;s, so from a recruiting perspective, aren&apos;t you working to fulfill your &quot;small community&quot; prophesy by dropping Trevni into the smallest community of the three?&lt;/p&gt;

&lt;p&gt;However, if you believe that the Avro project is currently the best home for Trevni, then I believe a top-level trevni/ directory in the Avro project is the right place for it.&lt;/p&gt;</comment>
                            <comment id="13449230" author="cutting" created="Wed, 5 Sep 2012 22:56:54 +0000"  >&lt;p&gt;Here&apos;s a patch that implements (1) above.&lt;/p&gt;</comment>
                            <comment id="13449239" author="cutting" created="Wed, 5 Sep 2012 23:08:24 +0000"  >&lt;p&gt;Raymie, putting the code in Avro&apos;s lang/java directory does not in fact create any tight coupling.  Trevni&apos;s core module still does not depend on anything in Avro.  Avro releases will contain a trevni-core.jar file that depends on nothing else in Avro.&lt;/p&gt;

&lt;p&gt;That said, you earlier complained (above) that Trevni&apos;s core implementation doesn&apos;t share enough with Avro.  Placing these in a common project would permit such sharing if it was desired.  For example, as you indicated, they might share an optimized library for serialization and deserialization of scalar values.&lt;/p&gt;

&lt;p&gt;The patch I&apos;ve attached makes no changes to Trevni code, only to build files so that Trevni&apos;s source code can be placed in Avro&apos;s lang/java tree.  Committing this patch says just two things:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;The Avro PMC will maintain the Trevni code (for now); and&lt;/li&gt;
	&lt;li&gt;Avro and Trevni will be released together (for now);&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;It does not say anything about other couplings or dependencies between the Avro and Trevni implementations or specifications.  That&apos;s all for future discussion.&lt;/p&gt;</comment>
                            <comment id="13449248" author="cutting" created="Wed, 5 Sep 2012 23:15:01 +0000"  >&lt;p&gt;Raymie said: &quot;... the Thrift and Protocol Buffer communities are larger than Avro&apos;s... &quot;.&lt;/p&gt;

&lt;p&gt;That&apos;s certainly true for RPC, but I doubt it&apos;s true for persistent data storage.  Neither Protocol Buffers nor Thrift even offer a standard file format.  In that realm I think Avro leads both.  Trevni is a file format.  We can shred Protocol Buffer and Thrift data structures to it regardless of where it lives in the code tree.&lt;/p&gt;
</comment>
                            <comment id="13449265" author="cutting" created="Wed, 5 Sep 2012 23:38:38 +0000"  >&lt;p&gt;Jakob, I think the more common case will be that fields whose values are small will produce small columns where seek time becomes significant.  When seek time is significant the returns of greater parallelism are diminished unless replication is also increased, which is unlikely.&lt;/p&gt;

&lt;p&gt;With multiple row groups per file you have to choose a size for the row groups.  Would you ever choose a size smaller than 64MB, the typical HDFS block size?  Column files are only an advantage when there are multiple columns, so the amount read will typically be a fraction of the row group size.&lt;/p&gt;

&lt;p&gt;What cases do you imagine where having a row group size less than a file is useful?&lt;/p&gt;</comment>
                            <comment id="13449730" author="cutting" created="Thu, 6 Sep 2012 15:15:09 +0000"  >&lt;p&gt;Also, Jakob, I may have overstated things a bit above.  With a Trevni file one can have a higher degree of parallelism than one processor per file.  A Trevni file can be efficiently split into row ranges.  So a file with 1M rows could be processed as 10 tasks, each processing 100k rows.  Values are chunked into ~64k compressed blocks and only those overlapping with the specified range would need to be decompressed and processed by a task.&lt;/p&gt;</comment>
                            <comment id="13452493" author="cutting" created="Mon, 10 Sep 2012 21:58:34 +0000"  >&lt;p&gt;Anyone object to me committing the attached patch, which keeps Trevni in its own packages, jar files and maven modules, but places its code in the lang/java tree so it&apos;s released along with Avro?  If not, I&apos;ll commit it soon.&lt;/p&gt;</comment>
                            <comment id="13453444" author="cutting" created="Tue, 11 Sep 2012 21:41:11 +0000"  >&lt;p&gt;I committed this.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12616892">PIG-3059</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12613954">PIG-3015</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12417027">HIVE-352</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12433139">HIVE-756</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12611990">HIVE-3585</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12482810" name="AVRO-806-v2.patch" size="76657" author="thiru_mg" created="Thu, 16 Jun 2011 15:53:07 +0000"/>
                            <attachment id="12543933" name="AVRO-806.patch" size="244285" author="cutting" created="Wed, 5 Sep 2012 22:56:54 +0000"/>
                            <attachment id="12477176" name="AVRO-806.patch" size="31694" author="cutting" created="Fri, 22 Apr 2011 22:57:10 +0000"/>
                            <attachment id="12482811" name="avro-file-columnar.pdf" size="62959" author="thiru_mg" created="Thu, 16 Jun 2011 15:53:07 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 20 Apr 2011 16:54:06 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>63985</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 19 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0e2kv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>80180</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>
</channel>
</rss>
