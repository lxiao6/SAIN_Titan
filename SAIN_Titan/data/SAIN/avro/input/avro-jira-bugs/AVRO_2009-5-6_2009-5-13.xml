<!--
RSS generated by JIRA (7.6.3#76005-sha1:8a4e38d34af948780dbf52044e7aafb13a7cae58) at Mon Jan 21 19:12:07 UTC 2019

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<!-- If you wish to do custom client-side styling of RSS, uncomment this:
<?xml-stylesheet href="https://issues.apache.org/jira/styles/jiraxml2html.xsl" type="text/xsl"?>
-->
<rss version="0.92">
    <channel>
        <title>ASF JIRA</title>
        <link>https://issues.apache.org/jira/issues/?jql=project+%3D+AVRO+AND+created+%3E%3D+2009-5-6+AND+created+%3C%3D+2009-5-13+ORDER+BY+key+ASC</link>
        <description>An XML representation of a search request</description>
                <language>en-uk</language>
                        <issue start="0" end="4" total="4"/>
                <build-info>
            <version>7.6.3</version>
            <build-number>76005</build-number>
            <build-date>09-01-2018</build-date>
        </build-info>

<item>
            <title>[AVRO-27] Consistent Overhead Byte Stuffing (COBS) encoded block format for Object Container Files</title>
                <link>https://issues.apache.org/jira/browse/AVRO-27</link>
                <project id="12310911" key="AVRO">Apache Avro</project>
                    <description>&lt;p&gt;Object Container Files could use a 1 byte sync marker (set to zero) using zig-zag and COBS encoding within blocks to efficiently escape zeros from the record data.&lt;/p&gt;

&lt;h4&gt;&lt;a name=&quot;ZigZagencoding&quot;&gt;&lt;/a&gt;Zig-Zag encoding&lt;/h4&gt;

&lt;p&gt;With zig-zag encoding only the value of 0 (zero) gets encoded into a value with a single zero byte.  This property means that we can write any non-zero zig-zag long inside a block within concern for creating an unintentional sync byte. &lt;/p&gt;

&lt;h4&gt;&lt;a name=&quot;COBSencoding&quot;&gt;&lt;/a&gt;COBS encoding&lt;/h4&gt;

&lt;p&gt;We&apos;ll use COBS encoding to ensure that all zeros are escaped inside the block payload.  You can read &lt;a href=&quot;http://www.sigcomm.org/sigcomm97/papers/p062.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.sigcomm.org/sigcomm97/papers/p062.pdf&lt;/a&gt; for the details about COBS encoding.&lt;/p&gt;

&lt;h1&gt;&lt;a name=&quot;BlockFormat&quot;&gt;&lt;/a&gt;Block Format&lt;/h1&gt;

&lt;p&gt;All blocks start and end with a sync byte (set to zero) with a type-length-value format internally as follows:&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; name &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; format &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; length in bytes &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; value &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; meaning &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; sync &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; byte &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 1 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; always 0 (zero) &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The sync byte serves as a clear marker for the start of a block &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; type &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; zig-zag long &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; variable &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; must be non-zero &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The type field expresses whether the block is for &lt;em&gt;metadata&lt;/em&gt; or &lt;em&gt;normal&lt;/em&gt; data. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; length &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; zig-zag long &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; variable &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; must be non-zero &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The length field expresses the number of bytes until the next record (including the cobs code and sync byte).  Useful for skipping ahead to the next block. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; cobs_code &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; byte &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 1 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; see COBS code table below &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Used in escaping zeros from the block payload &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; payload &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; cobs-encoded &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Greater than or equal to zero &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; all non-zero bytes &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The payload of the block &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; sync &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; byte &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 1 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; always 0 (zero) &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The sync byte serves as a clear marker for the end of the block &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;h2&gt;&lt;a name=&quot;COBScodetable&quot;&gt;&lt;/a&gt;COBS code table &lt;/h2&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Code &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Followed by &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Meaning &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x00 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (not applicable) &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (not allowed ) &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x01 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; nothing &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Empty payload followed by the closing sync byte &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x02 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; one data byte &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The single data byte, followed by the closing sync byte &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x03 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; two data bytes &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The pair of data bytes, followed by the closing sync byte &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x04 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; three data bytes &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The three data bytes, followed by the closing sync byte &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; n &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (n-1) data bytes &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The (n-1) data bytes, followed by the closing sync byte &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0xFD &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 252 data bytes &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The 252 data bytes, followed by the closing sync byte &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0xFE &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 253 data bytes &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The 253 data bytes, followed by the closing sync byte &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0xFF &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 254 data bytes &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The 254 data bytes &lt;b&gt;not&lt;/b&gt; followed by a zero. &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;(taken from &lt;a href=&quot;http://www.sigcomm.org/sigcomm97/papers/p062.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.sigcomm.org/sigcomm97/papers/p062.pdf&lt;/a&gt;)&lt;/p&gt;

&lt;h1&gt;&lt;a name=&quot;Encoding&quot;&gt;&lt;/a&gt;Encoding&lt;/h1&gt;

&lt;p&gt;Only the block writer needs to perform byte-by-byte processing to encode the block.  The overhead for COBS encoding is very small in terms of the in-memory state required.&lt;/p&gt;

&lt;h1&gt;&lt;a name=&quot;Decoding&quot;&gt;&lt;/a&gt;Decoding&lt;/h1&gt;

&lt;p&gt;Block readers are not required to do as much byte-by-byte processing as a writer.  The reader could (for example) find a &lt;em&gt;metadata&lt;/em&gt; block by doing the following:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Search for a zero byte in the file which marks the start of a record&lt;/li&gt;
	&lt;li&gt;Read and zig-zag decode the &lt;em&gt;type&lt;/em&gt; of the block
	&lt;ul&gt;
		&lt;li&gt;If the block is &lt;em&gt;normal&lt;/em&gt; data, read the &lt;em&gt;length&lt;/em&gt;, seek ahead to the next block and goto step #2 again&lt;/li&gt;
		&lt;li&gt;If the block is a &lt;em&gt;metadata&lt;/em&gt; block, cobs decode the data&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;
</description>
                <environment></environment>
        <key id="12424749">AVRO-27</key>
            <summary>Consistent Overhead Byte Stuffing (COBS) encoded block format for Object Container Files</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21141&amp;avatarType=issuetype">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="7">Later</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="massie">Matt Massie</reporter>
                        <labels>
                    </labels>
                <created>Wed, 6 May 2009 20:11:33 +0000</created>
                <updated>Tue, 14 Jul 2009 23:04:48 +0000</updated>
                            <resolved>Tue, 23 Jun 2009 20:02:38 +0000</resolved>
                                                                    <component>spec</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12706754" author="scott_carey" created="Thu, 7 May 2009 08:02:57 +0000"  >&lt;p&gt;An outsider here &amp;#8211; I&apos;ve got an idea on how to avoid the performance pitfalls of COBS&apos; byte-by-byte nature and as I thought through it, I spotted many other opportunities for enhancement since larger chunks afford a lot more bits in the Code that can be used for things other than the length of the following literal chunk.&lt;/p&gt;

&lt;h2&gt;&lt;a name=&quot;ProposalCOLS%2CamodificationofCOBS&quot;&gt;&lt;/a&gt;Proposal &amp;#8211; COLS, a modification of COBS &lt;/h2&gt;
&lt;h3&gt;&lt;a name=&quot;%28forgreaterperformanceandextensibilityforlargedatastreams%29&quot;&gt;&lt;/a&gt;(for greater performance and extensibility for large data streams) &lt;/h3&gt;

&lt;p&gt;Java is particularly bad at byte-by-byte operations.  The COBS paper clearly indicates its design intention was stuffing data through embedded systems such as telephone lines and other networks where byte-by-byte processing of the whole payload is already mandatory.&lt;/p&gt;

&lt;p&gt;Doing so here would be a performance bottleneck in Java.  Some simple tests can be constructed to prove or disprove this claim.&lt;/p&gt;

&lt;p&gt;I propose that rather than use COBS, one uses COLS or COWS ... that is Constant Overhead Long Stuffing or Constant Overhead Word Stuffing instead.&lt;/p&gt;

&lt;p&gt;This would be inefficient if we expect most payloads to be small (less than 256 bytes), but I suspect most hadoop related payloads to be large, and often very large.&lt;/p&gt;

&lt;p&gt;I favor stuffing Longs rather than Ints, since most systems will soon be running 64 bit JVMs in the future.  Sun&apos;s next JRE release has Object Pointer Compression, which makes the memory overhead of a 64 bit JVM very small compared to a 32 bit JVM, and performance is generally faster than the 32 bit JVM due to native 64 bit operations and more registers (for x86-64 at least).&lt;br/&gt;
&lt;a href=&quot;http://blog.juma.me.uk/2008/10/14/32-bit-or-64-bit-jvm-how-about-a-hybrid/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://blog.juma.me.uk/2008/10/14/32-bit-or-64-bit-jvm-how-about-a-hybrid/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I will describe the proposal below assuming a translation of COBS to COLS, from 1 byte at a time to 8 byte at a time encoding.  However, it is clear that a 4 byte variant is very similar and may be preferable.&lt;/p&gt;

&lt;h3&gt;&lt;a name=&quot;ProposedChangesSimpleBlockformatwithCOLS&quot;&gt;&lt;/a&gt;Proposed Changes &amp;#8211; Simple Block format with COLS&lt;/h3&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; name &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; format &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; length in bytes &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; value &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; meaning &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; sync &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; byte &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;b&gt;8&lt;/b&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;b&gt;0L&lt;/b&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The sync &lt;b&gt;long&lt;/b&gt; serves as a clear marker for the start of a block &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; type &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 1 byte &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 1 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; non-zero &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The type field expresses whether the block is for &lt;em&gt;metadata&lt;/em&gt; or &lt;em&gt;normal&lt;/em&gt; data. &lt;b&gt;note - if this is only ever going to be a binary flag, it can be packed into the length or sequence number as a sign value.&lt;/b&gt;   &lt;b&gt;However, it is decoding performance critical to keep the non-COLS header 8 byte aligned&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; block sequence number &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 3 byte unsigned int &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 3 bytes &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0 - 2^24 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; the block sequence number &amp;#8211; a client can use this to resume a stream from the last successful block. This may not be needed if the metadata blocks take care of this. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; length &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; fixed 4 byte signed int &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; variable &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;b&gt;&amp;gt;= 0L&lt;/b&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The length field expresses the number of bytes of COLS_payload data in bytes.  Useful for skipping ahead to the next block.  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; COLS_payload &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;b&gt;COLS&lt;/b&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;b&gt;length as above&lt;/b&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; see COLS description below &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The data in this block, encoded. &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;The above would put cap the stream length to 2GB * 16M = 32PB.  There is room to increase this significantly by taking bits from the type and giving those to the block count.  2GB blocks are rather unlikely for now however &amp;#8211; as is multi-PB streams. &lt;/p&gt;

&lt;h4&gt;&lt;a name=&quot;Discussion&quot;&gt;&lt;/a&gt;Discussion&lt;/h4&gt;
&lt;ul&gt;
	&lt;li&gt;The entire stream would need to be 8 byte aligned in order to process it cleanly with something like java.nio.LongBuffer.  This would include metadata blocks.&lt;/li&gt;
	&lt;li&gt;The sequence is assumed to be in network-order.   Endianness can be handled and is not discussed in detail here.&lt;/li&gt;
	&lt;li&gt;The type can likely be encoded in a single bit in the block sequence number or length field.  If more than two types of blocks are expected, more bits can be reserved for future use.&lt;/li&gt;
	&lt;li&gt;The length can be stored as the number of longs rather than bytes (bytes / 8) since the COLS payload is a multiple of 8 bytes.&lt;/li&gt;
	&lt;li&gt;The COLS payload here differs from the original proposal.  It will have an entire COBS-like stream, with possibly many COLS code markers (at least one per 0L value in the block data).&lt;/li&gt;
	&lt;li&gt;One may want to have both the encoded length above, and the decoded length (or a checksum) as extra data validation.  Perhaps even 4 types:  METADATA, METADATA_CSUM, NORMAL, NORMAL_CSUM &amp;#8211; where  the ordinary variants store the length (fast, but less reliable) and the _CSUM variants store a checksum (slower, but highly reliable).&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;&lt;a name=&quot;BasicCOBStoCOLSdescription&quot;&gt;&lt;/a&gt;Basic COBS to COLS description&lt;/h3&gt;
&lt;p&gt;COBS describes a byte-by-byte encoding where a zero byte cannot exist, and a set of codes are used to encode runs of data that does not contain a zero byte.  All codes but but one have an implicit trailing zero.  The last block is assumed to have no implicit zero regardless of the code.&lt;/p&gt;

&lt;p&gt;COLS is a simple extension of this scheme to 64 bit chunks.  In its base form, it does nothing more than work with larger chunks:&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; COLS Code (Long, 8 bytes) &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Followed by &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Meaning &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0L &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; N/A &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (not allowed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 1L &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; nothing &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; A single zero Long &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 2L &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; one long (8 bytes) &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The single data long, followed by a trailing zero long * &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 3L &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; two longs (16 bytes) &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The pair of data longs, followed by a trailing zero long * &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; nL &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (n-1) longs &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The (n-1) longs, followed by a trailing zero long * &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; MAX &amp;#42;&amp;#42; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; MAX - 1 longs &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; MAX -1 longs, with no trailing zero &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;&amp;#42; The last code in the sequence (which can be identified by the length header or a 0L indicating the start of the next block) does NOT have an implicit trailing zero. &lt;br/&gt;
&amp;#42;&amp;#42; MAX needs to be chosen, and can&apos;t realistically be very large since encoding requires an arraycopy of size (MAX -1) * 8&lt;/p&gt;

&lt;p&gt;The COLS_payload has multiple COLS Code entries (and literals), up to the length specified in the header (where a 0L should then occur).&lt;/p&gt;

&lt;p&gt;However &amp;#8211; there are drawbacks to using such a large chunk without other modifications from COBS:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;64 bits is far too large for a length field.  For encoding, a COBS code block must fit in RAM, and for performance, should probably fit in half an L2 cache.  However, for decoding COLS code length is irrelevant.&lt;/li&gt;
	&lt;li&gt;If the size of the data encoded is not a multiple of 8 bytes, we need a mechanism to encode that up to 7 trailing bytes should be truncated (3 bits).&lt;/li&gt;
	&lt;li&gt;For most blocks, the overhead will be exactly 8 bytes (unless the block has a trailing 0L).&lt;/li&gt;
	&lt;li&gt;Very long data streams without a zero Long are unlikely, so very large chunk lengths are not very useful.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;There are also benefits however. The above suggests that most of the 8 byte COLS code block space is not needed to encode length.  Much can be done with this!&lt;br/&gt;
Some thoughts:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The 3 bits needed to define the truncation behavior can be stored in the COLS code.&lt;/li&gt;
	&lt;li&gt;The overhead can be reduced, by encoding short trailing sequences into the upper bits rather than purely truncating &amp;#8211; e.g. you can append 2 bytes instead of truncating 6.&lt;/li&gt;
	&lt;li&gt;Rudimentary run-length encoding or other light weight compression can be done with the extra bits (completely encoder-optional).&lt;/li&gt;
	&lt;li&gt;We can remove the requirement that most codes have an implicit trailing zero, and encode that in one of the extra bits.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If only the lower 2 bytes of an 8 byte COLS code represent the size, (MAX = 2^16 - 1), then the max literal size is 512KB - 8B.  If we remove the implicit trailing zero, an encoder can optionally encode smaller literal sequences (perhaps for performance, or compression).&lt;br/&gt;
What can be done with the remaining 48 bits?&lt;br/&gt;
Some ideas:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;The highest 4 bytes can represent data to append to the literal.  In this way, half of the size overhead of the encoding is removed.  This should generally only apply to the last COLS code in the block (for performance reasons and maintaining 8 byte alignment on all arraycopy operations, but its encoder optional).&lt;/li&gt;
	&lt;li&gt;the next bit represents whether the COLS block has an implicit 0L appended.&lt;/li&gt;
	&lt;li&gt;a bit can be used to signify endianness (this might be a better fit for the Block header or stream metadata &amp;#8211; detecting zero&apos;s works without known endianness)&lt;/li&gt;
	&lt;li&gt;The next three bits can represent how much data is truncated or appended to the literal, (before the optional implicit 0L):&lt;/li&gt;
&lt;/ol&gt;


&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; value &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; meaning &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 000 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; do not truncate or append &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 100 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; append all 4 leading bytes in the COLS code after the literal &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 111 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; append the first 3 leading bytes in the COLS code after the literal &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 110 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; append the first 2 leading bytes in the COLS code after the literal &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 101 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; append the leading byte in the COLS code after the literal &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 011 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; truncate the last 3 bytes of the literal &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 010 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; truncate the last 2 bytes of the literal &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 001 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; truncate the last byte of the literal &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;This leaves us with 12 bits.  I propose that these be used for rudimentary (optional) compression:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Option A:
	&lt;ul&gt;
		&lt;li&gt;Run length only &amp;#8211; the 12 bits represent the number of times to repeat the literal.  Or 4 bits are the number of COLS chunks backwards (including this one) to repeat, and 8 bits is the number of repeats.  Or ... some other form of emitting copies of entire COLS chunks.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Option B:
	&lt;ul&gt;
		&lt;li&gt;Some form of LZ-like compression that copies in 8 byte chunks &amp;#8211; 4 bits represent the number of Longs to copy (so, max match size is 15 * 8 bytes), and 8 bits represents the number of Longs backwards (from the end of this COLS chunk) to begin that copy (up to 2KB).  Because of the truncation/append feature, this is not constrained to 8-byte aligned copies on the output, but the encoded format is entirely 8 byte aligned and all copies are multiples of 8 bytes.  I would not be surprised if this was as fast as LZO or faster, since it is very similar but operates in a more chunky fashion.  Compression levels would not be that great, but like most similar algorithms to this the encoder can do more work to search for matches.  Decoding uncompressed data should be essentially free (if the 4 bits are 0, do nothing &amp;#8211; and most COLS blocks would be fairly large so this check does not occur that frequently).&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Option C:
	&lt;ul&gt;
		&lt;li&gt;Reserve those 12 bits for future use / research&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Alternatively, one to 4 extra bytes used for the &quot;append&quot; feature can be reassigned to have more than 12 bits for compression metadata. &lt;/p&gt;

&lt;p&gt;So, with the above modifications, the COLS code  looks like this:&lt;/p&gt;

&lt;p&gt;The COLS code is 8 bytes.  The low 16 bits encode basic meaning.&lt;br/&gt;
An 8 byte COLS code cannot be 0L.&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Code &amp;amp; 0xFFFF (low 2 bytes) &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Followed by &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Meaning &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x0000 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; N/A &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (not allowed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x0001 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; nothing &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; A single zero Long &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x0002 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; one long (8 bytes) &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The single data long &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x0003 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; two longs (16 bytes) &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; The pair of data longs &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; n &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (n-1) longs &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  The (n-1) longs &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0xFFFF &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 2^16 - 2 longs &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 2^16 - 2 longs &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;The next portion, is to determine the state of truncation or appending. &lt;br/&gt;
Two options are listed &amp;#8211; only truncation, and truncation/appending.  The appending could be up to 5 bytes if we squeeze all the rest of the space.  The example below is for up to 4 bytes appended and 3 bytes truncated.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;appendCode = (Code &amp;gt;&amp;gt; 28) &amp;amp; 0xF;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; appendCode &amp;amp; 0x7 &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Append &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/add.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; or truncate &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/forbidden.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; From &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; truncate only option &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x0 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; nothing&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x1 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (-)1 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; nothing &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (-)1 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x2 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (-)2 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; nothing &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (-)2 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x3 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (-)3 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; nothing &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (-)3 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x4 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (+)1 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Code &amp;gt;&amp;gt;&amp;gt; 56 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (-)4 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x5 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (+)2 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Code &amp;gt;&amp;gt;&amp;gt; 48 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (-)5 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x6 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (+)3 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Code &amp;gt;&amp;gt;&amp;gt; 40 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (-)6 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0x7 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (+)4 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Code &amp;gt;&amp;gt;&amp;gt; 32 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; (-)7 &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;It may be wiser to choose an option between these.  If 3 bytes are chosen as the max arbitrary append length, with 4 truncated, 20 bits are left for other purposes, rather than 12.  The average COLS chunk would be one byte larger.&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; AppendCode &amp;amp; 0x8 &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Append 0L &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 0 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; do not append 0L (8 zero bytes) &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 1 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; do append 0L (8 zero bytes) &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;h2&gt;&lt;a name=&quot;Encoding&quot;&gt;&lt;/a&gt;Encoding&lt;/h2&gt;
&lt;p&gt;The writer would perform processing in 8 byte chunks until the end of the block where some byte-by-byte processing would occur. Compression options would be entirely the writer&apos;s choice.&lt;br/&gt;
The state overhead can be very low or large at the writer&apos;s whim.  Larger COLS chunk sizes require more state (and larger arraycopys), and any compression option adds state overhead.  &lt;/p&gt;

&lt;h2&gt;&lt;a name=&quot;Decoding&quot;&gt;&lt;/a&gt;Decoding&lt;/h2&gt;
&lt;p&gt;Decoding in all circumstances reads data in 8 byte chunks.  Copies occur in 8 byte chunks, 8 byte aligned save for the end of a block if the block does not have a multiple of 8 bytes in its payload.  An encoder can cause copy destinations (but not sources) to not be 8 byte aligned if certain special options (compression) or intentionally misaligned encoding is done.   Generally, an encoder can choose to make all but the last few bytes of the last block in the stream aligned. &lt;/p&gt;</comment>
                            <comment id="12707095" author="cutting" created="Thu, 7 May 2009 20:49:02 +0000"  >&lt;p&gt;Before we get to far, let&apos;s review the motivation for this.  From Matt&apos;s message:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It make more sense to make that we use the same record boundary (0) for all Avro records instead of having them be random.  The format would be more resilient to data corruption easier to parse.  It&apos;s also possible (although improbable) that the 16-byte UUID might be part of the payload... especially given the size of the data Hadoop processes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;ol&gt;
	&lt;li&gt;What&apos;s the tangible advantage of a single record boundary?&lt;/li&gt;
	&lt;li&gt;Why would this be more corruption resistant?&lt;/li&gt;
	&lt;li&gt;How likely is a collision?  By my reading of &lt;a href=&quot;http://en.wikipedia.org/wiki/Birthday_attack&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Birthday_attack&lt;/a&gt;, we have a ~1% chance of collision in an exabyte (10^18B) of data, roughly 1000 times todays largest datasets, if we used the same marker for the full exabyte, which we would not, since we&apos;d choose a new marker per output partition.  Switching to a 32 byte marker would raise this to 10^37B.  So we might consider that if we&apos;re worried about collisions.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="12707181" author="massie" created="Thu, 7 May 2009 23:29:40 +0000"  >&lt;blockquote&gt;
&lt;p&gt;1. What is the tangible advantage of a single record boundary?&lt;br/&gt;
2. Why would this be more corruption resistant?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m imagining a situation where you have part of an Avro Object container file minus the header/footer metablock because of data loss or subscribing to a data stream in &quot;real-time&quot; midstream.  In that situation, determining the random 16 byte sync marker would require some work (e.g. finding recurring 16-byte values, searching for the string &quot;schema&quot; and working back, etc).  Having a constant sync value (with an escaped payload) makes this recovery easier and the code a little cleaner.  To be honest, this point is weakened by the fact that we&apos;re not planning on streaming Object container files anyway.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;3. How likely is a collision?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Seems like this is a non-issue with a 16-byte sync value as it is now but it&apos;s always good to be future proof.   &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  &lt;/p&gt;

&lt;p&gt;I&apos;m curious what other Java experts  (since I&apos;m not) out there feel about COBS in Java .  It sounds from Scott&apos;s comment that byte stuffing in Java is a non-starter.&lt;/p&gt;

&lt;p&gt;There is code at..&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://bosshog.lbl.gov/repos/java-u3/trunk/sea/src/gov/lbl/dsd/sea/nio/util/COBSCodec.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://bosshog.lbl.gov/repos/java-u3/trunk/sea/src/gov/lbl/dsd/sea/nio/util/COBSCodec.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;...from Lawrence Berkeley Labs to do COBS encoding in Java with the following comment&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;/* Performance Note: The JDK 1.5 server VM runs &amp;lt;code&amp;gt;decode(encode(src))&amp;lt;/code&amp;gt;
 * at about 125 MB/s throughput on a commodity PC (2 GHz Pentium 4). Encoding is
 * the bottleneck, decoding is extremely cheap. Obviously, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is way more
 * efficient than Base64 encoding or similar application level &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; stuffing
 * mechanisms.
 */
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12707449" author="scott_carey" created="Fri, 8 May 2009 17:51:00 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m curious what other Java experts (since I&apos;m not) out there feel about COBS in Java . It sounds from Scott&apos;s comment that byte stuffing in Java is a non-starter.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That really depends on the performance requirement.&lt;/p&gt;

&lt;p&gt;If the requirement is to be able to encapsulate data and stream at near Gigabit ethernet speed or teamed Gigabit (~100MB/sec to 200MB/sec), it will get in the way.&lt;br/&gt;
If other things already significantly limit streaming capability then it may not be a large incremental overhead.&lt;br/&gt;
For example, if the Avro serialization process is already going byte-by-byte somewhere else, this could &apos;piggyback&apos; almost for free &amp;#8211; but it would have to be embedded in that other code, in the same loop.&lt;/p&gt;

&lt;p&gt;I also want to highlight that the byte-by-byte streaming in Java can be compared to larger chunk sizes with a fairly simple benchmark to validate (or disprove) my claims that it is slow in comparison.&lt;/p&gt;

&lt;p&gt;The data from LBL is useful.  It should be fairly easy to change that to a larger chunk size and compare on a new JVM.  &lt;/p&gt;

&lt;p&gt;I&apos;ll try to characterize this on my own time this weekend.&lt;/p&gt;</comment>
                            <comment id="12707454" author="tlipcon" created="Fri, 8 May 2009 18:10:35 +0000"  >&lt;p&gt;If the Java performance of byte-by-byte processing is the major issue, is it worth considering native code to optimize this? I don&apos;t generally like using native code, but I feel like it may be worth it if the advantages of COBS are significant enough.&lt;/p&gt;

&lt;p&gt;On a side note, I recently read a paper that added a JVM optimization to really improve element-by-element processing of arrays by automatically eliminating bounds checking. I imagine that would apply here. Unfortunately, basing a system around a JVM that doesn&apos;t exist yet isn&apos;t so wise &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; But down the road this performance issue may be ameliorated.&lt;/p&gt;</comment>
                            <comment id="12707471" author="massie" created="Fri, 8 May 2009 19:02:15 +0000"  >&lt;p&gt;The suspense was just killing me so I had to get some benchmarks myself.  &lt;/p&gt;

&lt;p&gt;Scott, I&apos;ll be interested to see if you have similar results over the weekend.&lt;/p&gt;

&lt;p&gt;I rewrote the LBL code to use ByteBuffers instead of ArrayByteList from the older Apache commons primitives.  The new API looks like...&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; void decode(ByteBuffer src, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; from, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; to, ByteBuffer dest) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; void encode(ByteBuffer src, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; from, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; to, ByteBuffer dest)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I chose ByteBuffers because I didn&apos;t want to realloc new byte arrays but instead operate on the same byte array for each test.  &lt;/p&gt;

&lt;p&gt;My test results are the average of 10 tests run on a 64 MB ByteBuffer running on my MacBook Pro&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  Model Name:	MacBook Pro
  Model Identifier:	MacBookPro5,1
  Processor Name:	Intel Core 2 Duo
  Processor Speed:	2.4 GHz
  Number Of Processors:	1
  Total Number Of Cores:	2
  L2 Cache:	3 MB
  Memory:	4 GB
  Bus Speed:	1.07 GHz
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since my test wasn&apos;t multithreaded... only one core was used.&lt;/p&gt;

&lt;p&gt;My tests verified that the byte array wasn&apos;t altered by the encoding/decoding process (there were no failures).&lt;/p&gt;

&lt;p&gt;These number are meant to be ballpark values since my MacBook was &quot;quiet&quot; during the tests... I was cranking some Radiohead on iTunes.&lt;/p&gt;

&lt;p&gt;One of the factors that can effect the speed of COBS is the number of zeros you need to encode/decode.  In the worse case, you are encoding nothing but zeros.  In that case, you&apos;ll essentially be replace all zeros with ones.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;The results from this worse case (nothing but zeros) are as follows...&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Encoding at 38.22 MB/sec&lt;br/&gt;
Decoding at 17.85 MB/sec&lt;/p&gt;

&lt;p&gt;&lt;b&gt;If we have one zero every 10 bytes...&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Encoding at 57.26 MB/sec&lt;br/&gt;
Decoding at 151.91 MB/sec&lt;/p&gt;

&lt;p&gt;&lt;b&gt;If you have one zero every 100 bytes...&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Encoding at 74.81 MB/sec&lt;br/&gt;
Decoding at 846.56 MB/sec&lt;/p&gt;

&lt;p&gt;&lt;b&gt;If you have one zero every 1000 bytes...&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Encoding at 73.70 MB/sec&lt;br/&gt;
Decoding at 1128.75 MB/sec&lt;/p&gt;

&lt;p&gt;&lt;b&gt;If you have one zero every 10,000 bytes...&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Encoding at 74.40 MB/sec&lt;br/&gt;
Decoding at 1118.88 MB/sec&lt;/p&gt;

&lt;p&gt;&lt;b&gt;If you have no zeros at all...&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Encoding at 73.98 MB/sec&lt;br/&gt;
Decoding at 1151.08 MB/sec&lt;/p&gt;

&lt;p&gt;So it looks to me like... even with native Java code... we&apos;ll be able to push ~100MB/sec - 200MB/sec... (except for the worse case where we have 64MB of zeros).&lt;/p&gt;

&lt;p&gt;I&apos;ll post my code to this Jira so others can point and laugh.  &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12707475" author="massie" created="Fri, 8 May 2009 19:08:52 +0000"  >&lt;p&gt;This is the Java code that I used for my benchmarks of COBS encoding/decoding&lt;/p&gt;</comment>
                            <comment id="12707478" author="massie" created="Fri, 8 May 2009 19:13:00 +0000"  >&lt;p&gt;Sorry for spamming so many comments here.&lt;/p&gt;

&lt;p&gt;I forgot to mention that I used the standard JVM 1.5.0 for MacOS for the tests.&lt;/p&gt;</comment>
                            <comment id="12707541" author="tlipcon" created="Fri, 8 May 2009 22:04:21 +0000"  >&lt;p&gt;It turns out the paper I read has been implemented in JDK 7. If someone has this mythical beast installed, it would be very interesting to see the results of Matt&apos;s benchmark code.&lt;/p&gt;

&lt;p&gt;Here&apos;s a link to someone else&apos;s experiences with it:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://lingpipe-blog.com/2009/03/30/jdk-7-twice-as-fast-as-jdk-6-for-arrays-and-arithmetic/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lingpipe-blog.com/2009/03/30/jdk-7-twice-as-fast-as-jdk-6-for-arrays-and-arithmetic/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Whether relying on optimizations only available in a not-yet-released JVM is a good idea is certainly up for debate. Given that Avro is still in its infancy, JDK 7 might be common by the time Avro is in production use.&lt;/p&gt;</comment>
                            <comment id="12708219" author="scott_carey" created="Mon, 11 May 2009 21:03:41 +0000"  >&lt;p&gt;Todd: I think that many of the JDK 7 enhancements have been backported to JDK 1.6.0_u14.  I&apos;ll run some experiments later.&lt;/p&gt;


&lt;p&gt;Matt:&lt;br/&gt;
Great stuff!  Your results make sense to me based on previous experience.  I went and made some modifications myself to try out doing this 4 bytes at a time.&lt;/p&gt;

&lt;p&gt;Unfortunately, this just made things more confusing for now.  &lt;/p&gt;

&lt;p&gt;First, on your results:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;75MB/sec is somewhat slow.  If anything else is roughly as expensive (say, the Avro serialization itself) then the max rate one client can encode and stream to another will be ~half that.  The decode rate is good.&lt;/li&gt;
	&lt;li&gt;As a microbenchmark of sorts, we&apos;ll want to make sure the JVM warms up, run an iteration or two of the test, garbage collect, then measure.&lt;/li&gt;
	&lt;li&gt;Apple&apos;s JVM is going to be a bit off.  I&apos;ll run some tests on a Linux server with Sun&apos;s JVM later, and try it with the 1.6.0_14 improvements as well.&lt;/li&gt;
	&lt;li&gt;There is a bug &amp;#8211; the max interval between 0 byte occurances is 256 &amp;#8211; which is probably why the results behaved like they did.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I ran the same tests on my machine using Apple&apos;s 1.5 JVM with similar results.  With Apple&apos;s (64 bit) 1.6 JVM, the results are much higher.&lt;/p&gt;

&lt;p&gt;One 0 byte per 1000 (actually less due to the bug).&lt;br/&gt;
Encoding at 224.48262 MB/sec&lt;br/&gt;
Decoding at 1233.1406 MB/sec&lt;/p&gt;

&lt;p&gt;All 0 bytes:&lt;br/&gt;
Encoding at 122.69939 MB/sec&lt;br/&gt;
Decoding at 62.184223 MB/sec&lt;/p&gt;

&lt;p&gt;one in 10 0&apos;s:&lt;br/&gt;
Encoding at 143.20877 MB/sec&lt;br/&gt;
Decoding at 405.06326 MB/sec&lt;/p&gt;

&lt;p&gt;So there is quite the potential for the latest Sun JVM to be fast ... or slow.&lt;/p&gt;

&lt;p&gt;I wrote a &quot;COWSCodec&quot; to try this out with 4 byte chunks.  The initial encoding results were good ... up to 300MB/sec with all 0 bytes.&lt;br/&gt;
However, that implementation uses ByteBuffer.asIntBuffer().  And those IntBuffer views do not support the .array() method, so I had to use the IntBuffer.put(IntBuffer) signature for bulk copies.&lt;br/&gt;
To do that cleanly, it made most sense to refactor the whole thing to use Java nio.Buffer style method signatures (set position, limit before a copy, use mark(), flip(), etc).  After doing so, it turns out that the IntBuffer views created by ByteBuffer.asIntBuffer do not really support bulk get/put operations.  The max decode speed is about 420MB/sec.&lt;/p&gt;

&lt;p&gt;So, there is one other way to do larger chunk encodings out of a ByteBuffer source and destination &amp;#8211; use the ByteBuffer.getInt() and raw copy stuff rather than an intermediate IntBuffer wrapper.  &lt;br/&gt;
I can also test out a &apos;real&apos; IntBuffer which is backed by an int[] rather than a byte[] which should be the fastest &amp;#8211; but not applicable to reading/writing from network or file.&lt;/p&gt;

&lt;p&gt;Both of those should be fairly simple &amp;#8211; I&apos;ll clean up what I have, add that stuff, and put it up here in a day or two.&lt;br/&gt;
Linux tests and variations with the latest/greatest JVM will be informative as well.&lt;/p&gt;</comment>
                            <comment id="12708244" author="cutting" created="Mon, 11 May 2009 22:08:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m imagining a situation where you have part of an Avro Object container file minus the header/footer metablock because of data loss or subscribing to a data stream in &quot;real-time&quot; midstream.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But metainfo is required to make sense of the stream.  You need its schema, codec, etc.  Getting the sync marker doesn&apos;t seem a huge burden on top of that, unless you&apos;re figuring you&apos;d skip to the next metadata flush before you try to make sense of the stream?  How critical is this streaming-without-metadata use case?  If it becomes an important use case, we might define a streaming-specific container, or use RTSP or somesuch, rather than using the existing container file format at all.&lt;/p&gt;

&lt;p&gt;Not that this isn&apos;t an interesting area, but I&apos;d much more interested in, e.g., gzip and lzf compression codecs for Avro&apos;s file format, or Avro InputFormat and OutputFormat&apos;s for mapreduce, or perhaps a version of Dumbo that uses the Pipes protocol to more efficiently get complex Avro data in and out of Python programs, etc.&lt;/p&gt;</comment>
                            <comment id="12712848" author="scott_carey" created="Tue, 26 May 2009 04:30:10 +0000"  >&lt;p&gt;Test COBS / COWS / COLS codecs.  First batch of files.  These three files are described as follows:&lt;/p&gt;

&lt;p&gt;COBSCodec2.java  &amp;#8211; minor modification of the previous version for an improved testing loop.  Also modified to test in batch with the other new additions.&lt;/p&gt;

&lt;p&gt;COWSCodec.java  &amp;#8211; first, hack-ish version of a COBS-like encoding that works in 4 byte chunks.  This version uses ByteBuffer.asIntBuffer(), and does all copies with the default nio &apos;copy from position() to limit()&apos; behavior.  This turns out to be slow.  asIntBuffer does not have optimal copy operation as can be seen in the slow decode.&lt;/p&gt;

&lt;p&gt;COWSCodec2.java &amp;#8211; re-implimented using ByteBuffer.getInt() and putInt().  Significantly faster.&lt;/p&gt;

&lt;p&gt;Three more files after this and a set of benchmarks on Linux with recent JRE&apos;s.&lt;/p&gt;

&lt;p&gt;The point of all this is experimentation and optimization.  Although this specific JIRA may not become relevant &amp;#8211; the results of this investigation may be useful in other contexts as well.&lt;/p&gt;</comment>
                            <comment id="12712849" author="scott_carey" created="Tue, 26 May 2009 04:35:11 +0000"  >&lt;p&gt;COWSCodec3.java  &amp;#8211; Slightly more optimized and cleaner version of COWSCodec2.&lt;br/&gt;
COLSCodec.java &amp;#8211; A version that encodes with 8 byte chunks using ByteBuffer getLong() and putLong().  &lt;/p&gt;

&lt;p&gt;The above two have at least one minor bug left but the performance experiment should still be valid (there is a case were the decoded output can be 1 word too large).  Also, these don&apos;t yet work with encoding or decoding streams that are not even multiples of 4 and 8 bytes.&lt;/p&gt;

&lt;p&gt;COBSPerfTest.java &amp;#8211; a class for executing a test against all the variants in one go, with various ratios of zero words.  Used for performance results that I&apos;ll post later.&lt;/p&gt;</comment>
                            <comment id="12712858" author="scott_carey" created="Tue, 26 May 2009 05:09:46 +0000"  >&lt;p&gt;Performance results using COBSPerfTest on some JVM / OS / Hardware combinations.&lt;/p&gt;

&lt;p&gt;First, an overview:&lt;br/&gt;
The 64 bit JRE on MacOS X has roughly similar performance characteristics in these tests to the Linux Sun JRE 1.6.0_12.   The Mac OSX 32 bit 1.5 JRE is vastly different.&lt;br/&gt;
A 32 bit JVM is slightly faster than a 64 bit JVM on the byte-by-byte work, roghly the same at 4 byte at a time work, and slower at 8 byte at a time work.  This is mostly expected.&lt;br/&gt;
Variations in VM from Sun 1.6.0_12 through a few early access versions of 1.6.0_14 have roghly the same performance.  That is, the performance improvements in the latest JRE (of which, there are many) don&apos;t seem to have an impact here.&lt;/p&gt;

&lt;p&gt;Larger byte chunks help decoding only a little unless zero words dominate, and then it helps a lot.&lt;br/&gt;
Larger chunks help encoding significantly across the board.  COLS &amp;#8211; working with 8 byte chunks &amp;#8211; is about 4x faster than COBS.&lt;/p&gt;

&lt;p&gt;The results below could use some formatting work &amp;#8211; it is very verbose.&lt;br/&gt;
Al results with Centos 5.3&lt;br/&gt;
Xeon 5335 is 2.0Ghz, 4MB cache per pair of cores, 2x quad core&lt;br/&gt;
Xeon E5440 is 2.83Ghz, 6MB cache per pair of cores, 2x quad core&lt;/p&gt;

&lt;p&gt;Results have the following headers if you wish to search:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;1.6.0_13 Xeon 5335 defaults&lt;/li&gt;
	&lt;li&gt;1.6.0_14b03 Xeon 5335 defaults&lt;/li&gt;
	&lt;li&gt;1.6.0_14b03 Xeon 5335 compressed pointers, escape analysis&lt;/li&gt;
	&lt;li&gt;1.6.0_14b06 32 bit Xeon 5335 defaults&lt;/li&gt;
	&lt;li&gt;1.6.0_12 Xeon E5440 defaults&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Results:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;1.6.0_13 Xeon 5335 defaults&lt;br/&gt;
$ /usr/java/jdk1.6.0_13/bin/java -server -Xmx512m -jar COBSPerfTest.jar &lt;br/&gt;
COBSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 89.13659 MB/sec&lt;br/&gt;
Decoding at 57.16702 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 69.81948 MB/sec&lt;br/&gt;
Decoding at 208.78065 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 100 words&lt;br/&gt;
Encoding at 144.56085 MB/sec&lt;br/&gt;
Decoding at 925.55365 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 1000 words&lt;br/&gt;
Encoding at 155.95493 MB/sec&lt;br/&gt;
Decoding at 1033.0511 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 157.28098 MB/sec&lt;br/&gt;
Decoding at 1038.535 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 1 words&lt;br/&gt;
Encoding at 248.32587 MB/sec&lt;br/&gt;
Decoding at 272.1762 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 10 words&lt;br/&gt;
Encoding at 158.03842 MB/sec&lt;br/&gt;
Decoding at 244.39633 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 100 words&lt;br/&gt;
Encoding at 178.13342 MB/sec&lt;br/&gt;
Decoding at 314.68652 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 1000 words&lt;br/&gt;
Encoding at 179.34563 MB/sec&lt;br/&gt;
Decoding at 319.3074 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 10000 words&lt;br/&gt;
Encoding at 179.1614 MB/sec&lt;br/&gt;
Decoding at 317.2071 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 100000 words&lt;br/&gt;
Encoding at 179.04832 MB/sec&lt;br/&gt;
Decoding at 318.77673 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 1 words&lt;br/&gt;
Encoding at 212.54866 MB/sec&lt;br/&gt;
Decoding at 239.11534 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 10 words&lt;br/&gt;
Encoding at 225.34329 MB/sec&lt;br/&gt;
Decoding at 466.0846 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 100 words&lt;br/&gt;
Encoding at 323.6535 MB/sec&lt;br/&gt;
Decoding at 1133.9556 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 1000 words&lt;br/&gt;
Encoding at 329.2162 MB/sec&lt;br/&gt;
Decoding at 1204.8298 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 10000 words&lt;br/&gt;
Encoding at 328.59866 MB/sec&lt;br/&gt;
Decoding at 1212.3876 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 100000 words&lt;br/&gt;
Encoding at 328.2159 MB/sec&lt;br/&gt;
Decoding at 1205.3972 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 1 words&lt;br/&gt;
Encoding at 224.24924 MB/sec&lt;br/&gt;
Decoding at 252.21263 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 10 words&lt;br/&gt;
Encoding at 235.53137 MB/sec&lt;br/&gt;
Decoding at 506.0203 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 100 words&lt;br/&gt;
Encoding at 320.96054 MB/sec&lt;br/&gt;
Decoding at 1094.6545 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 1000 words&lt;br/&gt;
Encoding at 328.45444 MB/sec&lt;br/&gt;
Decoding at 1213.4741 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 10000 words&lt;br/&gt;
Encoding at 328.3331 MB/sec&lt;br/&gt;
Decoding at 1231.2334 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 100000 words&lt;br/&gt;
Encoding at 328.1387 MB/sec&lt;br/&gt;
Decoding at 1217.2268 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 291.29678 MB/sec&lt;br/&gt;
Decoding at 343.89276 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 354.09015 MB/sec&lt;br/&gt;
Decoding at 812.4928 MB/sec&lt;br/&gt;
Original array was modified!&lt;br/&gt;
COLSCodec, one zero word every 100 words&lt;br/&gt;
Encoding at 433.4998 MB/sec&lt;br/&gt;
Decoding at 1204.3855 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000 words&lt;br/&gt;
Encoding at 423.8553 MB/sec&lt;br/&gt;
Decoding at 1237.3381 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 421.88364 MB/sec&lt;br/&gt;
Decoding at 1238.6761 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 100000 words&lt;br/&gt;
Encoding at 419.33118 MB/sec&lt;br/&gt;
Decoding at 1239.0199 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000000 words&lt;br/&gt;
Encoding at 420.57434 MB/sec&lt;br/&gt;
Decoding at 1218.9686 MB/sec&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;1.6.0_14b03 Xeon 5335 defaults&lt;br/&gt;
$ java -server -Xmx512m -jar COBSPerfTest.jar &lt;br/&gt;
COBSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 91.95477 MB/sec&lt;br/&gt;
Decoding at 57.01143 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 73.45933 MB/sec&lt;br/&gt;
Decoding at 207.67142 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 100 words&lt;br/&gt;
Encoding at 144.0236 MB/sec&lt;br/&gt;
Decoding at 913.1517 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 1000 words&lt;br/&gt;
Encoding at 155.32053 MB/sec&lt;br/&gt;
Decoding at 1032.9912 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 156.60835 MB/sec&lt;br/&gt;
Decoding at 1024.763 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 1 words&lt;br/&gt;
Encoding at 271.9177 MB/sec&lt;br/&gt;
Decoding at 276.57822 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 10 words&lt;br/&gt;
Encoding at 152.21716 MB/sec&lt;br/&gt;
Decoding at 191.38951 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 100 words&lt;br/&gt;
Encoding at 171.42383 MB/sec&lt;br/&gt;
Decoding at 224.81892 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 1000 words&lt;br/&gt;
Encoding at 173.3674 MB/sec&lt;br/&gt;
Decoding at 228.82373 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 10000 words&lt;br/&gt;
Encoding at 173.56622 MB/sec&lt;br/&gt;
Decoding at 228.35956 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 100000 words&lt;br/&gt;
Encoding at 173.60176 MB/sec&lt;br/&gt;
Decoding at 229.12196 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 1 words&lt;br/&gt;
Encoding at 214.48987 MB/sec&lt;br/&gt;
Decoding at 241.93507 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 10 words&lt;br/&gt;
Encoding at 244.36378 MB/sec&lt;br/&gt;
Decoding at 473.0567 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 100 words&lt;br/&gt;
Encoding at 345.88748 MB/sec&lt;br/&gt;
Decoding at 1003.376 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 1000 words&lt;br/&gt;
Encoding at 349.1008 MB/sec&lt;br/&gt;
Decoding at 1026.7786 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 10000 words&lt;br/&gt;
Encoding at 347.61612 MB/sec&lt;br/&gt;
Decoding at 1028.8761 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 100000 words&lt;br/&gt;
Encoding at 346.9563 MB/sec&lt;br/&gt;
Decoding at 1061.9762 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 1 words&lt;br/&gt;
Encoding at 210.84114 MB/sec&lt;br/&gt;
Decoding at 258.27982 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 10 words&lt;br/&gt;
Encoding at 252.59242 MB/sec&lt;br/&gt;
Decoding at 507.0884 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 100 words&lt;br/&gt;
Encoding at 353.3254 MB/sec&lt;br/&gt;
Decoding at 1150.1593 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 1000 words&lt;br/&gt;
Encoding at 358.27298 MB/sec&lt;br/&gt;
Decoding at 1208.8944 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 10000 words&lt;br/&gt;
Encoding at 357.32245 MB/sec&lt;br/&gt;
Decoding at 1215.5607 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 100000 words&lt;br/&gt;
Encoding at 356.93134 MB/sec&lt;br/&gt;
Decoding at 1210.7133 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 287.6796 MB/sec&lt;br/&gt;
Decoding at 362.4284 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 349.48486 MB/sec&lt;br/&gt;
Decoding at 817.0665 MB/sec&lt;br/&gt;
Original array was modified!&lt;br/&gt;
COLSCodec, one zero word every 100 words&lt;br/&gt;
Encoding at 418.7336 MB/sec&lt;br/&gt;
Decoding at 1214.5057 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000 words&lt;br/&gt;
Encoding at 410.76407 MB/sec&lt;br/&gt;
Decoding at 1239.2533 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 408.02432 MB/sec&lt;br/&gt;
Decoding at 1245.9232 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 100000 words&lt;br/&gt;
Encoding at 406.2959 MB/sec&lt;br/&gt;
Decoding at 1252.01 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000000 words&lt;br/&gt;
Encoding at 405.99057 MB/sec&lt;br/&gt;
Decoding at 1252.2338 MB/sec&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;1.6.0_14b03 Xeon 5335 compressed pointers, escape analysis&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;candiru@britney COBSPerfTest&amp;#93;&lt;/span&gt;$ java -server -Xmx512m -XX:+DoEscapeAnalysis -XX:+UseCompressedOops -jar COBSPerfTest.jar &lt;br/&gt;
COBSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 91.98761 MB/sec&lt;br/&gt;
Decoding at 53.635868 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 72.98973 MB/sec&lt;br/&gt;
Decoding at 205.35959 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 100 words&lt;br/&gt;
Encoding at 144.04861 MB/sec&lt;br/&gt;
Decoding at 918.5997 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 1000 words&lt;br/&gt;
Encoding at 154.981 MB/sec&lt;br/&gt;
Decoding at 1018.9709 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 156.41275 MB/sec&lt;br/&gt;
Decoding at 1032.3058 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 1 words&lt;br/&gt;
Encoding at 252.68245 MB/sec&lt;br/&gt;
Decoding at 307.39664 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 10 words&lt;br/&gt;
Encoding at 163.27182 MB/sec&lt;br/&gt;
Decoding at 209.55176 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 100 words&lt;br/&gt;
Encoding at 189.6774 MB/sec&lt;br/&gt;
Decoding at 263.66977 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 1000 words&lt;br/&gt;
Encoding at 193.37485 MB/sec&lt;br/&gt;
Decoding at 270.99658 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 10000 words&lt;br/&gt;
Encoding at 193.74573 MB/sec&lt;br/&gt;
Decoding at 271.46988 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 100000 words&lt;br/&gt;
Encoding at 194.11456 MB/sec&lt;br/&gt;
Decoding at 270.73804 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 1 words&lt;br/&gt;
Encoding at 216.82019 MB/sec&lt;br/&gt;
Decoding at 243.21117 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 10 words&lt;br/&gt;
Encoding at 242.51544 MB/sec&lt;br/&gt;
Decoding at 465.20282 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 100 words&lt;br/&gt;
Encoding at 344.99945 MB/sec&lt;br/&gt;
Decoding at 1157.6014 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 1000 words&lt;br/&gt;
Encoding at 351.1931 MB/sec&lt;br/&gt;
Decoding at 1211.4054 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 10000 words&lt;br/&gt;
Encoding at 349.90894 MB/sec&lt;br/&gt;
Decoding at 1217.9989 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 100000 words&lt;br/&gt;
Encoding at 349.40396 MB/sec&lt;br/&gt;
Decoding at 1210.6339 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 1 words&lt;br/&gt;
Encoding at 240.06367 MB/sec&lt;br/&gt;
Decoding at 228.17952 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 10 words&lt;br/&gt;
Encoding at 255.28317 MB/sec&lt;br/&gt;
Decoding at 496.779 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 100 words&lt;br/&gt;
Encoding at 360.55945 MB/sec&lt;br/&gt;
Decoding at 1142.717 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 1000 words&lt;br/&gt;
Encoding at 365.1012 MB/sec&lt;br/&gt;
Decoding at 1205.8257 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 10000 words&lt;br/&gt;
Encoding at 363.70743 MB/sec&lt;br/&gt;
Decoding at 1213.5723 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 100000 words&lt;br/&gt;
Encoding at 363.2405 MB/sec&lt;br/&gt;
Decoding at 1208.7316 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 298.33194 MB/sec&lt;br/&gt;
Decoding at 318.14648 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 368.6357 MB/sec&lt;br/&gt;
Decoding at 825.8583 MB/sec&lt;br/&gt;
Original array was modified!&lt;br/&gt;
COLSCodec, one zero word every 100 words&lt;br/&gt;
Encoding at 449.0997 MB/sec&lt;br/&gt;
Decoding at 1191.9662 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000 words&lt;br/&gt;
Encoding at 441.75586 MB/sec&lt;br/&gt;
Decoding at 1223.806 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 439.18317 MB/sec&lt;br/&gt;
Decoding at 1227.0127 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 100000 words&lt;br/&gt;
Encoding at 438.62714 MB/sec&lt;br/&gt;
Decoding at 1224.557 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000000 words&lt;br/&gt;
Encoding at 438.62115 MB/sec&lt;br/&gt;
Decoding at 1224.6772 MB/sec&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;1.6.0_14b06 32 bit Xeon 5335 defaults&lt;br/&gt;
$ /usr/java/jdk1.6.0_14ea6_32bit/bin/java -server -Xmx512m -jar COBSPerfTest.jar COBSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 101.488785 MB/sec&lt;br/&gt;
Decoding at 44.9381 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 76.98102 MB/sec&lt;br/&gt;
Decoding at 186.26143 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 100 words&lt;br/&gt;
Encoding at 154.48914 MB/sec&lt;br/&gt;
Decoding at 926.46204 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 1000 words&lt;br/&gt;
Encoding at 169.65015 MB/sec&lt;br/&gt;
Decoding at 996.02625 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 171.83167 MB/sec&lt;br/&gt;
Decoding at 1069.7236 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 1 words&lt;br/&gt;
Encoding at 229.62816 MB/sec&lt;br/&gt;
Decoding at 347.4478 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 10 words&lt;br/&gt;
Encoding at 137.4511 MB/sec&lt;br/&gt;
Decoding at 181.96013 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 100 words&lt;br/&gt;
Encoding at 170.84563 MB/sec&lt;br/&gt;
Decoding at 246.61394 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 1000 words&lt;br/&gt;
Encoding at 175.59972 MB/sec&lt;br/&gt;
Decoding at 255.19583 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 10000 words&lt;br/&gt;
Encoding at 176.94963 MB/sec&lt;br/&gt;
Decoding at 257.768 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 100000 words&lt;br/&gt;
Encoding at 175.58342 MB/sec&lt;br/&gt;
Decoding at 255.8668 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 1 words&lt;br/&gt;
Encoding at 212.1405 MB/sec&lt;br/&gt;
Decoding at 257.78635 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 10 words&lt;br/&gt;
Encoding at 231.08081 MB/sec&lt;br/&gt;
Decoding at 421.9081 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 100 words&lt;br/&gt;
Encoding at 348.02103 MB/sec&lt;br/&gt;
Decoding at 1133.5847 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 1000 words&lt;br/&gt;
Encoding at 358.29077 MB/sec&lt;br/&gt;
Decoding at 1170.7545 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 10000 words&lt;br/&gt;
Encoding at 360.5535 MB/sec&lt;br/&gt;
Decoding at 1223.9012 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 100000 words&lt;br/&gt;
Encoding at 358.03394 MB/sec&lt;br/&gt;
Decoding at 1216.9368 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 1 words&lt;br/&gt;
Encoding at 226.55222 MB/sec&lt;br/&gt;
Decoding at 275.24838 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 10 words&lt;br/&gt;
Encoding at 243.09453 MB/sec&lt;br/&gt;
Decoding at 469.97775 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 100 words&lt;br/&gt;
Encoding at 351.21555 MB/sec&lt;br/&gt;
Decoding at 1129.5447 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 1000 words&lt;br/&gt;
Encoding at 358.14252 MB/sec&lt;br/&gt;
Decoding at 1196.7433 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 10000 words&lt;br/&gt;
Encoding at 360.71323 MB/sec&lt;br/&gt;
Decoding at 1199.4408 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 100000 words&lt;br/&gt;
Encoding at 358.2802 MB/sec&lt;br/&gt;
Decoding at 1224.6678 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 208.82603 MB/sec&lt;br/&gt;
Decoding at 275.9128 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 265.03033 MB/sec&lt;br/&gt;
Decoding at 730.78546 MB/sec&lt;br/&gt;
Original array was modified!&lt;br/&gt;
COLSCodec, one zero word every 100 words&lt;br/&gt;
Encoding at 310.9054 MB/sec&lt;br/&gt;
Decoding at 1157.1534 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000 words&lt;br/&gt;
Encoding at 308.7317 MB/sec&lt;br/&gt;
Decoding at 1238.4891 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 306.90793 MB/sec&lt;br/&gt;
Decoding at 1220.6907 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 100000 words&lt;br/&gt;
Encoding at 305.49704 MB/sec&lt;br/&gt;
Decoding at 1205.0568 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000000 words&lt;br/&gt;
Encoding at 305.3674 MB/sec&lt;br/&gt;
Decoding at 1234.8855 MB/sec&lt;/li&gt;
&lt;/ul&gt;



&lt;ul&gt;
	&lt;li&gt;1.6.0_12 Xeon E5440 defaults&lt;br/&gt;
$ java -server -Xmx512m -jar COBSPerfTest.jar &lt;br/&gt;
COBSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 124.19903 MB/sec&lt;br/&gt;
Decoding at 80.51218 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 97.80887 MB/sec&lt;br/&gt;
Decoding at 293.82983 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 100 words&lt;br/&gt;
Encoding at 203.51627 MB/sec&lt;br/&gt;
Decoding at 1299.7317 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 1000 words&lt;br/&gt;
Encoding at 219.41322 MB/sec&lt;br/&gt;
Decoding at 1422.3486 MB/sec&lt;br/&gt;
COBSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 220.89801 MB/sec&lt;br/&gt;
Decoding at 1420.3978 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 1 words&lt;br/&gt;
Encoding at 344.6565 MB/sec&lt;br/&gt;
Decoding at 390.2233 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 10 words&lt;br/&gt;
Encoding at 220.47774 MB/sec&lt;br/&gt;
Decoding at 360.3579 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 100 words&lt;br/&gt;
Encoding at 250.53049 MB/sec&lt;br/&gt;
Decoding at 447.04602 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 1000 words&lt;br/&gt;
Encoding at 253.66922 MB/sec&lt;br/&gt;
Decoding at 450.34372 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 10000 words&lt;br/&gt;
Encoding at 253.64081 MB/sec&lt;br/&gt;
Decoding at 447.10074 MB/sec&lt;br/&gt;
COWSCodec variant 1, one zero word every 100000 words&lt;br/&gt;
Encoding at 252.63756 MB/sec&lt;br/&gt;
Decoding at 447.50485 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 1 words&lt;br/&gt;
Encoding at 275.47418 MB/sec&lt;br/&gt;
Decoding at 332.2978 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 10 words&lt;br/&gt;
Encoding at 316.82657 MB/sec&lt;br/&gt;
Decoding at 657.1525 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 100 words&lt;br/&gt;
Encoding at 449.77597 MB/sec&lt;br/&gt;
Decoding at 1545.4358 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 1000 words&lt;br/&gt;
Encoding at 457.52542 MB/sec&lt;br/&gt;
Decoding at 1653.704 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 10000 words&lt;br/&gt;
Encoding at 456.66467 MB/sec&lt;br/&gt;
Decoding at 1658.9537 MB/sec&lt;br/&gt;
COWSCodec variant 2, one zero word every 100000 words&lt;br/&gt;
Encoding at 455.9669 MB/sec&lt;br/&gt;
Decoding at 1655.1809 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 1 words&lt;br/&gt;
Encoding at 315.7178 MB/sec&lt;br/&gt;
Decoding at 360.02884 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 10 words&lt;br/&gt;
Encoding at 331.1007 MB/sec&lt;br/&gt;
Decoding at 723.18024 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 100 words&lt;br/&gt;
Encoding at 443.8783 MB/sec&lt;br/&gt;
Decoding at 1560.0219 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 1000 words&lt;br/&gt;
Encoding at 447.92645 MB/sec&lt;br/&gt;
Decoding at 1541.4951 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 10000 words&lt;br/&gt;
Encoding at 449.71402 MB/sec&lt;br/&gt;
Decoding at 1394.1431 MB/sec&lt;br/&gt;
COWSCodec variant 3, one zero word every 100000 words&lt;br/&gt;
Encoding at 441.31396 MB/sec&lt;br/&gt;
Decoding at 1361.6113 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 405.91794 MB/sec&lt;br/&gt;
Decoding at 482.06976 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 491.71738 MB/sec&lt;br/&gt;
Decoding at 1079.3405 MB/sec&lt;br/&gt;
Original array was modified!&lt;br/&gt;
COLSCodec, one zero word every 100 words&lt;br/&gt;
Encoding at 598.31836 MB/sec&lt;br/&gt;
Decoding at 1616.031 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000 words&lt;br/&gt;
Encoding at 586.9973 MB/sec&lt;br/&gt;
Decoding at 1666.9445 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 585.7841 MB/sec&lt;br/&gt;
Decoding at 1674.5248 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 100000 words&lt;br/&gt;
Encoding at 585.28375 MB/sec&lt;br/&gt;
Decoding at 1664.8573 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000000 words&lt;br/&gt;
Encoding at 585.0993 MB/sec&lt;br/&gt;
Decoding at 1662.1304 MB/sec&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="12712862" author="tlipcon" created="Tue, 26 May 2009 05:16:57 +0000"  >&lt;p&gt;What&apos;s with this?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;COLSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 354.09015 MB/sec&lt;br/&gt;
Decoding at 812.4928 MB/sec&lt;br/&gt;
Original array was modified!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;isn&apos;t that bad? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12712868" author="scott_carey" created="Tue, 26 May 2009 05:50:22 +0000"  >&lt;blockquote&gt;
&lt;p&gt;COLSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 354.09015 MB/sec&lt;br/&gt;
Decoding at 812.4928 MB/sec&lt;br/&gt;
Original array was modified!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That Sir, is the remaining bug I alluded to but didn&apos;t highlight enough in my previous comment.  If you change the size of the array, the random number seed, or just about anything else it will go away (or pop up elsewhere).&lt;/p&gt;

&lt;p&gt;The before and after arrays have the same bytes, but the one that was encoded and decoded has an extra word at the end.  I stepped through that case briefly, but was too lazy to fix it.  I don&apos;t think it is relevant to the overall results.  (and any real Codec would be written cleaner, with plenty of unit tests to cover the corner cases).&lt;/p&gt;


&lt;p&gt;Which reminds me, these are the main conclusions I draw not specific to this JIRA:&lt;/p&gt;

&lt;p&gt;ByteBuffer.getInt() , getLong(), are rather optimized, as are the matching putInt() and putLong() operations.   Bulk put operations are also fast on ByteBuffer, but not IntBuffer if created from ByteBuffer.asIntBuffer().&lt;/p&gt;

&lt;p&gt;Any encoder or decoder in Java will see potentially large performance gains if it can read / write in larger chunks.&lt;/p&gt;

&lt;p&gt;I could be evil and try the same test and misalign the array &amp;#8211; start at position 1 instead of 0 (the JVM aligns array data to 8 byte boundaries, and many processor instructions are faster if aligned).&lt;/p&gt;

&lt;p&gt;Ok, I decided to be evil and try it on my laptop with misaligned bytes (added a put(0) to the start of the encoder and a get() to the start of the decoder, to misalign the whole thing by a byte).  Now, perhaps getLong() will be a lot less efficient.  Lets see:&lt;/p&gt;

&lt;p&gt;Aligned (COLS):&lt;br/&gt;
COLSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 323.87604 MB/sec&lt;br/&gt;
Decoding at 419.4213 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 376.7943 MB/sec&lt;br/&gt;
Decoding at 1041.8271 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 439.01627 MB/sec&lt;br/&gt;
Decoding at 1350.2242 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000000 words&lt;br/&gt;
Encoding at 415.91876 MB/sec&lt;br/&gt;
Decoding at 1411.3434 MB/sec&lt;/p&gt;

&lt;p&gt;Misaligned (COLS):&lt;br/&gt;
COLSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 327.0196 MB/sec&lt;br/&gt;
Decoding at 402.65366 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 377.48105 MB/sec&lt;br/&gt;
Decoding at 974.4739 MB/se&lt;br/&gt;
COLSCodec, one zero word every 10000 words&lt;br/&gt;
Encoding at 445.4802 MB/sec&lt;br/&gt;
Decoding at 1440.7946 MB/s&lt;br/&gt;
COLSCodec, one zero word every 1000000 words&lt;br/&gt;
Encoding at 443.61166 MB/sec&lt;br/&gt;
Decoding at 1423.9922 MB/sec&lt;/p&gt;

&lt;p&gt;These are within the usual margin of error, and essentially the same.  Perhaps the JVM&apos;s JIT isn&apos;t smart enough to recognize that in the first case, all access is aligned and use the processor load instructions for aligned access which are faster?  I could write a COLSCodec2 that operated on LongBuffer rather than ByteBuffer to see what that does.&lt;/p&gt;

&lt;p&gt;But the main conclusion is that accessing in larger chunks has big gains when it is possible to do.&lt;/p&gt;</comment>
                            <comment id="12712871" author="scott_carey" created="Tue, 26 May 2009 06:16:34 +0000"  >&lt;p&gt;So, aligned access is important &amp;#8211;  However, the JVM &apos;s JIT can not guarantee it on a ByteBuffer or byte[], but can on a LongBuffer or long[].  Here are results on my laptop akin to the above, but with a COLSCodec2 that uses a LongBuffer rather than a ByteBuffer + getLong()/putLong().&lt;/p&gt;

&lt;p&gt;COLSCodec, one zero word every 1 words&lt;br/&gt;
Encoding at 939.8201 MB/sec&lt;br/&gt;
Decoding at 980.54034 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 10 words&lt;br/&gt;
Encoding at 822.7025 MB/sec&lt;br/&gt;
Decoding at 1188.7073 MB/sec&lt;br/&gt;
COLSCodec, one zero word every 1000 words&lt;br/&gt;
Encoding at 1104.4512 MB/sec&lt;br/&gt;
Decoding at 1429.9589 MB/sec&lt;/p&gt;


&lt;p&gt;Unfortunately, for anything reading/writing from the network or a file, byte streams and arrays are the only option.  And as demonstrated before, asLongBuffer or asIntBuffer is not optimized and fairly restrictive.  This seems to indicate that in the future, there is more that the JVM can do, or there are Java APIs that could be made so that the JIT can easily detect data alignment and be more efficient.&lt;/p&gt;</comment>
                            <comment id="12723259" author="cutting" created="Tue, 23 Jun 2009 19:25:45 +0000"  >&lt;p&gt;I don&apos;t think adding this is worthwhile pursuing at this point.  While having nice properties, this inserts a non-negligible decoding operation to all data file processing.&lt;/p&gt;

&lt;p&gt;We can potentially add this to a future file format, but for the file format specfied in the 1.0 release I&apos;d like to keep it as-is.  Objections?&lt;/p&gt;</comment>
                            <comment id="12723273" author="scott_carey" created="Tue, 23 Jun 2009 19:57:48 +0000"  >&lt;p&gt;I agree, COBS-like encoding is only useful for streaming data where a specific character or word must be avoided which is a format issue.&lt;/p&gt;

&lt;p&gt;If all that is needed is identifying block boundaries, there are other methods.&lt;/p&gt;

&lt;p&gt;A &quot;magic number&quot; approach can be collision proof by detecting the collision:  On encode, look for the magic number and if present, follow it with a &apos;not at the end of the block&apos; word; at the end of the block place the magic number and a &apos;end of block&apos; word.  On decode look for the magic number and discard the following word, if the following word is the end of block word also discard the magic word.  COBS came about because the worst case scenario for a magic word approach is poor, and if the size of the magic word is small (one byte) the worst case is likely.&lt;/p&gt;

&lt;p&gt;This might prove very useful at some point.  Some of the general optimization findings here will be useful somewhere.&lt;/p&gt;</comment>
                            <comment id="12723279" author="cutting" created="Tue, 23 Jun 2009 20:02:38 +0000"  >&lt;p&gt;Resolving this as a something we may implement later.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12407647" name="COBSCodec.java" size="8569" author="massie" created="Fri, 8 May 2009 19:08:52 +0000"/>
                            <attachment id="12408986" name="COBSCodec2.java" size="9797" author="scott_carey" created="Tue, 26 May 2009 04:30:10 +0000"/>
                            <attachment id="12408991" name="COBSPerfTest.java" size="568" author="scott_carey" created="Tue, 26 May 2009 04:35:11 +0000"/>
                            <attachment id="12408990" name="COLSCodec.java" size="8163" author="scott_carey" created="Tue, 26 May 2009 04:35:11 +0000"/>
                            <attachment id="12408997" name="COLSCodec2.java" size="8065" author="scott_carey" created="Tue, 26 May 2009 06:16:34 +0000"/>
                            <attachment id="12408987" name="COWSCodec.java" size="8724" author="scott_carey" created="Tue, 26 May 2009 04:30:10 +0000"/>
                            <attachment id="12408988" name="COWSCodec2.java" size="7992" author="scott_carey" created="Tue, 26 May 2009 04:30:10 +0000"/>
                            <attachment id="12408989" name="COWSCodec3.java" size="7732" author="scott_carey" created="Tue, 26 May 2009 04:35:11 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 7 May 2009 08:02:57 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>94356</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 31 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0dxs7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>79403</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[AVRO-28] python: permit default values for record fields</title>
                <link>https://issues.apache.org/jira/browse/AVRO-28</link>
                <project id="12310911" key="AVRO">Apache Avro</project>
                    <description>&lt;p&gt;Python implementation for &lt;a href=&quot;https://issues.apache.org/jira/browse/AVRO-8&quot; title=&quot;permit default values for record fields&quot; class=&quot;issue-link&quot; data-issue-key=&quot;AVRO-8&quot;&gt;&lt;del&gt;AVRO-8&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="12425067">AVRO-28</key>
            <summary>python: permit default values for record fields</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21140&amp;avatarType=issuetype">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sharadag">Sharad Agarwal</assignee>
                                    <reporter username="sharadag">Sharad Agarwal</reporter>
                        <labels>
                    </labels>
                <created>Mon, 11 May 2009 07:46:51 +0000</created>
                <updated>Tue, 14 Jul 2009 22:50:09 +0000</updated>
                            <resolved>Wed, 1 Jul 2009 16:54:07 +0000</resolved>
                                                    <fixVersion>1.0.0</fixVersion>
                                    <component>python</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12726025" author="sharadag" created="Wed, 1 Jul 2009 11:55:46 +0000"  >&lt;p&gt;This patch:&lt;br/&gt;
allows default values&lt;br/&gt;
implement actual/expected schema resolution&lt;br/&gt;
fixes a bug in java&apos;s GenericDatumReader#resolveExpected&lt;/p&gt;</comment>
                            <comment id="12726026" author="sharadag" created="Wed, 1 Jul 2009 11:56:44 +0000"  >&lt;p&gt;All tests passed.&lt;/p&gt;</comment>
                            <comment id="12726130" author="cutting" created="Wed, 1 Jul 2009 16:54:06 +0000"  >&lt;p&gt;I just committed this.  Thanks, Sharad!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12412262" name="AVRO-28_v1.patch" size="25938" author="sharadag" created="Wed, 1 Jul 2009 11:55:46 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 1 Jul 2009 16:54:06 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>94357</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 30 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0dxsf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>79404</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[AVRO-29] Validation and resolution for ValueInput/ValueOutput</title>
                <link>https://issues.apache.org/jira/browse/AVRO-29</link>
                <project id="12310911" key="AVRO">Apache Avro</project>
                    <description>&lt;p&gt;This is a companion to &lt;a href=&quot;https://issues.apache.org/jira/browse/AVRO-25&quot; title=&quot;Blocking for value output (with API change)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;AVRO-25&quot;&gt;&lt;del&gt;AVRO-25&lt;/del&gt;&lt;/a&gt;, which introduced the classes ValueOutput and ValueInput.  This patch adds two capabilities: validation of ValueInput/Output calls against a schema, and schema-resolution implemented in the context of ValueInput.&lt;/p&gt;

&lt;p&gt;ValidatingValueInput and ValidatingValueOutput take a schema and will validate calls against a schema.  For example, if the schema calls for a record consisting of two longs and a double, then ValidatingOutput will allow the call-sequence readLong, readLong, readDouble and throw an error otherwise.&lt;/p&gt;

&lt;p&gt;ResolvingValueInput takes two schemas, the writer&apos;s and the reader&apos;s schema, and automatically performs Avro&apos;s schema-resolution logic on behalf of the reader.  For example, if the writer&apos;s schema calls for a long, and the readers calls for a double, then the reader can call readDouble, and ResolvingValueInput will automatically decode the long sent by the writer and convert it into the double expected by the reader.&lt;/p&gt;

&lt;p&gt;ResolvingValueInput is an alternative to Avro&apos;s current GenericDatumReader, which also implements Avro&apos;s resolution logic.  In many use-cases, the programmer has their own data structures into which they want to store data read from an Avro stream, data structures that cannot easily be put into the GenericRecord/Array class hierarchy.  With ResolvingValueInput, programmers get the benefit of this resolution logic without being forced into the GenericRecord/Array class hierarchy.&lt;/p&gt;

&lt;p&gt;We recommend that ResolvingValueInput become the standard implementation of the resolution logic, and that GenericDatumReader be implemented in terms of ResolvingValueInput.  However, we haven&apos;t implemented this change pending feedback from others.&lt;/p&gt;

&lt;p&gt;We haven&apos;t implemented default values, but can add that feature.&lt;/p&gt;

&lt;p&gt;Implementation note: this patch is implemented by translating Avro schemas to LL(1) parsing tables.  This translation is straight forward, but tedious.  If you want to understand how the code works, we recommend that you look in the file &quot;parsing.html&quot; (included in the patch), which explains the translation.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12425206">AVRO-29</key>
            <summary>Validation and resolution for ValueInput/ValueOutput</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21140&amp;avatarType=issuetype">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="thiru_mg">Thiruvalluvan M. G.</assignee>
                                    <reporter username="raymie">Raymie Stata</reporter>
                        <labels>
                    </labels>
                <created>Tue, 12 May 2009 12:35:52 +0000</created>
                <updated>Tue, 14 Jul 2009 22:50:09 +0000</updated>
                            <resolved>Thu, 25 Jun 2009 18:53:39 +0000</resolved>
                                                    <fixVersion>1.0.0</fixVersion>
                                    <component>java</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12708491" author="hong.tang" created="Tue, 12 May 2009 16:32:31 +0000"  >&lt;p&gt;Attachment?&lt;/p&gt;</comment>
                            <comment id="12709009" author="thiru_mg" created="Wed, 13 May 2009 17:06:16 +0000"  >&lt;p&gt;I missed a few file in the previous patch.&lt;/p&gt;</comment>
                            <comment id="12712301" author="cutting" created="Fri, 22 May 2009 23:15:48 +0000"  >&lt;p&gt;&amp;gt; ValidatingValueInput and ValidatingValueOutput take a schema and will validate calls against a schema.&lt;/p&gt;

&lt;p&gt;That would be very useful to have for debugging.  If the performance were good enough, one might even use it all the time.&lt;/p&gt;

&lt;p&gt;&amp;gt; With ResolvingValueInput, programmers get the benefit of this resolution logic without being forced into the GenericRecord/Array class hierarchy.&lt;/p&gt;

&lt;p&gt;Much of the need for GenericRecord is around handling field reordering, insertion and removal.  I don&apos;t see how this changes that.  What am I missing?  Does each data representation need to loop over the schema itself, implementing that logic?  Is there an example of how a record is read somewhere?&lt;/p&gt;</comment>
                            <comment id="12720634" author="thiru_mg" created="Wed, 17 Jun 2009 11:51:56 +0000"  >&lt;p&gt;This patch has more complete implementation of the validating and resolving reader/writers. This has been adapted to the recent changes to readers and writers in JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/AVRO-25&quot; title=&quot;Blocking for value output (with API change)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;AVRO-25&quot;&gt;&lt;del&gt;AVRO-25&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The ResolvingValueReader supports the following rules for resolution:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;long can read int.&lt;/li&gt;
	&lt;li&gt;double can read int, long, float&lt;/li&gt;
	&lt;li&gt;Fixed matches only size matchs&lt;/li&gt;
	&lt;li&gt;Enums match symbols (if the writer defines 3 for symbol &quot;x&quot; and reader defines 5 for &quot;x&quot;, then a value 3 in the stream will return 5 to the caller)&lt;/li&gt;
	&lt;li&gt;If writer has a Union, it matches reader&apos;s non-union type if it is one of branches of the writer and data on the stream is of that type.&lt;/li&gt;
	&lt;li&gt;If the writer has a non-union type and reader has a union type with the writer&apos;s type as a branch, reader sees the union with that branch.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;GenericDatumReader is modified to use ValidatingValueReader.&lt;/p&gt;

&lt;p&gt;My performance tests show that using ValidatingValueWriter is about 8% slower than not using it. Using ResolvingValueReader degrades performance from 0% to 8% depending on the kind of &quot;resolution&quot; used. When there is no resolution (i.e. reader and writer schemas are identical), it is functionally equivalent to ValidatingValueReader, the overhead is the maximum. &lt;span class=&quot;error&quot;&gt;&amp;#91;These results, of course cache the resolving table. That is the resolving table is not constructed for every object being decoded&amp;#93;&lt;/span&gt;.&lt;/p&gt;</comment>
                            <comment id="12720921" author="cutting" created="Wed, 17 Jun 2009 21:50:52 +0000"  >&lt;p&gt;Some comments and questions:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;What&apos;s the advantage of using ResovlingRecordReader over the existing code in GenericDatumReader?  This replaces existing logic with a lot more code, code that&apos;s harder to understand, IMHO.  It&apos;s interesting that it can be done this way, but what value does this give applications?  Above Raymie states, &quot;With ResolvingValueInput, programmers get the benefit of this resolution logic without being forced into the GenericRecord/Array class hierarchy.&quot; But applications already are not forced into this.  They simply must override addField(), getField() and removeField(), as with ResolvingRecordReader.  For example, SpecificDatumReader extends GenericDatumReader to read records that do not implement GenericRecord.  What am I missing?&lt;/li&gt;
	&lt;li&gt;Similarly, what&apos;s the advantage of the ValidatingValueReader/Writer?  It feels a bit like belt-and-suspenders.  The existing DatumReader and DatumWriter implementations do not perform invalid operations.  This is tested by unit tests, so why check it again at runtime for every record, even if it can be done relatively cheaply?  We don&apos;t expect there to be thousands of DatumReader and DatumWriter implementations.  All existing ones extend GenericDatumReader/Writer, which should prevent invalid operations, and I have a hard time imagining an application which cannot be reasonably implemented by extending these.  Again, what am I missing?&lt;/li&gt;
	&lt;li&gt;GenericDatumReader: ValidatingValueReader import is not used;&lt;/li&gt;
	&lt;li&gt;GenericDatumReader: It&apos;s unfortunate that this includes two versions of readRecord.&lt;/li&gt;
	&lt;li&gt;Might the resolving/parsing classes would better live in an io.validate package?&lt;/li&gt;
	&lt;li&gt;What should we do with the parsing.html file?  If it&apos;s for developers only, then it could live with the source code (perhaps alongside the .java files), in javadoc or on the wiki.  If we think end-users might be interested, then it should go into the forrest-generated docs.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12721329" author="thiru_mg" created="Thu, 18 Jun 2009 16:17:34 +0000"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The only real use of ValidatingValueReader/Writer is validation. It can be used for testing new class that directly uses ValueReader/Writer objects. Since it is designed as a filter, it can be inserted into the chain to detect any corner-case bugs even in production environments. At best it can be used for diagnostic purposes.&lt;/li&gt;
	&lt;li&gt;There are two versions of readRecord because of the difference in behavior of ResolvingValueReader compared to ValueReader. The ValueReader returns objects in the order of their declaration in the reader&apos;s schema. For ResolvingValueReader could return in a different order depending on writer&apos;s schema. If we can achieve reordering of fields (which is possible with some more effort), then we can get rid of the second version of readRecord(). In fact if reader can expect its contents in the order of its schema and if support for default values is added, all the resolution is internal to the ResolvingValueReader. Any reader can simply read as if the data is serialized according to its schema.&lt;/li&gt;
	&lt;li&gt;The parsing table can be considered as a binary version of schema. (There is some information loss presently, but it can be taken care of). One can define an avro schema that serializes parsing table itself. With that, an RPC can send data along with its schema which a receiver can readily use to resolve against receiver&apos;s schema. This is functionally equivalent to sending the JSON version of schema, but is more efficient. This is particularly useful for scatter/gather kind of RPCs where many receivers receive the same request. The time saved thus could be significant.&lt;/li&gt;
	&lt;li&gt;Once we agree on the usefulness of these classes, we can move them around appropriately.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12721364" author="cutting" created="Thu, 18 Jun 2009 17:34:02 +0000"  >&lt;p&gt;&amp;gt; The parsing table can be considered as a binary version of schema.&lt;/p&gt;

&lt;p&gt;Would this be better than simply defining a schema for schemas, something like the following?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;{&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;record&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;Schema&quot;&lt;/span&gt;
 &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt; : [
  {&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: [&lt;span class=&quot;code-quote&quot;&gt;&quot;Record&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;Map&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;Array&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;Union&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;Bytes&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;&quot;&lt;/span&gt;, ...]}
 ]
}

{&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;record&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;Record&quot;&lt;/span&gt;
 &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt; : [
  {&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt;},
  {&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: {&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;array&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;items&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;Field&quot;&lt;/span&gt;}}
 ]
}

{&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;record&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;Field&quot;&lt;/span&gt;
 &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt; : [
    {&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt;},
    {&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;schema&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;Schema&quot;&lt;/span&gt;}
}

{&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;record&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;Array&quot;&lt;/span&gt;
 &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt; : [{&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;items&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;Schema&quot;&lt;/span&gt;}]
}

{&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;record&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;Map&quot;&lt;/span&gt;
 &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt; : [{&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;values&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;Schema&quot;&lt;/span&gt;}]
}

{&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;record&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;Union&quot;&lt;/span&gt;
 &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt; : [{&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;types&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: {&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;array&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;items&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;Schema&quot;&lt;/span&gt;}}]
}

{&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;record&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt; : [] }
{&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;record&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;Bytes&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt; : [] }
{&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;record&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt; : [] }
{&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;record&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt; : []}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Such a schema could be included in Avro, and any schema could be efficiently serialized in binary with it.  Would a parsing table be substantially more efficient?&lt;/p&gt;</comment>
                            <comment id="12724102" author="cutting" created="Thu, 25 Jun 2009 14:16:50 +0000"  >&lt;p&gt;My concern is mostly a documentation one.  We ought to include this code, the question is how we should frame it.&lt;/p&gt;

&lt;p&gt;Having thought a bit more, i think the case for this is applications that do not directly represent Avro data with in-memory objects, but rather stream through data.  Such applications would use the Encoder/Decoder API (nee ValueReader/Writer) directly, SAX-like rather than DOM-like, and validation would be useful.  It would be good to have some examples of such applications.  Can anyone think of any?&lt;/p&gt;

&lt;p&gt;I broke this patch with &lt;a href=&quot;https://issues.apache.org/jira/browse/AVRO-57&quot; title=&quot;rename ValueReader and ValueWriter and make them abstract&quot; class=&quot;issue-link&quot; data-issue-key=&quot;AVRO-57&quot;&gt;&lt;del&gt;AVRO-57&lt;/del&gt;&lt;/a&gt;.  I can try to update it later today.&lt;/p&gt;</comment>
                            <comment id="12724150" author="thiru_mg" created="Thu, 25 Jun 2009 17:11:07 +0000"  >&lt;p&gt;This new patch adjusts the class names and other things to suit the changes made by &lt;a href=&quot;https://issues.apache.org/jira/browse/AVRO-57&quot; title=&quot;rename ValueReader and ValueWriter and make them abstract&quot; class=&quot;issue-link&quot; data-issue-key=&quot;AVRO-57&quot;&gt;&lt;del&gt;AVRO-57&lt;/del&gt;&lt;/a&gt;. The other changes suggested by Doug (keeping the validating classes in a separate package, placement of parsing.html  etc.) are not in, though.&lt;/p&gt;</comment>
                            <comment id="12724190" author="cutting" created="Thu, 25 Jun 2009 18:53:39 +0000"  >&lt;p&gt;I just committed this. Thanks Thiru &amp;amp; Raymie!&lt;/p&gt;</comment>
                            <comment id="12724231" author="cutting" created="Thu, 25 Jun 2009 19:44:48 +0000"  >&lt;p&gt;I just reverted the changes to GenericDatumReader, as I&apos;m unsure of the performance impacts.  Perhaps, in a new Jira issue, we should reconsider this.  If the parsing table must be reused for good performance, then we might cache these in a WeakIdentityHashMap&amp;lt;Schema,ParsingTable&amp;gt;, so that naive applications do not suffer.&lt;/p&gt;

&lt;p&gt;Also, I should have mentioned with the commit, I made a few changes to the patch:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;made references to ParsingTable and Resolving table package private so that they would not show up in end-user javadoc;&lt;/li&gt;
	&lt;li&gt;fixed javadoc comments that still linked to ValueReader/Writer; and&lt;/li&gt;
	&lt;li&gt;moved parsing.html into the javadoc tree linked to from theses classes javadoc.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                    <attachments>
                            <attachment id="12411834" name="AVRO-29.patch" size="172225" author="thiru_mg" created="Thu, 25 Jun 2009 17:11:07 +0000"/>
                            <attachment id="12410924" name="AVRO-29.patch" size="169492" author="thiru_mg" created="Wed, 17 Jun 2009 11:51:56 +0000"/>
                            <attachment id="12408015" name="AVRO-29.patch" size="207500" author="thiru_mg" created="Wed, 13 May 2009 17:06:16 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 12 May 2009 16:32:31 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>94358</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 31 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0dxsn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>79405</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[AVRO-30] name lookup should consider namespace</title>
                <link>https://issues.apache.org/jira/browse/AVRO-30</link>
                <project id="12310911" key="AVRO">Apache Avro</project>
                    <description>&lt;p&gt;When a record schema is referred to by name, Avro will currently return any known record definition with that name, regardless of its namespace.  Instead, unqualified references should refer to names in the current namespace, and namespace-qualfied names should look for a record in the given namespace.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12425281">AVRO-30</key>
            <summary>name lookup should consider namespace</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="3">Duplicate</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="cutting">Doug Cutting</reporter>
                        <labels>
                    </labels>
                <created>Tue, 12 May 2009 23:36:49 +0000</created>
                <updated>Wed, 14 Oct 2009 18:53:04 +0000</updated>
                            <resolved>Thu, 24 Sep 2009 21:25:40 +0000</resolved>
                                                    <fixVersion>1.2.0</fixVersion>
                                    <component>java</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="12709035" author="cutting" created="Wed, 13 May 2009 18:05:42 +0000"  >&lt;p&gt;Currently we define classes and protocols with two separately specified parts, name and namespace, but we refer to them only with one, a name.&lt;/p&gt;

&lt;p&gt;We should expand the reference to include both parts.  So one might define a field as&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;{&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;foo&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;com.acme.Foo&quot;&lt;/span&gt;}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Questions:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Should we also permit definitions with both parts?  E.g.
{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;:&quot;com.acme.Foo&quot;}
&lt;p&gt; instead of &lt;/p&gt;
{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Foo&quot;, &quot;namespace&quot;:&quot;com.acme&quot;}&lt;/li&gt;
	&lt;li&gt;If so, should we eliminate the use of &quot;namespace&quot; altogether?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Regardless, unqualified names would always be in the namespace (if any) defined by their containing definition.  So usually, in a protocol, one need only specify the namespace in the top-level definition.  Note that this is include-friendly, since an included schema file should specify it&apos;s namespace in its top-level definition.&lt;/p&gt;</comment>
                            <comment id="12709327" author="sharadag" created="Thu, 14 May 2009 09:10:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;Should we also permit definitions with both parts?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;If so, should we eliminate the use of &quot;namespace&quot; altogether?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1. Removing only from type definitions. Top level we will still keep. Right?&lt;/p&gt;

&lt;p&gt;On a related note, should we consider schema and protocol definitions within the same file ? Not directly related to this issue, but I thought to bring it up at this point. On rethinking I thought we can combine two into one, with extension say &quot;.avro&quot;. This will also allow to have multiple types defined in single file, which is currently possible only in protocol definition. The format could look like:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;{&lt;span class=&quot;code-quote&quot;&gt;&quot;namespace&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.avro.test&quot;&lt;/span&gt;,

 &lt;span class=&quot;code-quote&quot;&gt;&quot;types&quot;&lt;/span&gt;: [
     {&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;TestRecord&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;record&quot;&lt;/span&gt;,
      &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt;: [
          {&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt;}
      ]
     },
     {&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;TestError&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;error&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;fields&quot;&lt;/span&gt;: [
         {&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;message&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt;}
      ]
     }
 ],

 &lt;span class=&quot;code-quote&quot;&gt;&quot;messages&quot;&lt;/span&gt;: { &lt;span class=&quot;code-quote&quot;&gt;&quot;protocol&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;Test&quot;&lt;/span&gt;,
     &lt;span class=&quot;code-quote&quot;&gt;&quot;hello&quot;&lt;/span&gt;: {
         &lt;span class=&quot;code-quote&quot;&gt;&quot;request&quot;&lt;/span&gt;: [{&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;greeting&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt;}],
         &lt;span class=&quot;code-quote&quot;&gt;&quot;response&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt;
     }
 }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note I have just moved the protocol name under &quot;messages&quot; . Rest of the structure remains same.&lt;/p&gt;</comment>
                            <comment id="12709462" author="cutting" created="Thu, 14 May 2009 16:37:58 +0000"  >&lt;p&gt;&amp;gt; I have just moved the protocol name under &quot;messages&quot; .&lt;/p&gt;

&lt;p&gt;That would not permit a message named &quot;protocol&quot;.  Perhaps instead we could make the messages and protocol fields optional, so that a .avro file might just contain namespace and types.&lt;/p&gt;

&lt;p&gt;We might also consider a &quot;.avroh&quot; format that&apos;s suitable for textual inclusion, so that one might have:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;{&lt;span class=&quot;code-quote&quot;&gt;&quot;namespace&quot;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.avro.test&quot;&lt;/span&gt;,

 &lt;span class=&quot;code-quote&quot;&gt;&quot;types&quot;&lt;/span&gt;: [
  #include mytypes.avroh
...
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I&apos;d rather avoid having explicit support for include in the JSON data, since we generally want schema and protocol JSON to include the transitive closure, so that they&apos;re portable, easy to persist, and standalone.&lt;/p&gt;</comment>
                            <comment id="12727337" author="raymie" created="Sun, 5 Jul 2009 14:25:54 +0000"  >&lt;p&gt;Back to namespaces: I agree that we should eliminate namespaces for schemas but keep them for protocols.&lt;/p&gt;

&lt;p&gt;Implications:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;When comparing two schemas each from a different protocol, the namespaces of the containing protocols should be ignored.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The Avro spec says that records exchanged during RPC handshake use a&quot;namespace of &quot;org.apache.avro.ipc&quot;.  This should be removed.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12728816" author="cutting" created="Wed, 8 Jul 2009 17:41:23 +0000"  >&lt;p&gt;&amp;gt; we should eliminate namespaces for schemas but keep them for protocols&lt;/p&gt;

&lt;p&gt;So it would then be illegal to use a given name twice in either a schema or a protocol, right?  This would make assembling protocols and schemas by file inclusion from datastructure libraries tricky, since different libraries might use the same name for different things.&lt;/p&gt;

&lt;p&gt;Also, we&apos;d like runtimes to be able to match Avro schemas with defined datastructures in a language.  E.g., in Java it might be nice if a schema whose name was &quot;Path&quot; and whose namespace was &quot;org.apache.hadoop.fs&quot; could be represented as an org.apache.hadoop.fs.Path instance.  Tracking the namespace of schemas enables this.&lt;/p&gt;</comment>
                            <comment id="12759271" author="cutting" created="Thu, 24 Sep 2009 21:25:40 +0000"  >&lt;p&gt;This has been incorporated into &lt;a href=&quot;https://issues.apache.org/jira/browse/AVRO-120&quot; title=&quot;specific and reflect APIs handle packages poorly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;AVRO-120&quot;&gt;&lt;del&gt;AVRO-120&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                                                <inwardlinks description="is part of">
                                        <issuelink>
            <issuekey id="12436446">AVRO-120</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 14 May 2009 09:10:56 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>94359</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 18 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0dxsv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>79406</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>
</channel>
</rss>
