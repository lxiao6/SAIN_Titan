<!--
RSS generated by JIRA (7.6.3#76005-sha1:8a4e38d34af948780dbf52044e7aafb13a7cae58) at Tue Jan 22 15:14:30 UTC 2019

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<!-- If you wish to do custom client-side styling of RSS, uncomment this:
<?xml-stylesheet href="https://issues.apache.org/jira/styles/jiraxml2html.xsl" type="text/xsl"?>
-->
<rss version="0.92">
    <channel>
        <title>ASF JIRA</title>
        <link>https://issues.apache.org/jira/issues/?jql=project+%3D+HIVE+AND+created+%3E%3D+2009-10-21+AND+created+%3C%3D+2009-10-28+ORDER+BY+key+ASC</link>
        <description>An XML representation of a search request</description>
                <language>en-uk</language>
                        <issue start="0" end="11" total="11"/>
                <build-info>
            <version>7.6.3</version>
            <build-number>76005</build-number>
            <build-date>09-01-2018</build-date>
        </build-info>

<item>
            <title>[HIVE-895] Add SerDe for Avro serialized data</title>
                <link>https://issues.apache.org/jira/browse/HIVE-895</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;As Avro continues to mature, having a SerDe to allow HiveQL queries over Avro data seems like a solid win.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12438666">HIVE-895</key>
            <summary>Add SerDe for Avro serialized data</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21141&amp;avatarType=issuetype">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jghoman">Jakob Homan</assignee>
                                    <reporter username="hammer">Jeff Hammerbacher</reporter>
                        <labels>
                    </labels>
                <created>Wed, 21 Oct 2009 06:18:08 +0000</created>
                <updated>Thu, 2 May 2013 02:30:54 +0000</updated>
                            <resolved>Sat, 2 Jun 2012 02:16:39 +0000</resolved>
                                    <version>0.9.0</version>
                                    <fixVersion>0.9.1</fixVersion>
                    <fixVersion>0.10.0</fixVersion>
                                    <component>Serializers/Deserializers</component>
                        <due></due>
                            <votes>15</votes>
                                    <watches>43</watches>
                                                                <comments>
                            <comment id="12889558" author="arov" created="Sun, 18 Jul 2010 04:12:30 +0000"  >&lt;p&gt;Can some one please explain to me how would this serde work?&lt;/p&gt;

&lt;p&gt;Specifically how would it deserialize the data?&lt;/p&gt;

&lt;p&gt;From what I understand Avro file has a header that defines the data that is stored in the file. In order to deserialize the data you need to read the header which is a challenge in Hive&apos;s Deserialize interface because the initialize() method does not know anything about the input file. (Note: there is a hack that can get you the file by getting the map.input hadoop property.... this hack however is not good enough in hive because some one might be using the CLI to query which will not trigger a map reduce job.&lt;/p&gt;

&lt;p&gt;Does anyone know a good solution to this issue?&lt;/p&gt;

&lt;p&gt;I am actually trying to implements a different file format but the idea of our format is similar to Avro: Each file has a header in which it contains a &quot;schema&quot;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="12889772" author="zshao" created="Mon, 19 Jul 2010 07:03:56 +0000"  >&lt;p&gt;We should just copy \the schema information from the file header to the hive metastore.&lt;/p&gt;</comment>
                            <comment id="12996108" author="cutting" created="Thu, 17 Feb 2011 21:51:18 +0000"  >&lt;p&gt;This may be similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-1748&quot; title=&quot;Add load/store function AvroStorage for avro data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-1748&quot;&gt;&lt;del&gt;PIG-1748&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13017983" author="rbodkin" created="Sat, 9 Apr 2011 23:11:37 +0000"  >&lt;p&gt;To properly support Avro file format will also require adding a store as format (as was done for RCFile). It looks like that&apos;s not yet a pluggable interface&lt;/p&gt;</comment>
                            <comment id="13026753" author="jghoman" created="Thu, 28 Apr 2011 22:55:26 +0000"  >&lt;p&gt;We&apos;ve written a library for this that we&apos;ll be open sourcing in a short timeframe (6-8 weeks).  If there is interest in that, I can take a look at reformatting it as a patch.  &lt;/p&gt;</comment>
                            <comment id="13026760" author="cwsteinbach" created="Thu, 28 Apr 2011 23:02:16 +0000"  >&lt;p&gt;@Jakob: There&apos;s lots of interest &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Please post the patch, even if it&apos;s a WIP.&lt;/p&gt;</comment>
                            <comment id="13054027" author="jghoman" created="Thu, 23 Jun 2011 18:50:59 +0000"  >&lt;p&gt;We&apos;ve released the serde (we&apos;re calling it haivvreo) into open source here: &lt;a href=&quot;http://bit.ly/iwEQzJ&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://bit.ly/iwEQzJ&lt;/a&gt;   We&apos;ve been testing it in our production ETL for a while and it&apos;s working well.  I&apos;d like to hold off on making a Hive patch until we&apos;ve iterated a bit more and ironed out any bugs that are in hiding.  I&apos;ll plan on getting a patch in before 0.8 is released.  In the mean time, anyone who wants to kick the tires is welcome to. &lt;/p&gt;</comment>
                            <comment id="13054116" author="cwsteinbach" created="Thu, 23 Jun 2011 21:32:46 +0000"  >&lt;p&gt;@jakob: The code on github looks really good.&lt;/p&gt;

&lt;p&gt;The release branch for 0.8.0 is going to get created sometime in the next couple of weeks. Do you think it will be possible to get a patch ready for review before then?&lt;/p&gt;</comment>
                            <comment id="13054124" author="jghoman" created="Thu, 23 Jun 2011 21:47:58 +0000"  >&lt;p&gt;A couple weeks is probably not feasible.  Assuming 0.9 comes out in a few months after that, that&apos;s probably a better bet.&lt;/p&gt;</comment>
                            <comment id="13094127" author="cwsteinbach" created="Tue, 30 Aug 2011 21:48:24 +0000"  >&lt;p&gt;@Jakob: Just wanted to check in and see if you&apos;re ready to get this&lt;br/&gt;
patch committed to trunk. The 0.8.0 release branch was created &lt;br/&gt;
yesterday, so you wouldn&apos;t have to worry about this work immediately&lt;br/&gt;
appearing in a release.&lt;/p&gt;</comment>
                            <comment id="13122896" author="lianhuiwang" created="Fri, 7 Oct 2011 15:51:54 +0000"  >&lt;p&gt;@Jakob:i read the code of the haivvreo.&lt;br/&gt;
and i think it can do with protocol buffers like haivvreo.&lt;br/&gt;
google &apos;s paper tenzing like hive said it support protocol buffers and columnIO.&lt;/p&gt;</comment>
                            <comment id="13212335" author="cwsteinbach" created="Tue, 21 Feb 2012 04:03:19 +0000"  >&lt;p&gt;@Jakob: Do you think we can get this committed in time for the 0.9.0 release? Are you willing to attach a patch? Thanks.&lt;/p&gt;</comment>
                            <comment id="13214985" author="appodictic" created="Thu, 23 Feb 2012 19:45:25 +0000"  >&lt;p&gt;This has a Apache V2 licence if anyone wants we should be able to just git --clone this and patch it in if we keep the licence file.&lt;/p&gt;</comment>
                            <comment id="13214993" author="jghoman" created="Thu, 23 Feb 2012 20:01:26 +0000"  >&lt;p&gt;yes, I&apos;ve set aside some time early next week to get it into patch form. &lt;/p&gt;</comment>
                            <comment id="13267001" author="jghoman" created="Wed, 2 May 2012 23:13:50 +0000"  >&lt;p&gt;Here&apos;s a first draft of the port to ASF.  It corresponds to the mergeHive8ToMaster branch on github, which has all the latest fixes and is compatible with Hive 8.  Need to re-format to Hive style and run full unit tests.  &lt;/p&gt;

&lt;p&gt;One thing of concern is that the avroserde relies on the ql package, which required a change to the build script to build serde afterwards.  Is there a defined dependency for Hive&apos;s modules, and if so does this break that?  If so, the other option would be to move this to the contrib package, but to me contrib is a dirty word and I&apos;d like to avoid that.  &lt;/p&gt;

&lt;p&gt;Also, this bundles the avro serde into the serde jar.  It&apos;d be nice for those not using Avro to not require it, but Avro is already a build-time dependency so it&apos;s not a new problem.  Eventually it&apos;d be nice to have a separate jar with just the serde in it to make the code more modular.&lt;/p&gt;

&lt;p&gt;I&apos;ll finish the port in the next couple of days, but take a glance and comment if you&apos;d like.&lt;/p&gt;</comment>
                            <comment id="13268619" author="cwsteinbach" created="Fri, 4 May 2012 18:51:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;One thing of concern is that the avroserde relies on the ql package, which required a change to the build script to build serde afterwards. Is there a defined dependency for Hive&apos;s modules, and if so does this break that? If so, the other option would be to move this to the contrib package, but to me contrib is a dirty word and I&apos;d like to avoid that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right now ql depends on serde, but not vice-versa, so it sounds like this patch will add a circular dependency which is something we should probably try to avoid. I also agree with you that this belongs in the serde package as opposed to contrib. Do you think it&apos;s possible to move the ql code that the avro serde depends on to common?&lt;/p&gt;

&lt;p&gt;Also, I apologize in advance for this request, but would you mind posting a review for this on phabricator? Directions are located here:&lt;br/&gt;
&lt;a href=&quot;https://cwiki.apache.org/Hive/phabricatorcodereview.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/Hive/phabricatorcodereview.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If this looks like too much of I pain I can post the review request for you, but I may need some help applying the patch to trunk.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                            <comment id="13269914" author="jghoman" created="Mon, 7 May 2012 19:20:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do you think it&apos;s possible to move the ql code that the avro serde depends on to common?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Should be fine.  Will do this week.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, I apologize in advance for this request, but would you mind posting a review for this on phabricator? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Due it its reliance on facebook.com, this site still doesn&apos;t display correctly for me, but I&apos;ll use a different browser just for this request.&lt;/p&gt;</comment>
                            <comment id="13276107" author="jghoman" created="Tue, 15 May 2012 19:05:02 +0000"  >&lt;p&gt;btw, moving stuff to common doesn&apos;t work, so I&apos;m doing a bit of refactoring.  New patch shortly... (schedule permitting)&lt;/p&gt;</comment>
                            <comment id="13280437" author="jghoman" created="Mon, 21 May 2012 20:14:00 +0000"  >&lt;p&gt;Final patch.  Swtiching to TBLPROPERTIES rather than SERDEPROPERTIES obviated the need for the ql calls previously.  Spent a lot of frustrtating time trying to get the phabricator to work (quite surprised that this FB-specific framework is kosher in the ASF).  It&apos;s up at &lt;a href=&quot;https://reviews.facebook.net/D3321&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D3321&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13280438" author="jghoman" created="Mon, 21 May 2012 20:14:49 +0000"  >&lt;p&gt;Forgot to grant ASF license.  Re-uping.&lt;/p&gt;</comment>
                            <comment id="13283592" author="appodictic" created="Fri, 25 May 2012 16:50:53 +0000"  >&lt;p&gt;This is a great addition to hive. I will start reviewing to wrap my head around it but since you only made a tiny upstream change to hive (an ivy file) and you have extensive unit tests this looks good to go. +1 Will commit if tests pass.&lt;/p&gt;</comment>
                            <comment id="13286866" author="jghoman" created="Thu, 31 May 2012 19:36:25 +0000"  >&lt;p&gt;Did the tests pass? Anything else I can do to help?&lt;/p&gt;</comment>
                            <comment id="13286875" author="cwsteinbach" created="Thu, 31 May 2012 19:53:01 +0000"  >&lt;p&gt;@Jakob: I&apos;ll take a look at this weekend. Sorry for the delay, and thanks again for putting this together!&lt;/p&gt;</comment>
                            <comment id="13286892" author="appodictic" created="Thu, 31 May 2012 20:14:23 +0000"  >&lt;p&gt;@Carl  you don&apos;t have to worry. @JAKOB I will get it done probably later tonight. You did a solid job, I had to familiarize myself with avro a bit before I did the review.&lt;/p&gt;</comment>
                            <comment id="13286918" author="appodictic" created="Thu, 31 May 2012 20:49:12 +0000"  >&lt;p&gt;Running this.&lt;br/&gt;
ant test -Dtestcase=TestCliDriver -Dqfile=avro_joins.q -Dtest.silent=false&lt;br/&gt;
Throws this.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.ql.metadata.HiveException: Cannot validate serde: org.apache.hadoop.hive.serde2.avro.AvroSerDe
	at org.apache.hadoop.hive.ql.exec.DDLTask.validateSerDe(DDLTask.java:3168)
	at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:3290)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:243)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1322)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1108)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:943)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:341)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:669)
	at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_joins(TestCliDriver.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
Caused by: org.apache.hadoop.hive.serde2.SerDeException: SerDe org.apache.hadoop.hive.serde2.avro.AvroSerDe does not exist
	at org.apache.hadoop.hive.serde2.SerDeUtils.lookupDeserializer(SerDeUtils.java:85)
	at org.apache.hadoop.hive.ql.exec.DDLTask.validateSerDe(DDLTask.java:3163)
	... 28 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Any thoughts?&lt;/p&gt;</comment>
                            <comment id="13286977" author="appodictic" created="Thu, 31 May 2012 21:33:00 +0000"  >&lt;p&gt;I think the issue is the avro jars are not on the compile classpath but either need to be on the runtime classpath or added with add jar.&lt;/p&gt;</comment>
                            <comment id="13287775" author="jghoman" created="Fri, 1 Jun 2012 23:42:56 +0000"  >&lt;p&gt;The problem is that the tests in ql load up the serde package from the local ivy rather than from the build path, unless you do a full very-clean.  These jars don&apos;t have the new classes and hence fail.  I could reproduce this by running a test without the patch, applying the patch, running a test and it would then fail from the local jars.  Running very-clean, applying the patch and then running the test passes:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    [junit] Running org.apache.hadoop.hive.cli.TestCliDriver
    [junit] Begin query: avro_joins.q
    [junit] Copying file: file:/private/tmp/tp895/git/data/files/doctors.avro
    [junit] Copying file: file:/private/tmp/tp895/git/data/files/episodes.avro
    [junit] diff -a /private/tmp/tp895/git/build/ql/test/logs/clientpositive/avro_joins.q.out /private/tmp/tp895/git/ql/src/test/results/clientpositive/avro_joins.q.out
    [junit] Done query: avro_joins.q elapsedTime=16s
    [junit] Cleaning up TestCliDriver
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 24.91 sec
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I reproduced this on both my Mac and RHEL boxes and verified that if you go and blow away the &lt;tt&gt;~./cache/org.apache.hive/hive-serde/jars/&lt;/tt&gt; directory and leave everything else constant, the test passes. This is a problem with how the test infrastructure loads classes, not with this patch itself...&lt;/p&gt;</comment>
                            <comment id="13287794" author="ashutoshc" created="Sat, 2 Jun 2012 00:24:24 +0000"  >&lt;p&gt;Yup, &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-3035&quot; title=&quot;Modify clean target to remove ~/.ivy2/local/org.apache.hive ~/.ivy2/cache/org.apache.hive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-3035&quot;&gt;&lt;del&gt;HIVE-3035&lt;/del&gt;&lt;/a&gt; needs to be fixed.&lt;/p&gt;</comment>
                            <comment id="13287818" author="jghoman" created="Sat, 2 Jun 2012 01:00:41 +0000"  >&lt;p&gt;Yeah, that should get fixed, but the bigger problem is that tests shouldn&apos;t be relying on ivy artifacts all (for any of the Hive artifacts).  The classes-under-test should be loaded directly from build/ either as classes or jars.  Currently, all new patches that go between components and aren&apos;t very-clean&apos;ed first are not getting tested correctly.&lt;/p&gt;</comment>
                            <comment id="13287826" author="appodictic" created="Sat, 2 Jun 2012 01:29:31 +0000"  >&lt;p&gt;avro_sanity_test.q had a different comment i patched it for you&lt;/p&gt;</comment>
                            <comment id="13287834" author="appodictic" created="Sat, 2 Jun 2012 02:15:38 +0000"  >&lt;p&gt;+1 committed. Thank you Jakob. nice contribution.&lt;/p&gt;</comment>
                            <comment id="13287874" author="hudson" created="Sat, 2 Jun 2012 08:26:37 +0000"  >&lt;p&gt;Integrated in Hive-trunk-h0.21 #1463 (See &lt;a href=&quot;https://builds.apache.org/job/Hive-trunk-h0.21/1463/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hive-trunk-h0.21/1463/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-895&quot; title=&quot;Add SerDe for Avro serialized data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-895&quot;&gt;&lt;del&gt;HIVE-895&lt;/del&gt;&lt;/a&gt; Add SerDe for Avro serialized data (Jakob Homan via egc) (Revision 1345420)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
ecapriolo : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1345420&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1345420&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hive/trunk/data/files/doctors.avro&lt;/li&gt;
	&lt;li&gt;/hive/trunk/data/files/episodes.avro&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/avro&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroContainerInputFormat.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroContainerOutputFormat.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroGenericRecordReader.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroGenericRecordWriter.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_change_schema.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_evolved_schemas.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_joins.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_sanity_test.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_schema_error_message.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_schema_literal.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_change_schema.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_evolved_schemas.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_joins.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_sanity_test.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_schema_error_message.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_schema_literal.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/ivy.xml&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroGenericRecordWritable.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroObjectInspectorGenerator.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeException.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/BadSchemaException.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/InstanceCache.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/ReaderWriterSchemaPair.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/SchemaResolutionProblem.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/SchemaToTypeInfo.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerde.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerdeUtils.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerializer.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestGenericAvroRecordWritable.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestInstanceCache.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestSchemaReEncoder.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestThatEvolvedSchemasActAsWeWant.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/Utils.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13287980" author="ashutoshc" created="Sat, 2 Jun 2012 17:26:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;committed. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Wuhoo. Awesome, Thanks Jakob for contributing, making Avro data easily accessible from Hive.&lt;/p&gt;</comment>
                            <comment id="13289505" author="lars_francke" created="Tue, 5 Jun 2012 16:07:01 +0000"  >&lt;p&gt;Thanks! This is awesome to have in Hive.&lt;/p&gt;

&lt;p&gt;Could I please ask you to document this feature on the Wiki? Or if you can list all the features it supports I&apos;ll happily write something up too.&lt;/p&gt;</comment>
                            <comment id="13289638" author="cwsteinbach" created="Tue, 5 Jun 2012 18:56:20 +0000"  >&lt;p&gt;@Jakob: Thanks for contributing this to Hive!&lt;br/&gt;
@Ed: Thanks for getting this committed!&lt;/p&gt;</comment>
                            <comment id="13290073" author="jghoman" created="Wed, 6 Jun 2012 10:39:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;Could I please ask you to document this feature on the Wiki?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sure thing.  I&apos;ll transfer all the text from the github account shortly.  I&apos;m traveling so it may take a few days.&lt;/p&gt;</comment>
                            <comment id="13401669" author="jghoman" created="Tue, 26 Jun 2012 20:39:52 +0000"  >&lt;p&gt;I&apos;ve moved all the relevant info from the github page to the Hive Wiki and linked to it from the main page: &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/AvroSerDe+-+working+with+Avro+from+Hive&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/AvroSerDe+-+working+with+Avro+from+Hive&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13401671" author="lars_francke" created="Tue, 26 Jun 2012 20:42:04 +0000"  >&lt;p&gt;Thank you very much!&lt;/p&gt;</comment>
                            <comment id="13490912" author="mithun" created="Mon, 5 Nov 2012 20:45:10 +0000"  >&lt;p&gt;Jakob, a quick question: The JIRA indicates that the AvroSerde is available in 0.9.1 and in 0.10.0. (And that&apos;s in sync with &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/AvroSerDe+-+working+with+Avro+from+Hive&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/AvroSerDe+-+working+with+Avro+from+Hive&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;But it looks like the branch-0.9/ doesn&apos;t have this code. Are there plans to port this over?&lt;/p&gt;

&lt;p&gt;(Thanks for writing this, by the way. :])&lt;/p&gt;</comment>
                            <comment id="13490998" author="jghoman" created="Mon, 5 Nov 2012 22:43:04 +0000"  >&lt;p&gt;The fixed version is 9.1, but at least in the JIRA, I don&apos;t see it having been committed to anywhere else.  You can check that branch for the 895 commit, but I don&apos;t think it was. &lt;/p&gt;

&lt;p&gt;I don&apos;t plan to do any porting.  You&apos;re welcome to try it or just go ahead and use haivvreo until 0.10 comes out.&lt;/p&gt;</comment>
                            <comment id="13491177" author="metaruslan" created="Tue, 6 Nov 2012 03:27:39 +0000"  >&lt;p&gt;Mithun, what distro do you use? Cloudera patched an earlier version of Hive with this SerDe:&lt;br/&gt;
&lt;a href=&quot;https://ccp.cloudera.com/display/CDHDOC/New+Features+in+CDH3#NewFeaturesinCDH3-What%27sNewinCDH3Update5&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://ccp.cloudera.com/display/CDHDOC/New+Features+in+CDH3#NewFeaturesinCDH3-What%27sNewinCDH3Update5&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://ccp.cloudera.com/display/DOC/CDH+Version+and+Packaging+Information#CDHVersionandPackagingInformation-CDH3Update5Packaging&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://ccp.cloudera.com/display/DOC/CDH+Version+and+Packaging+Information#CDHVersionandPackagingInformation-CDH3Update5Packaging&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13491238" author="mithun" created="Tue, 6 Nov 2012 06:31:04 +0000"  >&lt;p&gt;@&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jghoman&quot; class=&quot;user-hover&quot; rel=&quot;jghoman&quot;&gt;Jakob Homan&lt;/a&gt;: Thanks for clarifying. I&apos;ll see if this can&apos;t be merged into branch-0.9/.&lt;br/&gt;
@&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=metaruslan&quot; class=&quot;user-hover&quot; rel=&quot;metaruslan&quot;&gt;Ruslan Al-Fakikh&lt;/a&gt;: Thank you for the heads-up. We at Yahoo are currently using our own builds instead of a commercial distro. :] It&apos;d be great to have this included in 0.9.1.&lt;/p&gt;</comment>
                            <comment id="13547928" author="hudson" created="Wed, 9 Jan 2013 10:23:37 +0000"  >&lt;p&gt;Integrated in Hive-trunk-hadoop2 #54 (See &lt;a href=&quot;https://builds.apache.org/job/Hive-trunk-hadoop2/54/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hive-trunk-hadoop2/54/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-895&quot; title=&quot;Add SerDe for Avro serialized data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-895&quot;&gt;&lt;del&gt;HIVE-895&lt;/del&gt;&lt;/a&gt; Add SerDe for Avro serialized data (Jakob Homan via egc) (Revision 1345420)&lt;/p&gt;

&lt;p&gt;     Result = ABORTED&lt;br/&gt;
ecapriolo : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1345420&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1345420&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hive/trunk/data/files/doctors.avro&lt;/li&gt;
	&lt;li&gt;/hive/trunk/data/files/episodes.avro&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/avro&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroContainerInputFormat.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroContainerOutputFormat.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroGenericRecordReader.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroGenericRecordWriter.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_change_schema.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_evolved_schemas.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_joins.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_sanity_test.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_schema_error_message.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/avro_schema_literal.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_change_schema.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_evolved_schemas.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_joins.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_sanity_test.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_schema_error_message.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/avro_schema_literal.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/ivy.xml&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroGenericRecordWritable.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroObjectInspectorGenerator.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeException.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/BadSchemaException.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/InstanceCache.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/ReaderWriterSchemaPair.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/SchemaResolutionProblem.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/avro/SchemaToTypeInfo.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerde.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerdeUtils.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerializer.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestGenericAvroRecordWritable.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestInstanceCache.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestSchemaReEncoder.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestThatEvolvedSchemasActAsWeWant.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/avro/Utils.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13550213" author="ashutoshc" created="Thu, 10 Jan 2013 19:53:49 +0000"  >&lt;p&gt;This issue is fixed and released as part of 0.10.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12431762">MAPREDUCE-815</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12460747">AVRO-493</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12606641">HIVE-3447</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12611990">HIVE-3585</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12556338">HIVE-3035</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12480859">PIG-1748</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12559409">HIVE-3088</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12525360" name="HIVE-895-draft.patch" size="164080" author="jghoman" created="Wed, 2 May 2012 23:13:50 +0000"/>
                            <attachment id="12528483" name="HIVE-895.patch" size="193954" author="jghoman" created="Mon, 21 May 2012 20:14:49 +0000"/>
                            <attachment id="12528481" name="doctors.avro" size="521" author="jghoman" created="Mon, 21 May 2012 20:14:49 +0000"/>
                            <attachment id="12528482" name="episodes.avro" size="597" author="jghoman" created="Mon, 21 May 2012 20:14:49 +0000"/>
                            <attachment id="12530630" name="hive-895.patch.1.txt" size="195045" author="appodictic" created="Sat, 2 Jun 2012 01:29:31 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 18 Jul 2010 04:12:30 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>42690</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 2 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02ufz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>14533</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-896] Add LEAD/LAG/FIRST/LAST analytical windowing functions to Hive.</title>
                <link>https://issues.apache.org/jira/browse/HIVE-896</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Windowing functions are very useful for click stream processing and similar time-series/sliding-window analytics.&lt;/p&gt;

&lt;p&gt;More details at:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://download-west.oracle.com/docs/cd/B13789_01/server.101/b10736/analysis.htm#i1006709&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://download-west.oracle.com/docs/cd/B13789_01/server.101/b10736/analysis.htm#i1006709&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://download-west.oracle.com/docs/cd/B13789_01/server.101/b10736/analysis.htm#i1007059&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://download-west.oracle.com/docs/cd/B13789_01/server.101/b10736/analysis.htm#i1007059&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://download-west.oracle.com/docs/cd/B13789_01/server.101/b10736/analysis.htm#i1007032&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://download-west.oracle.com/docs/cd/B13789_01/server.101/b10736/analysis.htm#i1007032&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#8211; amr&lt;/p&gt;</description>
                <environment></environment>
        <key id="12438744">HIVE-896</key>
            <summary>Add LEAD/LAG/FIRST/LAST analytical windowing functions to Hive.</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21141&amp;avatarType=issuetype">New Feature</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rhbutani">Harish Butani</assignee>
                                    <reporter username="aaa">Amr Awadallah</reporter>
                        <labels>
                    </labels>
                <created>Wed, 21 Oct 2009 20:54:47 +0000</created>
                <updated>Tue, 26 Aug 2014 05:29:32 +0000</updated>
                            <resolved>Tue, 2 Apr 2013 14:17:25 +0000</resolved>
                                                    <fixVersion>0.11.0</fixVersion>
                                    <component>OLAP</component>
                    <component>UDF</component>
                        <due></due>
                            <votes>13</votes>
                                    <watches>46</watches>
                                                                <comments>
                            <comment id="13062727" author="clarkyzl" created="Sun, 10 Jul 2011 12:37:10 +0000"  >&lt;p&gt;I think it is necessary to have a kind of functions called UDWF (User-Defined Windowing Function).&lt;/p&gt;</comment>
                            <comment id="13187738" author="hammer" created="Tue, 17 Jan 2012 15:11:16 +0000"  >&lt;p&gt;Guy who appears to be from SAP has posted some code for windowing functions using Hive on Github: &lt;a href=&quot;https://github.com/hbutani/SQLWindowing&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/hbutani/SQLWindowing&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13255251" author="rhbutani" created="Tue, 17 Apr 2012 01:55:37 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Yes, I have been developing SQL Windowing on top of Hive(&lt;a href=&quot;https://github.com/hbutani/SQLWindowing/wiki&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/hbutani/SQLWindowing/wiki&lt;/a&gt;). Users can intersperse Hive Queries with Windowing Clauses and Table functions. Users can use this from a CLI (hive --service windowingCLI) or via a Query API. &lt;br/&gt;
The hope is to provide Users:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Windowing Clauses and Table functions as a simple extension to HQL&lt;/li&gt;
	&lt;li&gt;overtime to fold the implementation into the Hive code base with hopefully minimal impact to the User experience.&lt;br/&gt;
Here are some thoughts on how to fold this functionality into Hive: &lt;a href=&quot;https://github.com/hbutani/SQLWindowing/wiki/MoveToHive&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/hbutani/SQLWindowing/wiki/MoveToHive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Any thoughts/comments/suggestions greatly welcomed.&lt;/p&gt;

&lt;p&gt;regards,&lt;br/&gt;
Harish Butani.&lt;/p&gt;</comment>
                            <comment id="13539072" author="rhbutani" created="Sun, 23 Dec 2012 18:48:09 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;We are posting a preliminary patch for a Partitioned Table Function mechanism and &lt;br/&gt;
Windowing clause support based on this. The solution let&apos;s you invoke a &lt;br/&gt;
Partitioned Table Function anywhere a Table/SubQuery can appear in HQL.&lt;br/&gt;
The Windowing clause support matches standard SQL as much as possible: &lt;br/&gt;
ability to define windows with the Query or individual Function; ability to &lt;br/&gt;
specify a range or value based window with any UDAF. But since Windowing is &lt;br/&gt;
handled as a PTF invocation, all Window specification must have the same Partition &lt;br/&gt;
and Order specification.&lt;/p&gt;

&lt;p&gt;You can read about the details in a (work in progress) document &lt;br/&gt;
here &lt;a href=&quot;http://tinyurl.com/ck4nopn&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tinyurl.com/ck4nopn&lt;/a&gt;.  We have added a  lot of tests to show case the &lt;br/&gt;
functionality. A good starting point is ptf_general_queries.q, which has 49 queries.&lt;/p&gt;

&lt;p&gt;But let us emphasize that this is a preliminary patch. We wanted to get this out early &lt;br/&gt;
to get your feedback sooner rather than later. We need to do a lot of cleanup, &lt;br/&gt;
refactoring and documentation. The starting point was our SQLWindowing on top of Hive &lt;br/&gt;
project; which used Hive&apos;s metadata and runtime components but had its own Query form. &lt;br/&gt;
So some components still reflect the assumptions from that project. We started by &lt;br/&gt;
taking all the code from that project and placing it in the ql.ptf package. &lt;br/&gt;
Gradually we have dissipated the stuff under this package; but we still have some &lt;br/&gt;
ways to go. For background it may help to look at our Hadoop Summit &lt;br/&gt;
presentation(&lt;a href=&quot;http://tinyurl.com/bm4qb7z&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tinyurl.com/bm4qb7z&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Finally and most importantly we are not completely finished. We are missing support for &lt;br/&gt;
Queries with multiple Inserts. We have to address the case of Queries with aggregations &lt;br/&gt;
with no group by and with constants as columns in the Select List.  On the entire ql &lt;br/&gt;
testsuite there  are still around 15 failures, because of these 2 issues.&lt;/p&gt;

&lt;p&gt;Harish Butani, Prajakta Kalmegh&lt;/p&gt;</comment>
                            <comment id="13547347" author="alangates" created="Tue, 8 Jan 2013 22:24:27 +0000"  >&lt;p&gt;Harish,&lt;/p&gt;

&lt;p&gt;Could you point out the interfaces (in the API sense, not the Java sense) that are most important in this patch?  In particular I&apos;m intersted in interfaces between UDFs and Hive.  Based on my review so far the classes that stand out as important in this regard are TableFunctionEvaluator, TableFunctionResolver, and PTFPartition.  Are there others I should be looking at?&lt;/p&gt;

&lt;p&gt;Questions I have so far:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;If I read this right you are using CLUSTER BY and SORT BY instead of PARTITION BY and ORDER BY for syntax in OVER.  Why?&lt;/li&gt;
	&lt;li&gt;Does it ever make sense for a windowing function to return a partition?  Should there be a interface/abstract class specific for windowing functions that only returns a single entry?&lt;/li&gt;
	&lt;li&gt;Can I put one of the existing aggregate functions in an OVER clause using this?&lt;/li&gt;
	&lt;li&gt;Could you explain how the partition is handled in memory?  It looks to me as if the entire partition is read into memory.  Is that correct?  If so, does it read it aggresively or as the iterator moves through the records?  It also appears there is no effort to drop earlier parts of the partition that are now out of range of the window.  Is that also correct?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13547625" author="rhbutani" created="Wed, 9 Jan 2013 04:31:45 +0000"  >&lt;p&gt;Hi Alan,&lt;br/&gt;
Thanks for taking the time. Here are my responses:&lt;/p&gt;

&lt;p&gt;1. Could you point out the interfaces...&lt;br/&gt;
Yes you are right, from a function writer perspective TableFunctionEvaluator, TableFunctionResolver are the important ifcs; PTFPartition(and PTFPartitionIterator) is the data container ifc.&lt;/p&gt;

&lt;p&gt;2. If I read this right you are using CLUSTER BY and SORT BY instead of PARTITION BY and ORDER BY for syntax in OVER. Why?&lt;br/&gt;
To highlight the similarity. The Partition/Order specs in a Window clause have the same meaning as Cluster/Distribute in HQL. Note you can  use a Cluster/Distribute at the query level and not specify any Partition spec in a Window clause. So the following are different ways for saying the same thing:&lt;/p&gt;

&lt;p&gt;a.&lt;br/&gt;
select p_mfgr, p_name, &lt;br/&gt;
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row)&lt;br/&gt;
from part;&lt;br/&gt;
b.&lt;br/&gt;
select p_mfgr, p_name, p_size,&lt;br/&gt;
sum(p_retailprice) over (rows between unbounded preceding and current row)&lt;br/&gt;
from part&lt;br/&gt;
distribute by p_mfgr&lt;br/&gt;
sort by p_name;&lt;br/&gt;
c.&lt;br/&gt;
select p_mfgr, p_name, p_size,&lt;br/&gt;
sum(p_retailprice) over (w1)&lt;br/&gt;
from part&lt;br/&gt;
window w1 as distribute by p_mfgr  sort by p_name rows between 2 preceding and 2 following;&lt;/p&gt;

&lt;p&gt;(I just realized that there are no egs of using Cluster/Distribute in Wdw clauses in the tests; we are adding them now)&lt;/p&gt;

&lt;p&gt;3. Can I put one of the existing aggregate functions in an OVER clause using this?&lt;br/&gt;
I am not exactly clear what your question is. I may have answered it above. To be clear there is no special Window Function. Any existing Hive UDAF invocation can have a Windowing specification. &lt;br/&gt;
tests 31,40,41 cover most of the UDAFs.&lt;/p&gt;

&lt;p&gt;4. Could you explain how the partition is handled in memory...&lt;br/&gt;
Partitions are backed by a Persistent List ( see ptf.ds.PartitionedByteBasedList) . We need do to some work to refactor this package. Yes you are right, things can be done in delaying bringing rows into a partition and getting rid of rows once outside the window. This is true for Windowing Table Function; especially for Range based Windows.&lt;/p&gt;

&lt;p&gt;But for a general PTF the contract is Partition in Partition out. For e.g. CandidateFrequency function will read the rows in a partition multiple times.&lt;/p&gt;

&lt;p&gt;The PartitionedByteBasedList is backed by a set of PersistentByteBasedLists which uses weak refs and stores its data on disk. Done some testing with partitions with a million rows. But I agree with what you are getting at: there is stuff that can be done to reduce the memory footprint. Haven&apos;t gotten around to it....&lt;/p&gt;</comment>
                            <comment id="13557700" author="alangates" created="Fri, 18 Jan 2013 22:40:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;If I read this right you are using CLUSTER BY and SORT BY instead of PARTITION BY and ORDER BY for syntax in OVER. Why?  To highlight the similarity. The Partition/Order specs in a Window clause have the same meaning as Cluster/Distribute in HQL. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is only true as long as you have only one OVER clause, right?  As soon as you add the ability to have separate OVER clauses partitioning by different keys (which users will want very soon) you lose this identity.&lt;/p&gt;

&lt;p&gt;Even if you decide to retain this I would argue that the standard PARTITION BY/ORDER BY syntax should be accepted as well.  HQL already has enough one off syntax that makes life hard for people coming from more standard SQL.  It should not be exacerbated.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Could you explain how the partition is handled in memory...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Partitions are backed by a Persistent List ( see ptf.ds.PartitionedByteBasedList) . We need do to some work to refactor this package. Yes you are right, things can be done in delaying bringing rows into a partition and getting rid of rows once outside the window. This is true for Windowing Table Function; especially for Range based Windows.&lt;br/&gt;
But for a general PTF the contract is Partition in Partition out. For e.g. CandidateFrequency function will read the rows in a partition multiple times.&lt;/p&gt;

&lt;p&gt;This is part of where I was going with my earlier question on why a windowing function would ever return a partition.  I am becoming less convinced that it makes sense to combine windowing and partition functions.  While they both take partitions as inputs they return different things.  Partition functions return partitions and windowing functions return a single value.  As you point out here the partition functions will also not be interested in the range limiting features of windowing functions.  But taking advantage of this in windowing functions will be very important for performance optimizations, I suspect.  At the very least it seems like partitioning functions and windowing functions should be presented as separate entities to users and UDF writers, even if for now Hive shares some of the framework for handling them underneath.  This way in the future optimizations and new features can be added in a way that is advantageous for each.&lt;/p&gt;</comment>
                            <comment id="13558058" author="rhbutani" created="Sat, 19 Jan 2013 17:41:53 +0000"  >&lt;p&gt;1. &lt;br/&gt;
This is only true as long as you have only one OVER clause, right? As soon as you add the ability to have separate OVER clauses partitioning by different keys (which users will want very soon) you lose this identity. Even if you decide to retain this I would argue that the standard PARTITION BY/ORDER BY syntax should be accepted as well. HQL already has enough one off syntax that makes life hard for people coming from more standard SQL. It should not be exacerbated.&lt;/p&gt;

&lt;p&gt;I am agnostic about the second point. We can support Partition/Order or Distribute/Sort or both...&lt;/p&gt;

&lt;p&gt;Regarding the first point, no it applies beyond having the same partitioning. If you say something like:&lt;/p&gt;

&lt;p&gt;select p_mfgr, p_name,&lt;br/&gt;
 sum(p_size) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row)&lt;br/&gt;
from part&lt;br/&gt;
distribute by p_mfgr&lt;br/&gt;
sort by p_name;&lt;/p&gt;

&lt;p&gt;This is allowed in the language; if we don&apos;t relate windowing to the distribute/sort, this means do the Windowing and then do the distribute/sort. I doubt this would ever be what the user intended. So we propose to associate the distribute/sort with windowing; and use it at as the &apos;default&apos; partitioning specification. So we allow a Query to be specified this way:&lt;/p&gt;

&lt;p&gt;select p_mfgr, p_name,&lt;br/&gt;
sum(p_size) over (rows between unbounded preceding and current row)&lt;br/&gt;
from part&lt;br/&gt;
distribute by p_mfgr&lt;br/&gt;
sort by p_name;&lt;/p&gt;

&lt;p&gt;The concept of inheriting the default partitioning still works even when we allow different partitioning specifications. So in the future this will be how you specify multiple ordering:&lt;/p&gt;

&lt;p&gt;select p_mfgr, p_name,&lt;br/&gt;
 sum(p_size) over ( rows between unbounded preceding and current row),&lt;br/&gt;
 sum(p_retailprice) (sort by p_size rows between unbounded preceding and current row)&lt;br/&gt;
from part&lt;br/&gt;
distribute by p_mfgr&lt;br/&gt;
sort by p_name;&lt;/p&gt;

&lt;p&gt;Similarly you can have multiple distribute specs.&lt;/p&gt;

&lt;p&gt;2.&lt;br/&gt;
This is part of where I was going with my earlier question on why a windowing function would ever return a partition. I am becoming less convinced that it makes sense to combine windowing and partition functions. While they both take partitions as inputs they return different things. Partition functions return partitions and windowing functions return a single value. As you point out here the partition functions will also not be interested in the range limiting features of windowing functions. But taking advantage of this in windowing functions will be very important for performance optimizations, I suspect. At the very least it seems like partitioning functions and windowing functions should be presented as separate entities to users and UDF writers, even if for now Hive shares some of the framework for handling them underneath. This way in the future optimizations and new features can be added in a way that is advantageous for each.&lt;/p&gt;

&lt;p&gt;There are several points here.&lt;/p&gt;

&lt;p&gt;a. Windowing doesn&apos;t return a single value. The output of applying a WindowFunction on a Partition is a Column with the same number of rows as the partition.&lt;br/&gt;
b. The combined output of all the WIndow Functions in a Statement is a Partition that combines output from the individual Wdw Functions.&lt;/p&gt;

&lt;p&gt;Now let me say something about the seperation of Windowing and PTF functionality. There are 4 areas:&lt;/p&gt;

&lt;p&gt;a. The Language level: there is no intersection. The user will not see the connection. One is used as a UDAF; the other whereever tables can appear.&lt;/p&gt;

&lt;p&gt;b. The Ifc/Function writer level: there is no intersection. There is no &apos;Window Function&apos;; a UDAF writer can continue to write UDAFs. They automatically become available in Window expressions. Table Functions are written using the TableFunctionResolver and Evaluator ifcs. This is very different from writing a UDAF. (We have setup a functions branch; you will see some egs of TblFuncs, past the pedantic Noop and NoopWithMap; also we were hoping to add NPath into the first patch).&lt;/p&gt;

&lt;p&gt;c. The Query Specification Level: Here things get a little messy. First let me describe the situation, and then how it is relatively eay to fix. We have extended the QueryBlock(QB) to have the following information(per destination):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;a map of PTF invocations. These are any PTF invocations that appear in the from clause. Roughly equivalent to how SubQuery information is captured in the QB. This information is held in an instance of PTFSpec; which captures all the details of the PTF invocation.&lt;/li&gt;
	&lt;li&gt;a destination may also have a PTFSpec attached which represents the Windowing processing associated with this QB destination.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Here is where things need correction. Our implementation details are leaking into the Specification classes. Since today we use the PTFOp to execute windowing we are capturing the windowing clauses information in a PTFSpec. But it is relatively easy to correct this. We can have a different set of classes to capture the Window processing.&lt;/p&gt;

&lt;p&gt;c.2. The other place where the implemenation is leaking is how we try to optimize Windowing processing when it is combined  with a PTF invocation. (this may be too much detail; the gist of this point is that this too is easily fixable; if not interested skip to point d.) &lt;/p&gt;

&lt;p&gt;When we see that the from clause is only a PTF invocation then we associate the windowing clauses with its PTFSpec; so as to treat both things as one PTF Chain. Once in a chain we use our PTF Chain breaking logic to decide whether Windowing can be done in the same PTFOp for we need to break them. Again this is relatively easy to fix; for now we remove the logic that trys to associate a Windowing processing with an existing PTFSpec on the QB. This keeps the translation clean; the decision to combine can be pushed off to a later stage.&lt;/p&gt;

&lt;p&gt;d. The execution of Windowing: think of PTFOperator as an implementaion for Windowing. With the changes above, it will be easy to choose other available implementaions in the future.&lt;/p&gt;

&lt;p&gt;Finally the execution of Windowing by the PTFOp has some uses:&lt;br/&gt;
d.1&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;the handling of value based ranges: it is more work to predict the boundary of the window; and sometimes it may just make sense to keep the whole partition. For e.g.:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;select  p_mfgr,p_name, p_size,  &lt;br/&gt;
sum(p_size) as s2 over (range between p_size 5 less and current row), &lt;br/&gt;
from part &lt;br/&gt;
distribute by p_mfgr &lt;br/&gt;
sort by p_mfgr, p_name;&lt;/p&gt;

&lt;p&gt;The tradeoff is doing the window calculation for each row to decide how much to keep around vs just keeping the whole thing(spilled to disk if needed)&lt;/p&gt;

&lt;p&gt;d.2&lt;br/&gt;
The support for multiple ordering, with the same partition is possible by doing just one shuffle and then doing a sort of the PersistentPartitionedList backing the Partition. This is much easier to support then to invoke multiple MR jobs and assemble the output into a final Partition.&lt;/p&gt;</comment>
                            <comment id="13559097" author="alangates" created="Mon, 21 Jan 2013 21:04:29 +0000"  >&lt;p&gt;Harish,&lt;/p&gt;

&lt;p&gt;Thanks for you replies.  I want to think on your explanation in 2 above some more, but at least I think I understand your rationale now.  &lt;/p&gt;

&lt;p&gt;One other question.  I tried playing around with this but kept getting an error.  I&apos;m not sure what I&apos;m doing wrong.  I have a table that I created with the following statement:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;create table studenttab10k (name string, age &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;, gpa &lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When I run &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;select avg(gpa) over (cluster by age) from studenttab10k;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I get &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;FAILED: SemanticException 1:43 No partition specification associated with start of PTF chain . Error encountered near token &lt;span class=&quot;code-quote&quot;&gt;&apos;age&apos;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I looked through the syntax file and I think I&apos;m doing the right thing, but obviously I&apos;m not. &lt;/p&gt;</comment>
                            <comment id="13559154" author="rhbutani" created="Mon, 21 Jan 2013 22:06:22 +0000"  >&lt;p&gt;Alan,&lt;/p&gt;

&lt;p&gt;Thanks for spending the time.&lt;/p&gt;

&lt;p&gt;Yes your e.g. is going to fail. There was a bug in the patch we posted.&lt;br/&gt;
This was fixed in commit 0eff864d765c91e0bece497e0f007c6cd2cec72f in our repo on Jan 9th.&lt;br/&gt;
I can send you a patch privately or post the updated patch here. &lt;br/&gt;
Sorry about this. &lt;/p&gt;</comment>
                            <comment id="13559257" author="alangates" created="Tue, 22 Jan 2013 01:06:38 +0000"  >&lt;p&gt;I&apos;d definitely like to get a new version of the patch.  I&apos;m happy to pull from github.  I looked at the repo referenced above ( &lt;a href=&quot;https://github.com/hbutani/SQLWindowing&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/hbutani/SQLWindowing&lt;/a&gt; ) but it didn&apos;t have any recent updates.  &lt;/p&gt;</comment>
                            <comment id="13559263" author="rhbutani" created="Tue, 22 Jan 2013 01:20:51 +0000"  >&lt;p&gt;Its &lt;a href=&quot;https://github.com/hbutani/hive&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/hbutani/hive&lt;/a&gt; (ptf branch)&lt;br/&gt;
The SQLWindowing repo has the work we did on top of hive.&lt;/p&gt;</comment>
                            <comment id="13560860" author="rhbutani" created="Wed, 23 Jan 2013 17:42:09 +0000"  >&lt;p&gt;Alan,&lt;/p&gt;

&lt;p&gt;I have attached a document describing the PTF &amp;amp; Windowing Specification classes. This is a formal description of the changes I was describing above. The output of Phase 1 will be Windowing &amp;amp; PTFInvocation objects attached to the QB.&lt;/p&gt;

&lt;p&gt;The thought process is:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Phase 1 generates Specification that doesn&apos;t tie PTFs and Windowing&lt;/li&gt;
	&lt;li&gt;We alter the PTFTranslator to handle both Windowing &amp;amp; PTFInvocations; for now we keep the PTFDef classes mostly intact.&lt;/li&gt;
	&lt;li&gt;in the future we:&lt;/li&gt;
	&lt;li&gt;build a separate Operator for Windowing, based on the Spec classes.&lt;/li&gt;
	&lt;li&gt;tbd: we refactor the PTFTranslator and Definition classes to share more translation code with the Windowing Operator.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13561116" author="ashutoshc" created="Wed, 23 Jan 2013 21:41:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rhbutani&quot; class=&quot;user-hover&quot; rel=&quot;rhbutani&quot;&gt;Harish Butani&lt;/a&gt; Image in your html doc didn&apos;t make it. May be attach a pdf version containing image? Or, just attaching standalone image is fine as well.&lt;/p&gt;</comment>
                            <comment id="13561145" author="rhbutani" created="Wed, 23 Jan 2013 22:08:08 +0000"  >&lt;p&gt;Done. Sorry about that.&lt;/p&gt;</comment>
                            <comment id="13563594" author="rhbutani" created="Sat, 26 Jan 2013 19:32:19 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Just posted a second preliminary patch. This has:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;support for multiple inserts&lt;/li&gt;
	&lt;li&gt;Finished all the cleanup&lt;/li&gt;
	&lt;li&gt;Added the NPath Table Function&lt;/li&gt;
	&lt;li&gt;Added more tests, now there are around 70 tests.&lt;/li&gt;
	&lt;li&gt;Many bug fixes and minor enhancements, to list a few : don&apos;t interfere with GBys with no Group By clause; support for multiple invocations of same UDAF on diff wdws; allow aliases to be optional in wdw clauses; fix issues with default wdw specification and inheriting partitioning; enhance the PTF ifc to specify output col names; fix bugs in value based wdws; fix issues when mixing distinct and wdwing clauses.&lt;/li&gt;
	&lt;li&gt;made sure all the ql tests pass ( though this is a moving target; we apologize if failures have crept in since we checked yesterday)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We now:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;plan to make the changes listed in the comments to Alan; so there is a clean separation between Windowing &amp;amp; PTFs at the specification level.&lt;/li&gt;
	&lt;li&gt;In parallel we have started a functions branch, where we are testing more PTFs. See out github repo for details.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13563596" author="ashutoshc" created="Sat, 26 Jan 2013 19:37:26 +0000"  >&lt;p&gt;Hey Harish,&lt;br/&gt;
Thanks for updating the patch. While I was playing with previous patch, though unit tests ran fine. I found ptf queries failed when I ran them on cluster. I found out that antlr-runtime jar is required at tasktracker, so I need to &apos;add jar&apos; to send it across. This increases our runtime dependency. Though, thats not my biggest concern. On investigating a bit, I found this is due to we are serializing antlr structures from client to backend. I think we should get rid of this. We should populate all the information we need in hive&apos;s data structure in front end  ( probably in *desc objects) and than use these structures to pull out the information. Having pieces of antlr structures at backend is not ideal. Does the latest patch improves this situation?&lt;/p&gt;</comment>
                            <comment id="13563617" author="rhbutani" created="Sat, 26 Jan 2013 20:18:23 +0000"  >&lt;p&gt;No this needs addressing.  Currently the Definition classes ( same as Desc classes) are holding onto the Spec classes which have the ASTNodes. Most cases it is easy to make the Spec reference transient (or better yet move the association outside of the Def classes). There are couple of places that the ASTNode is being referenced; need to take a closer look.  I am in process of refactoring the Spec classes; will have better sense in a couple of days.&lt;/p&gt;</comment>
                            <comment id="13566946" author="ashutoshc" created="Wed, 30 Jan 2013 21:57:10 +0000"  >&lt;p&gt;PTFDesc only contains a serialized string for PTFDef. I think we should just merge these two classes. Rename the existing PTFDef to PTFDesc and removing the existing PTFDef. And than make sure that PTFDesc is serializable. Does that sound right?&lt;/p&gt;
</comment>
                            <comment id="13566952" author="ashutoshc" created="Wed, 30 Jan 2013 22:03:29 +0000"  >&lt;p&gt;Also need to make sure that ASTNode and other antlr datastructures referenced (directly or via contained fields) in this new PTFDesc are not required in PTFOperator and are thus not serialized, thereby eliminating antlr runtime dependency.&lt;/p&gt;</comment>
                            <comment id="13567271" author="rhbutani" created="Thu, 31 Jan 2013 02:42:23 +0000"  >&lt;p&gt;Yes, exactly. Will start to introduce the new Spec classes as noted in the DataStruct attachment, and refactor the Def classes to remove the antlr dependency. &lt;/p&gt;

&lt;p&gt;But before doing this had to handle the following issue. So the plan we generate has the form &lt;br/&gt;
... -&amp;gt; ReduceSink -&amp;gt; Extract -&amp;gt; PTF Op -&amp;gt; ...&lt;br/&gt;
The Reduce Sink RowResolver contains the Virtual Columns from its input Operators. During translation we set the RowResolver of the Extract Op to be the same as the Reduce Sink RR; and this same RR was used to setup the ExprNodeDescs in PTF translation. But at runtime the Extract Op doesn&apos;t contain the Virtual Columns and so the internal column names can be different. For e.g. in our testJoinWithLeadLag testCase, which is a self join on part and also has a Windowing expression. The RR of the RS op at translation time looks something like this:&lt;br/&gt;
  (_co1,_col2,..,_col7, _col8(vc=true),_col9(vc=true),_col10,_col11,.._col15(vc=true),_col16(vc=true),..)&lt;br/&gt;
At runtime the Virtual columns are removed and all the columns after _col7 are shifted 1 or 2 positions. So in child Operators ColumnExprNodeDescs are no longer referring to the right columns.&lt;br/&gt;
We were handling this issue by recreating the ExprNodeDescs from the ASTNodes at runtime. &lt;br/&gt;
So to avoid carrying forward the ASTNodes we now build a new RR for the Extract Op, with the Virtual Columns removed. We hand this to the PTFTranslator as the starting RR to use to translate a PTF Chain. &lt;/p&gt;

&lt;p&gt;With the above change, now it should be possible to use the ExprNodeDescs created during translation in the execution of the PTF Op. So will now start a sequence of steps to move to the new data structures and avoid recreation of ExprNodeDescs at runtime. &lt;/p&gt;

&lt;p&gt;I apologize if I am not being clear. This is a little hard to explain w/o walking through an example. Happy to go over this in detail offline.&lt;/p&gt;</comment>
                            <comment id="13567779" author="ashutoshc" created="Thu, 31 Jan 2013 16:31:23 +0000"  >&lt;p&gt;That sounds like a bug in existing trunk. If I got this right than query involving sort/distribute by and virtual columns would yield incorrect result on current trunk. If so, perhaps we should address this bug in a separate ticket for trunk? &lt;/p&gt;</comment>
                            <comment id="13567856" author="pkalmegh" created="Thu, 31 Jan 2013 17:38:47 +0000"  >&lt;p&gt;This is not exactly a bug. In the existing trunk, the ExtractOperator is followed by a FileSinkOperator and hence does not have this problem. For queries like below:&lt;/p&gt;

&lt;p&gt;select p1.p_mfgr, p1.p_name, &lt;br/&gt;
p1.p_size &lt;br/&gt;
from part p1 join part p2 on p1.p_partkey = p2.p_partkey &lt;br/&gt;
distribute by p1.p_mfgr &lt;br/&gt;
sort by p1.p_name;&lt;/p&gt;

&lt;p&gt;a SelectOperator after JoinOperator solves this problem by filtering the virtual columns (VCs) and setting up a correct RR for ReduceSinkOperator. We cannot insert a SelectOperator in our case as the PTF chain is a black-box for us. &lt;/p&gt;

&lt;p&gt;In queries with the PTFOperator, we use the RowResolver of the ExtractOperator to construct ExprNodeDescs during translation. The problem here is: if we do not filter out the VCs from the ExtractOperator and use them during translation, the ColumnPrunerTableScanProc adds these VCs in the newVirtualCols List. This causes a non-empty virtualCols on TableScanDesc. During runtime, in the MapOperator the &apos;hasVC&apos; boolean is set to true eventually resulting in a ClassCastException in ReduceSinkOperator during row evaluation. This problem occurs particularly for queries involving join with PTF (We can walk through some examples offline to explain why this is not a problem for queries with a PTF and no join). So currently, we are filtering the VCs and setting up a new RowResolver for ExtractOperator during translation so that the columns at runtime match with those during translation. &lt;/p&gt;</comment>
                            <comment id="13567950" author="ashutoshc" created="Thu, 31 Jan 2013 19:00:44 +0000"  >&lt;p&gt;Thanks Prajakta for explanation. Make sense, But to your last point which Harish also noted in previous comment:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;So currently, we are filtering the VCs and setting up a new RowResolver for ExtractOperator during translation so that the columns at runtime match with those during translation.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think we cannot always filter out VC&apos;s for RR of ExtractOp. That will depend on whether user has selected VCs or not in the query. No? &lt;/p&gt;</comment>
                            <comment id="13568018" author="ashutoshc" created="Thu, 31 Jan 2013 19:38:00 +0000"  >&lt;p&gt;Did some further testing on my 1-node pseudo cluster. After getting past dependency problems, I hit this OOM exception. This is with &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;-- 10. testOnlyPTF 
select p_mfgr, p_name, p_size from noop(part distribute by p_mfgr sort by p_name);


java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing row (tag=0) {&lt;span class=&quot;code-quote&quot;&gt;&quot;key&quot;&lt;/span&gt;:{&lt;span class=&quot;code-quote&quot;&gt;&quot;reducesinkkey0&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;Manufacturer#2&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;reducesinkkey1&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;almond antique violet chocolate turquoise&quot;&lt;/span&gt;},&lt;span class=&quot;code-quote&quot;&gt;&quot;value&quot;&lt;/span&gt;:{&lt;span class=&quot;code-quote&quot;&gt;&quot;_col0&quot;&lt;/span&gt;:105685,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col1&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;almond antique violet chocolate turquoise&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col2&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;Manufacturer#2&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col3&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;Brand#22&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col4&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;MEDIUM ANODIZED COPPER&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col5&quot;&lt;/span&gt;:14,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col6&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;MED CAN&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col7&quot;&lt;/span&gt;:1690.68,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col8&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;ly pending requ&quot;&lt;/span&gt;},&lt;span class=&quot;code-quote&quot;&gt;&quot;alias&quot;&lt;/span&gt;:0}
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:274)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:518)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:419)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:259)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:253)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing row (tag=0) {&lt;span class=&quot;code-quote&quot;&gt;&quot;key&quot;&lt;/span&gt;:{&lt;span class=&quot;code-quote&quot;&gt;&quot;reducesinkkey0&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;Manufacturer#2&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;reducesinkkey1&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;almond antique violet chocolate turquoise&quot;&lt;/span&gt;},&lt;span class=&quot;code-quote&quot;&gt;&quot;value&quot;&lt;/span&gt;:{&lt;span class=&quot;code-quote&quot;&gt;&quot;_col0&quot;&lt;/span&gt;:105685,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col1&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;almond antique violet chocolate turquoise&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col2&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;Manufacturer#2&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col3&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;Brand#22&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col4&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;MEDIUM ANODIZED COPPER&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col5&quot;&lt;/span&gt;:14,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col6&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;MED CAN&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col7&quot;&lt;/span&gt;:1690.68,&lt;span class=&quot;code-quote&quot;&gt;&quot;_col8&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;ly pending requ&quot;&lt;/span&gt;},&lt;span class=&quot;code-quote&quot;&gt;&quot;alias&quot;&lt;/span&gt;:0}
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:262)
	... 7 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.hive.ql.exec.PTFPersistence.createList(PTFPersistence.java:47)
	at org.apache.hadoop.hive.ql.exec.PTFPartition.init(PTFPartition.java:42)
	at org.apache.hadoop.hive.ql.exec.PTFPartition.&amp;lt;init&amp;gt;(PTFPartition.java:35)
	at org.apache.hadoop.hive.ql.exec.PTFOperator.createFirstPartitionForChain(PTFOperator.java:371)
	at org.apache.hadoop.hive.ql.exec.PTFOperator.processOp(PTFOperator.java:133)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:521)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:849)
	at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:521)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:253)
	... 7 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hive.ql.exec.PTFPersistence.createList(PTFPersistence.java:43)
	... 16 more
Caused by: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.hive.ql.exec.PTFPersistence$ByteBasedList.&amp;lt;init&amp;gt;(PTFPersistence.java:77)
	at org.apache.hadoop.hive.ql.exec.PTFPersistence$PartitionedByteBasedList.addPartition(PTFPersistence.java:407)
	at org.apache.hadoop.hive.ql.exec.PTFPersistence$PartitionedByteBasedList.&amp;lt;init&amp;gt;(PTFPersistence.java:386)
	... 21 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;I tried with other queries as well and all of them failed there. Note that these queries are from ptf_general_queries.q which when run via ant for unit tests, passes fine.&lt;/p&gt;
</comment>
                            <comment id="13568035" author="rhbutani" created="Thu, 31 Jan 2013 19:50:49 +0000"  >&lt;p&gt;can you try&lt;br/&gt;
set hive.ptf.partition.persistence.memsize=something lower&lt;br/&gt;
default is 64MB.&lt;/p&gt;</comment>
                            <comment id="13568053" author="ashutoshc" created="Thu, 31 Jan 2013 20:08:14 +0000"  >&lt;p&gt;Yeah that worked fine. My 1-node pseudo cluster was running with default config which resulted in Xmx value of 256M which wasn&apos;t well equipped to handle arrays of 64MB : ) worked fine with 10MB. &lt;/p&gt;
</comment>
                            <comment id="13568420" author="ashutoshc" created="Fri, 1 Feb 2013 03:08:25 +0000"  >&lt;p&gt;Created phabricator review entry to ease review : &lt;a href=&quot;https://reviews.facebook.net/D8331&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D8331&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13568425" author="rhbutani" created="Fri, 1 Feb 2013 03:16:57 +0000"  >&lt;p&gt;Ashutosh,&lt;/p&gt;

&lt;p&gt;So the plan looks like this:&lt;/p&gt;

&lt;p&gt;... -&amp;gt; ReduceSink -&amp;gt; Extract -&amp;gt; PTFOp&lt;/p&gt;

&lt;p&gt;We don&apos;t know what columns a PTF will access; the contract is it has access to all columns in its input. So we don&apos;t want any Column Pruning to happen. So we don&apos;t put a Select Op before the Reduce Sink. At translation time we see all the Columns, including the VCs. It appears as though during optimization VCs are carried forward only if required; so at runtime the ColumnExprNodeDescs are referring to the wrong internalNames. Does this make sense? Is there a way to carry forward the VCs when a PTF is present. The other option is (which we have taken is) to say VCs are not available to PTFs.&lt;/p&gt;

&lt;p&gt;Having said this, when the PTF is Windowing, we do know the columns being referred; so we should put a Select Op in front of the ReduceSink.&lt;/p&gt;</comment>
                            <comment id="13569211" author="alangates" created="Fri, 1 Feb 2013 23:00:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rhbutani&quot; class=&quot;user-hover&quot; rel=&quot;rhbutani&quot;&gt;Harish Butani&lt;/a&gt; I&apos;ve been looking at the syntax for invoking a PTF in your patch.  If I read it correctly it looks something like:&lt;/p&gt;

&lt;p&gt;functionname ( expr_list ON source ... )&lt;/p&gt;

&lt;p&gt;where expr_list is a comma separated list of expressions, source is a table, subquery or another PTF, and ... is the DISTRIBUTE BY etc.&lt;/p&gt;

&lt;p&gt;Is that right?  If so, what is that list of expressions?  Are those arguments to be passed to the PTF?  Also, if I read it correctly the ON keyword is optional.  Does ON not being there have some meaning or do you just see it as a noise word so you&apos;re allowing it to be optional? &lt;/p&gt;</comment>
                            <comment id="13569257" author="rhbutani" created="Fri, 1 Feb 2013 23:41:18 +0000"  >&lt;p&gt;Yes you got most of it correct. So if there are no arguments the form is:&lt;/p&gt;

&lt;p&gt;functioname(source....) for e.g. ptf1(part distribute by mfgr sort by name)&lt;/p&gt;

&lt;p&gt;If there are arguments the form is:&lt;/p&gt;

&lt;p&gt;functionname(args on source ...) for e.g. ptf2(arg1, arg2 on part distribute by mfgr sort by name)&lt;/p&gt;

&lt;p&gt;If you have args the ON is required. Probably can make the ON optional here. I think there was some ambiguity if ON is optional; didn&apos;t want to deal with it in the first pass... If you want I can go back and check.&lt;/p&gt;</comment>
                            <comment id="13569324" author="alangates" created="Sat, 2 Feb 2013 00:50:07 +0000"  >&lt;p&gt;I was wondering if there&apos;s a &quot;standard&quot; way to do selections from partition table functions in SQL.  (I know it isn&apos;t actually part of the standard yet.)  If there&apos;s a way others have done it, it makes sense to align Hive&apos;s syntax with those, as long as theirs is rational. &lt;/p&gt;

&lt;p&gt;The only other one I&apos;m aware of is Aster.  There syntax is:&lt;/p&gt;

&lt;p&gt;functionname ( ON source [PARTITION BY expression &lt;span class=&quot;error&quot;&gt;&amp;#91;ORDER BY expression&amp;#93;&lt;/span&gt;] &lt;span class=&quot;error&quot;&gt;&amp;#91;key_value_list&amp;#93;&lt;/span&gt; )&lt;/p&gt;

&lt;p&gt;where a key_value_list is:  key(value)&lt;span class=&quot;error&quot;&gt;&amp;#91;, key(value)...&amp;#93;&lt;/span&gt; where key is the name of the argument in the function and value is the value to be passed.&lt;/p&gt;

&lt;p&gt;We&apos;ve already agreed to add PARTITION BY and ORDER BY as synonyms for DISTRIBUTE/CLUSTER BY and SORT BY.  It seems to me it would make sense to do the function argument passing in the same way to avoid confusion for users.&lt;/p&gt;</comment>
                            <comment id="13569345" author="rhbutani" created="Sat, 2 Feb 2013 01:20:53 +0000"  >&lt;p&gt;Yes Aster is the one I had looked at too. The other one I looked at was Pipelined Partitioned Table Functions in Oracle; they are bit different, the partitioning information is part of the Function definition.&lt;/p&gt;

&lt;p&gt;I am assuming key is an identifier; so this is really an invocation using named parameters.&lt;br/&gt;
Since Hive doesn&apos;t have named parameters for other functions, could we do positional parameters for now? &lt;br/&gt;
Later, do named parameters for all functions not just PTFs.&lt;/p&gt;</comment>
                            <comment id="13570306" author="rhbutani" created="Mon, 4 Feb 2013 14:50:11 +0000"  >&lt;p&gt;Attached patch to be used as starting point for hive branch. &lt;br/&gt;
Has minor changes since the last patch.&lt;/p&gt;</comment>
                            <comment id="13583650" author="ashutoshc" created="Thu, 21 Feb 2013 23:07:20 +0000"  >&lt;p&gt;This is an umbrella jira for this feature. All the sub-tasks are linked to it.&lt;/p&gt;</comment>
                            <comment id="13593095" author="hagleitn" created="Tue, 5 Mar 2013 05:53:24 +0000"  >&lt;p&gt;.4 has all the changes from the branch in a single patch.&lt;/p&gt;</comment>
                            <comment id="13617954" author="rhbutani" created="Sat, 30 Mar 2013 02:03:35 +0000"  >&lt;p&gt;uploaded .5&lt;/p&gt;</comment>
                            <comment id="13618537" author="ashutoshc" created="Mon, 1 Apr 2013 01:04:54 +0000"  >&lt;p&gt;I am +1 on this. I have overseen this entire work, have reviewed all ~45 patches which went in the branch. With ~200 tests (including -ve ones), feature is well tested. Code is well commented and relatively easier to follow. &lt;/p&gt;</comment>
                            <comment id="13619526" author="ashutoshc" created="Tue, 2 Apr 2013 05:19:01 +0000"  >&lt;p&gt;Running tests. Will commit if tests pass.&lt;/p&gt;</comment>
                            <comment id="13619835" author="ashutoshc" created="Tue, 2 Apr 2013 14:17:25 +0000"  >&lt;p&gt;Committed to trunk. Thanks, Harish for this awesome feature!&lt;/p&gt;</comment>
                            <comment id="13619850" author="brocknoland" created="Tue, 2 Apr 2013 14:28:44 +0000"  >&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;Nice work guys! Sorry I have been OOO so I haven&apos;t been able to help lately.&lt;/p&gt;</comment>
                            <comment id="13619930" author="ashutoshc" created="Tue, 2 Apr 2013 15:38:46 +0000"  >&lt;p&gt;I must acknowledge other folks have also contributed in this effort including &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brocknoland&quot; class=&quot;user-hover&quot; rel=&quot;brocknoland&quot;&gt;Brock Noland&lt;/a&gt; , &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pkalmegh&quot; class=&quot;user-hover&quot; rel=&quot;pkalmegh&quot;&gt;Prajakta Kalmegh&lt;/a&gt; and myself. Thanks everyone for their time and enthusiasm to see this through.&lt;/p&gt;</comment>
                            <comment id="13620104" author="hagleitn" created="Tue, 2 Apr 2013 18:23:16 +0000"  >&lt;p&gt;Amazing! Thanks Harish!&lt;/p&gt;</comment>
                            <comment id="13620226" author="eric14" created="Tue, 2 Apr 2013 20:15:14 +0000"  >&lt;p&gt;very cool&lt;/p&gt;


</comment>
                            <comment id="13620592" author="rhbutani" created="Wed, 3 Apr 2013 04:25:11 +0000"  >&lt;p&gt;I second Ashutosh&apos;s comment. Thanks Prajakta, Ashutosh, Brock and Alan for your help and support.&lt;/p&gt;</comment>
                            <comment id="13622998" author="hudson" created="Thu, 4 Apr 2013 23:39:46 +0000"  >&lt;p&gt;Integrated in Hive-trunk-hadoop2 #138 (See &lt;a href=&quot;https://builds.apache.org/job/Hive-trunk-hadoop2/138/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hive-trunk-hadoop2/138/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-896&quot; title=&quot;Add LEAD/LAG/FIRST/LAST analytical windowing functions to Hive.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-896&quot;&gt;&lt;del&gt;HIVE-896&lt;/del&gt;&lt;/a&gt; : Add LEAD/LAG/FIRST/LAST analytical windowing functions to Hive. (Harish Butani via Ashutosh Chauhan) (Revision 1463556)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
hashutosh : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1463556&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1463556&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hive/trunk/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/data/files/flights_tiny.txt&lt;/li&gt;
	&lt;li&gt;/hive/trunk/data/files/over10k&lt;/li&gt;
	&lt;li&gt;/hive/trunk/data/files/part.rc&lt;/li&gt;
	&lt;li&gt;/hive/trunk/data/files/part.seq&lt;/li&gt;
	&lt;li&gt;/hive/trunk/data/files/part_tiny.txt&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/if/queryplan.thrift&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/gen/thrift/gen-cpp/queryplan_types.cpp&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/gen/thrift/gen-cpp/queryplan_types.h&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Operator.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/OperatorType.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Query.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Stage.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Task.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/gen/thrift/gen-php/Types.php&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/gen/thrift/gen-py/queryplan/ttypes.py&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/gen/thrift/gen-rb/queryplan_types.rb&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/QueryProperties.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorFactory.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFFunctionInfo.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPersistence.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFUtils.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionTableFunctionDescription.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/WindowFunctionDescription.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/WindowFunctionInfo.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/hooks/LineageInfo.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPruner.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/optimizer/lineage/Generator.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/FromClauseParser.g&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/IdentifiersParser.g&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/PTFInvocationSpec.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/PTFTranslator.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/SelectClauseParser.g&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingComponentizer.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingExprNodeEvaluatorFactory.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDesc.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/ppd/PredicatePushDown.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCumeDist.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFDenseRank.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFFirstValue.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFLag.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFLastValue.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFLead.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFLeadLag.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFNTile.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFPercentRank.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFRank.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFRowNumber.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFLeadLag.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/NPath.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/Noop.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/NoopWithMap.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/TableFunctionEvaluator.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/TableFunctionResolver.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_negative_AggrFuncsWithNoGBYNoPartDef.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_negative_AmbiguousWindowDefn.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_negative_DistributeByOrderBy.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_negative_DuplicateWindowAlias.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_negative_HavingLeadWithNoGBYNoWindowing.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_negative_HavingLeadWithPTF.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_negative_InvalidValueBoundary.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_negative_JoinWithAmbigousAlias.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_negative_PartitionBySortBy.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_negative_WhereWithRankCond.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_window_boundaries.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientnegative/ptf_window_boundaries2.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/leadlag.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/leadlag_queries.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/ptf.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/ptf_general_queries.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/ptf_npath.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/ptf_rcfile.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/ptf_seqfile.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/windowing.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/windowing_columnPruning.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/windowing_expressions.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/windowing_multipartitioning.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/windowing_navfn.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/windowing_ntile.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/windowing_rank.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/windowing_udaf.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/queries/clientpositive/windowing_windowspec.q&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_negative_AggrFuncsWithNoGBYNoPartDef.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_negative_AmbiguousWindowDefn.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_negative_DistributeByOrderBy.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_negative_DuplicateWindowAlias.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_negative_HavingLeadWithNoGBYNoWindowing.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_negative_HavingLeadWithPTF.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_negative_InvalidValueBoundary.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_negative_JoinWithAmbigousAlias.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_negative_PartitionBySortBy.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_negative_WhereWithRankCond.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_window_boundaries.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientnegative/ptf_window_boundaries2.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/leadlag.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/leadlag_queries.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/ptf.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/ptf_general_queries.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/ptf_npath.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/ptf_rcfile.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/ptf_seqfile.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/show_functions.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/windowing.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/windowing_columnPruning.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/windowing_expressions.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/windowing_multipartitioning.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/windowing_navfn.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/windowing_ntile.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/windowing_rank.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/windowing_udaf.q.out&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/results/clientpositive/windowing_windowspec.q.out&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13724575" author="gbraccialli" created="Tue, 30 Jul 2013 23:11:30 +0000"  >&lt;p&gt;Harish,&lt;/p&gt;

&lt;p&gt;I noticed that NPath class is on Hive 0.11 source and it&apos;s also a known function on hive. Is it working? Could you please give us a sample query? I tried the query below, but its not working.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;

&lt;p&gt;create external table flights_tiny (ORIGIN_CITY_NAME string, DEST_CITY_NAME string, YEAR int, MONTH int, DAY_OF_MONTH int, ARR_DELAY float, FL_NUM string)&lt;br/&gt;
location &apos;/user/xxxxx&apos;;&lt;/p&gt;

&lt;p&gt;select npath(&lt;br/&gt;
&apos;ONTIME.LATE+&apos;, &lt;br/&gt;
&apos;LATE&apos;, arr_delay &amp;gt; 15, &lt;br/&gt;
&apos;EARLY&apos;, arr_delay &amp;lt; 0,&lt;br/&gt;
&apos;ONTIME&apos;, arr_delay &amp;gt;=0 and arr_delay &amp;lt;= 15,&lt;br/&gt;
&apos;origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath as tpath&apos;&lt;br/&gt;
)&lt;br/&gt;
from flights_tiny;&lt;/p&gt;

&lt;p&gt;FAILED: NullPointerException null&lt;/p&gt;</comment>
                            <comment id="13724866" author="rhbutani" created="Wed, 31 Jul 2013 04:48:50 +0000"  >&lt;p&gt;The syntax now is different from the original patch. Here is an e.g. from ptf_npath.q test file&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;select origin_city_name, fl_num, year, month, day_of_month, sz, tpath 
from npath(on 
        flights_tiny 
        distribute by fl_num 
        sort by year, month, day_of_month  
      arg1(&apos;LATE.LATE+&apos;), 
      arg2(&apos;LATE&apos;), arg3(arr_delay &amp;gt; 15), 
    arg4(&apos;origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath&apos;) 
   );  
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One thing I should point out is that it is not clear if NPath will remain in its current form. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12637457">HIVE-4197</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12461990">HIVE-1304</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12663566">HIVE-5087</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                            <outwardlinks description="requires">
                                        <issuelink>
            <issuekey id="12632913">HIVE-4037</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12630793">HIVE-3984</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12630794">HIVE-3985</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12630795">HIVE-3986</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12630801">HIVE-3987</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12632693">HIVE-4028</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12632717">HIVE-4030</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12632778">HIVE-4034</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12632901">HIVE-4035</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12632903">HIVE-4036</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12633126">HIVE-4041</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12633525">HIVE-4052</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12634348">HIVE-4080</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12634349">HIVE-4081</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12633125">HIVE-4040</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12632256">HIVE-4019</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12633475">HIVE-4048</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12630673">HIVE-3981</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12630727">HIVE-3982</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12632718">HIVE-4031</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310051">
                    <name>Supercedes</name>
                                            <outwardlinks description="supercedes">
                                        <issuelink>
            <issuekey id="12441553">HIVE-952</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12566198" name="DataStructs.pdf" size="347554" author="rhbutani" created="Wed, 23 Jan 2013 22:06:08 +0000"/>
                            <attachment id="12562281" name="HIVE-896.1.patch.txt" size="629140" author="rhbutani" created="Sun, 23 Dec 2012 18:49:50 +0000"/>
                            <attachment id="12572037" name="HIVE-896.4.patch" size="1243946" author="hagleitn" created="Tue, 5 Mar 2013 05:53:24 +0000"/>
                            <attachment id="12576212" name="HIVE-896.5.patch.txt" size="2415784" author="rhbutani" created="Sat, 30 Mar 2013 02:03:10 +0000"/>
                            <attachment id="12566638" name="Hive-896.2.patch.txt" size="1191811" author="rhbutani" created="Sat, 26 Jan 2013 19:27:01 +0000"/>
                            <attachment id="12567841" name="hive-896.3.patch.txt" size="1232180" author="rhbutani" created="Mon, 4 Feb 2013 14:48:46 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 10 Jul 2011 12:37:10 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>42689</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 25 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0lcbj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122646</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-897] fix inconsistent expectations from table/partition location value</title>
                <link>https://issues.apache.org/jira/browse/HIVE-897</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;currently code expects this to be full URI in some locations (LoadSemanticAnalyzer). Also HiveAlterHandle should work in either case. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12438831">HIVE-897</key>
            <summary>fix inconsistent expectations from table/partition location value</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="prasadc">Prasad Chakka</reporter>
                        <labels>
                    </labels>
                <created>Thu, 22 Oct 2009 18:16:41 +0000</created>
                <updated>Sun, 4 Jul 2010 14:23:42 +0000</updated>
                                                                                <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12878940" author="raj_velu" created="Tue, 15 Jun 2010 11:38:09 +0000"  >&lt;p&gt;Prasad, do we have any details on this issue, We were looking at fixing this but the problem statement is very abstract.&lt;/p&gt;</comment>
                            <comment id="12884338" author="prasadc" created="Thu, 1 Jul 2010 16:20:13 +0000"  >&lt;p&gt;@Velu,&lt;br/&gt;
I don&apos;t remember details any more. I think the problem is that code expectes full URI in some cases (LoadSemanticAnalyzer) and relative URI in some cases. The metastore stores whatever is given to it so the metastore db can contain either full or relative depending on how the table is created. The purpose of this task to make the handling of &apos;location&apos; parameter uniform throughout the code (hive.metastore, hive.ql.metastore &amp;amp; load &amp;amp; move related classes).&lt;/p&gt;

&lt;p&gt;Feel free to take over as appropriate.&lt;/p&gt;</comment>
                            <comment id="12885036" author="raj_velu" created="Sun, 4 Jul 2010 14:23:42 +0000"  >&lt;p&gt;Thanks Prasad, will look at this... &lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 15 Jun 2010 11:38:09 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>42688</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 30 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0lcbr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122647</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>


<item>
            <title>[HIVE-898] Hive ODBC build fails on OSX</title>
                <link>https://issues.apache.org/jira/browse/HIVE-898</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Building the Hive ODBC library on OS X fails with the following error:&lt;/p&gt;

&lt;p&gt;     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt; g++ -m32 -DARCH32 -shared /Users/carl/src/SVN/hive-trunk2/build/service/fb303/objs/FacebookService.o /Users/carl/src/SVN/hive-trunk2/build/service/fb303/objs/fb303_constants.o /Users/carl/src/SVN/hive-trunk2/build/service/fb303/objs/fb303_types.o /Users/carl/src/SVN/hive-trunk2/build/metastore/objs/ThriftHiveMetastore.o /Users/carl/src/SVN/hive-trunk2/build/metastore/objs/hive_metastore_constants.o /Users/carl/src/SVN/hive-trunk2/build/metastore/objs/hive_metastore_types.o /Users/carl/src/SVN/hive-trunk2/build/service/objs/ThriftHive.o /Users/carl/src/SVN/hive-trunk2/build/service/objs/hive_service_constants.o /Users/carl/src/SVN/hive-trunk2/build/service/objs/hive_service_types.o /Users/carl/src/SVN/hive-trunk2/build/odbc/objs/hiveclient.o /Users/carl/src/SVN/hive-trunk2/build/odbc/objs/HiveResultSet.o /Users/carl/src/SVN/hive-trunk2/build/odbc/objs/HiveColumnDesc.o /Users/carl/src/SVN/hive-trunk2/build/odbc/objs/HiveRowSet.o /Users/carl/src/SVN/hive-trunk2/build/odbc/objs/hiveclienthelper.o -L/Users/carl/tmp/thrift-install/lib -lthrift -o /Users/carl/src/SVN/hive-trunk2/build/odbc/lib/libhiveclient.so.1.0.0 \&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;         &amp;amp;&amp;amp; ln -sf libhiveclient.so.1.0.0 /Users/carl/src/SVN/hive-trunk2/build/odbc/lib/libhiveclient.so&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt; Undefined symbols:&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;   &quot;_main&quot;, referenced from:&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt;       start in crt1.10.5.o&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt; ld: symbol(s) not found&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt; collect2: ld returned 1 exit status&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;exec&amp;#93;&lt;/span&gt; make: *** &lt;span class=&quot;error&quot;&gt;&amp;#91;/Users/carl/src/SVN/hive-trunk2/build/odbc/lib/libhiveclient.so.1.0.0&amp;#93;&lt;/span&gt; Error 1&lt;/p&gt;

&lt;p&gt;BUILD FAILED&lt;br/&gt;
/Users/carl/src/SVN/hive-trunk2/build.xml:126: The following error occurred while executing this line:&lt;br/&gt;
/Users/carl/src/SVN/hive-trunk2/build.xml:69: The following error occurred while executing this line:&lt;br/&gt;
/Users/carl/src/SVN/hive-trunk2/odbc/build.xml:62: exec returned: 2&lt;/p&gt;


&lt;p&gt;The problem is that OSX does not support -shared. You need to either use -dynamiclib or -bundle when making dynamic libraries.&lt;/p&gt;
</description>
                <environment>&lt;p&gt;OS X 10.5.8&lt;/p&gt;</environment>
        <key id="12438964">HIVE-898</key>
            <summary>Hive ODBC build fails on OSX</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nzhang">Ning Zhang</assignee>
                                    <reporter username="cwsteinbach">Carl Steinbach</reporter>
                        <labels>
                    </labels>
                <created>Fri, 23 Oct 2009 22:37:01 +0000</created>
                <updated>Sat, 17 Dec 2011 00:05:23 +0000</updated>
                            <resolved>Wed, 27 Jan 2010 02:35:40 +0000</resolved>
                                                    <fixVersion>0.5.0</fixVersion>
                                    <component>Build Infrastructure</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="12769601" author="nzhang" created="Sat, 24 Oct 2009 06:54:41 +0000"  >&lt;p&gt;+1&lt;br/&gt;
thanks for making this work on Mac.&lt;/p&gt;</comment>
                            <comment id="12805307" author="nzhang" created="Wed, 27 Jan 2010 01:04:24 +0000"  >&lt;p&gt;I found the patch stopped working the trunk now. I&apos;m uploading a new patch based on Carl&apos;s previous patch. This patches solves the following compilation errors:&lt;br/&gt;
 1) assert.h should be included in hiveclienthelper.cpp&lt;br/&gt;
 2) if boost is included other than the /usr/include or the C compiler&apos;s default include directory, compilation error since boost/include cannot be found. I modified the odbc/build.xml and odbc/Makefile to add a variable boost.home so that you can pointing it to the boost&apos;s the install directory. Say if you install boost and thrift in /opt/local, you can compile the ODBC client by &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  $ ant compile-cpp -Dthrift.home=/opt/local -Dboost.home=/opt/local -Dword.size=32
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12805312" author="cwsteinbach" created="Wed, 27 Jan 2010 01:39:16 +0000"  >&lt;p&gt;+1 works for me!&lt;/p&gt;</comment>
                            <comment id="12805324" author="zshao" created="Wed, 27 Jan 2010 02:35:39 +0000"  >&lt;p&gt;Committed to branch-0.4, 0,5 and trunk. Thanks Carl and Ning!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12423079" name="HIVE-898.patch" size="533" author="cwsteinbach" created="Fri, 23 Oct 2009 22:50:28 +0000"/>
                            <attachment id="12431489" name="HIVE-898_2.patch" size="2228" author="nzhang" created="Wed, 27 Jan 2010 01:04:24 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 24 Oct 2009 06:54:41 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73232</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0lcbz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122648</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-898&quot; title=&quot;Hive ODBC build fails on OSX&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-898&quot;&gt;&lt;strike&gt;HIVE-898&lt;/strike&gt;&lt;/a&gt;. Hive ODBC build fails on OSX. (Carl Steinbach and Ning Zhang via zshao)</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310230" key="com.atlassian.jira.plugin.system.customfieldtypes:textfield">
                        <customfieldname>Tags</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>ODBC OSX</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-899] Allow &apos;A != B&apos; as well as  &apos;A &lt;&gt; B&apos;</title>
                <link>https://issues.apache.org/jira/browse/HIVE-899</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Users with limited SQL experience are frequently frustrated by the use of &apos;&amp;lt;&amp;gt;&apos; as the not-equals comparison operator. Other SQL-like languages such as Transact-SQL permit the use of &apos;!=&apos; in place of &apos;&amp;lt;&amp;gt;&apos;. &lt;/p&gt;
</description>
                <environment></environment>
        <key id="12438968">HIVE-899</key>
            <summary>Allow &apos;A != B&apos; as well as  &apos;A &lt;&gt; B&apos;</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21140&amp;avatarType=issuetype">Improvement</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cwsteinbach">Carl Steinbach</assignee>
                                    <reporter username="cwsteinbach">Carl Steinbach</reporter>
                        <labels>
                    </labels>
                <created>Fri, 23 Oct 2009 23:13:28 +0000</created>
                <updated>Sat, 17 Dec 2011 00:05:27 +0000</updated>
                            <resolved>Thu, 7 Jan 2010 21:55:32 +0000</resolved>
                                                    <fixVersion>0.5.0</fixVersion>
                                    <component>Query Processor</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                <comments>
                            <comment id="12797804" author="cwsteinbach" created="Thu, 7 Jan 2010 21:55:32 +0000"  >&lt;p&gt;Resolved in &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-1031&quot; title=&quot;&amp;quot;DESCRIBE FUNCTION array&amp;quot; throws ParseException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-1031&quot;&gt;&lt;del&gt;HIVE-1031&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12797825" author="namit" created="Thu, 7 Jan 2010 23:02:23 +0000"  >&lt;p&gt;Both the syntaxes will be supported&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="12410424">HIVE-156</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 7 Jan 2010 23:02:23 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73231</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 3 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0lcc7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122649</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-900] Map-side join failed if there are large number of mappers</title>
                <link>https://issues.apache.org/jira/browse/HIVE-900</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Map-side join is efficient when joining a huge table with a small table so that the mapper can read the small table into main memory and do join on each mapper. However, if there are too many mappers generated for the map join, a large number of mappers will simultaneously send request to read the same block of the small table. Currently Hadoop has a upper limit of the # of request of a the same block (250?). If that is reached a BlockMissingException will be thrown. That cause a lot of mappers been killed. Retry won&apos;t solve but worsen the problem.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12438972">HIVE-900</key>
            <summary>Map-side join failed if there are large number of mappers</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21140&amp;avatarType=issuetype">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="2">Won&apos;t Fix</resolution>
                                        <assignee username="nzhang">Ning Zhang</assignee>
                                    <reporter username="nzhang">Ning Zhang</reporter>
                        <labels>
                    </labels>
                <created>Sat, 24 Oct 2009 00:43:14 +0000</created>
                <updated>Mon, 4 Jan 2010 07:37:33 +0000</updated>
                            <resolved>Mon, 4 Jan 2010 07:37:33 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12769573" author="indigoviolet" created="Sat, 24 Oct 2009 02:51:05 +0000"  >&lt;p&gt;This is a high-priority bug for me, blocking me on fairly important stuff . The workaround that Dhruba had, of downloading data to the client and adding to the distributedcache is a pretty good solution.&lt;/p&gt;</comment>
                            <comment id="12769602" author="nzhang" created="Sat, 24 Oct 2009 07:01:09 +0000"  >&lt;p&gt;distributed cache is definitely one option. It seems it also works on copying file from hdfs: uri in addition to local directory. However based on the documentation of distributed cache, the cached file should be copied at the beginning of the mapper task. This may also have the same network inbound congestion issue if 3000 mappers are trying to copy the same file at the same time. Or is the distributed cache uses a smarter copying mechanism (like hierarchical rather than 1:ALL)? Otherwise distributing the jar file will face the same issue. &lt;/p&gt;</comment>
                            <comment id="12770135" author="namit" created="Mon, 26 Oct 2009 18:49:58 +0000"  >&lt;p&gt;Instead of relying on the mapper to copy each file to the distributed cache - can we rely on the hive client (ExecDriver) to do that ? &lt;br/&gt;
From the work, the client knows the tasks that need to be executed on the map side. Before submitting the job, execute that work.&lt;br/&gt;
The execmapper needs to change to copy from that portion of the work only instead of executing the whole work.&lt;/p&gt;</comment>
                            <comment id="12771544" author="nzhang" created="Thu, 29 Oct 2009 18:38:52 +0000"  >&lt;p&gt;The essential problem is that there are too many mappers are trying to accessing the same block at the same time so that it exceeds the threshold of accessing the same block. Thus the BlockMissingException is thrown. &lt;/p&gt;

&lt;p&gt;Discussed with Namit and Dhruba offline. There are the proposed solutions:&lt;/p&gt;

&lt;p&gt;1) Make the HDFS fault tolerant to this issue. Dhruba mentioned there already exists retry logic implemented in the DFS client code: if the BlockMissingException is throw it will wait about 400ms and retry. If there are still exceptions then wait for 800 ms and so on until 5 unsuccessful retry. This mechanism works for non-correlated simultaneous request of the same block. However in this case, almost all the mappers request the same block at the same time, so their retries will be also at about the same time. So it would be better to introduce a random factor into the wait time. Dhruba will look into the DFS code and working on that. This will solve a broader type of issues beside the map-side join.&lt;/p&gt;

&lt;p&gt;2) Another orthogonal issue brought up by Namit for map-side join is that if there are too many mappers and each of them request the same small table, it comes with a cost of transferring the small file to all these mappers. Even though the BlockMissingException is resolved, the cost is still there and it is proportional to the number of mappers. In this respect it would be better to reduce the number of mappers. But it also comes with the cost that each mappers then has to deal with larger portion of the large table. So we have to tradeoff the network cost of the small table and the processing cost of the large table. Will come with a heuristic on tune the parameters to decide the number of mappers for map join. &lt;/p&gt;</comment>
                            <comment id="12771551" author="prasadc" created="Thu, 29 Oct 2009 18:46:40 +0000"  >&lt;p&gt;just a of the wall idea, temporarily increase the replication factor for this block so that it is available in more racks thus reducing the network cost and also avoiding BlockMissingException. ofcourse, we need to find a way to reliably set the replication factor back to original setting.&lt;/p&gt;</comment>
                            <comment id="12771553" author="prasadc" created="Thu, 29 Oct 2009 18:47:28 +0000"  >&lt;p&gt;@venky, may be you can unblock your work by manually increasing the replication factory to very high and then issuing the query?&lt;/p&gt;</comment>
                            <comment id="12771569" author="nzhang" created="Thu, 29 Oct 2009 19:16:44 +0000"  >&lt;p&gt;@parasad, yes that&apos;s definitely a good idea to scale out mapjoin with a large number of mappers. Dhruba also suggested to increase the replication factor for the small file. But as you mentioned, we need to revert the replication factor before mapjoin finishes or any exception is caught. I&apos;ll also investigate that. &lt;/p&gt;</comment>
                            <comment id="12784995" author="nzhang" created="Wed, 2 Dec 2009 20:54:20 +0000"  >&lt;p&gt;Tried different approaches in Hive and it turns out none of them is perfect. Finally the solution should be in the HDFS side. The JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-767&quot; title=&quot;Job failure due to BlockMissingException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-767&quot;&gt;&lt;del&gt;HDFS-767&lt;/del&gt;&lt;/a&gt; is trying to solve that. &lt;/p&gt;</comment>
                            <comment id="12796105" author="nzhang" created="Mon, 4 Jan 2010 07:37:09 +0000"  >&lt;p&gt;Closing this issue since &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-767&quot; title=&quot;Job failure due to BlockMissingException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-767&quot;&gt;&lt;del&gt;HDFS-767&lt;/del&gt;&lt;/a&gt; is committed.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 24 Oct 2009 02:51:05 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73230</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 4 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0lccf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122650</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-901] Build Hive C++ Client library with Autoconf/Automake</title>
                <link>https://issues.apache.org/jira/browse/HIVE-901</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;In order to avoid more bugs like &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-898&quot; title=&quot;Hive ODBC build fails on OSX&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-898&quot;&gt;&lt;del&gt;HIVE-898&lt;/del&gt;&lt;/a&gt; it would be a good idea to build the C++ Client library using autoconf/automake.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12439022">HIVE-901</key>
            <summary>Build Hive C++ Client library with Autoconf/Automake</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21140&amp;avatarType=issuetype">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="cwsteinbach">Carl Steinbach</reporter>
                        <labels>
                    </labels>
                <created>Sun, 25 Oct 2009 00:30:25 +0000</created>
                <updated>Sat, 6 Dec 2014 09:59:04 +0000</updated>
                                                                            <component>Build Infrastructure</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12974751" author="appodictic" created="Thu, 23 Dec 2010 21:21:59 +0000"  >&lt;p&gt;Is there any licensing issues with bringing GPL build stuff into an apache project (Not sure just throwing it out there)&lt;/p&gt;</comment>
                            <comment id="12974757" author="nzhang" created="Thu, 23 Dec 2010 21:40:30 +0000"  >&lt;p&gt;@edward, I guess this JIRA is about making libhiveclient.so whose path is hive_trunk/odbc. This part is licensed under Apache. The unixODBC part is GPL which is not included in the Hive&apos;s repository. &lt;/p&gt;</comment>
                            <comment id="12974767" author="cwsteinbach" created="Thu, 23 Dec 2010 22:07:49 +0000"  >&lt;p&gt;@Ed: The FSF makes a specific exception for autoconf scripts. See &lt;a href=&quot;http://www.gnu.org/licenses/autoconf-exception.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.gnu.org/licenses/autoconf-exception.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12975348" author="appodictic" created="Mon, 27 Dec 2010 21:55:26 +0000"  >&lt;p&gt;Awesome. Rock on then.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 23 Dec 2010 21:21:59 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>42687</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 5 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i08nsv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>48455</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>


<item>
            <title>[HIVE-902] cli.sh can not correctly identify Hadoop minor version numbers less than 20</title>
                <link>https://issues.apache.org/jira/browse/HIVE-902</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;cli.sh uses the following logic to detect the version of hadoop:&lt;/p&gt;

&lt;p&gt;  version=$($HADOOP version | awk &apos;&lt;/p&gt;
{print $2;}&apos;);&lt;br/&gt;
&lt;br/&gt;
  if [[ $version =~ &quot;^0\.17&quot; ]] || [[ $version =~ &quot;^0\.18&quot; ]] || [[ $version =~ &quot;^0.19&quot; ]]; then&lt;br/&gt;
      exec $HADOOP jar $AUX_JARS_CMD_LINE ${HIVE_LIB}/hive_cli.jar $CLASS $HIVE_OPTS &quot;$@&quot;&lt;br/&gt;
  else&lt;br/&gt;
      # hadoop 20 or newer - skip the aux_jars option. picked up from hiveconf&lt;br/&gt;
      exec $HADOOP jar ${HIVE_LIB}/hive_cli.jar $CLASS $HIVE_OPTS &quot;$@&quot; &lt;br/&gt;
  fi&lt;br/&gt;
&lt;br/&gt;
Apparently bash doesn&apos;t expect you to quote the regex:&lt;br/&gt;
&lt;br/&gt;
% ./bash -version&lt;br/&gt;
GNU bash, version 4.0.0(1)-release (i386-apple-darwin9.8.0)&lt;br/&gt;
&lt;br/&gt;
% hadoop version&lt;br/&gt;
Hadoop 0.19.0&lt;br/&gt;
Subversion &lt;a href=&quot;https://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.19&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.19&lt;/a&gt; -r 713890&lt;br/&gt;
Compiled by ndaley on Fri Nov 14 03:12:29 UTC 2008&lt;br/&gt;
&lt;br/&gt;
% version=$(hadoop version | awk &apos;{print $2;}
&lt;p&gt;&apos;)&lt;/p&gt;

&lt;p&gt;% echo $version&lt;br/&gt;
0.19.0 &lt;a href=&quot;https://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.19&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.19&lt;/a&gt; by&lt;/p&gt;

&lt;p&gt;% [[ $version =~ &quot;^0\.19&quot; ]] &amp;amp;&amp;amp; echo &quot;Yes&quot; || echo &quot;No&quot;&lt;br/&gt;
No&lt;/p&gt;

&lt;p&gt;% [[ $version =~ &quot;^0.19&quot; ]] &amp;amp;&amp;amp; echo &quot;Yes&quot; || echo &quot;No&quot;&lt;br/&gt;
No&lt;/p&gt;

&lt;p&gt;% [[ $version =~ ^0.19 ]] &amp;amp;&amp;amp; echo &quot;Yes&quot; || echo &quot;No&quot;&lt;br/&gt;
Yes&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12439045">HIVE-902</key>
            <summary>cli.sh can not correctly identify Hadoop minor version numbers less than 20</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cwsteinbach">Carl Steinbach</assignee>
                                    <reporter username="cwsteinbach">Carl Steinbach</reporter>
                        <labels>
                    </labels>
                <created>Mon, 26 Oct 2009 02:14:00 +0000</created>
                <updated>Sat, 17 Dec 2011 00:06:02 +0000</updated>
                            <resolved>Mon, 2 Nov 2009 07:20:42 +0000</resolved>
                                                    <fixVersion>0.4.1</fixVersion>
                    <fixVersion>0.5.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12772391" author="zshao" created="Mon, 2 Nov 2009 00:19:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-902&quot; title=&quot;cli.sh can not correctly identify Hadoop minor version numbers less than 20&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-902&quot;&gt;&lt;del&gt;HIVE-902&lt;/del&gt;&lt;/a&gt;.1.patch:&lt;/p&gt;

&lt;p&gt;I remember there is a hadoop version 0.17.2.1. It seems the patch does not handle this case.&lt;br/&gt;
Can you change the regex to work for that?&lt;br/&gt;
Also, please print out the hadoop version in the error message in case we cannot detect hadoop version, to make it easier to debug.&lt;/p&gt;

&lt;p&gt;Can you also help try it out on earlier versions of bash (bash 3.00 for example)?&lt;/p&gt;</comment>
                            <comment id="12772422" author="cwsteinbach" created="Mon, 2 Nov 2009 05:08:01 +0000"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Adjusted regex to handle version numbers like &quot;0.17.2.1&quot;&lt;/li&gt;
	&lt;li&gt;On error, now prints out string returned by &apos;hadoop version&apos;.&lt;/li&gt;
	&lt;li&gt;Verified that this works on Bash 3.2.17&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12772437" author="zshao" created="Mon, 2 Nov 2009 06:50:48 +0000"  >&lt;p&gt;+1. Will commit after test passes.&lt;/p&gt;</comment>
                            <comment id="12772442" author="zshao" created="Mon, 2 Nov 2009 07:20:42 +0000"  >&lt;p&gt;Committed. Thanks Carl!&lt;/p&gt;</comment>
                            <comment id="12772639" author="zshao" created="Mon, 2 Nov 2009 20:12:32 +0000"  >&lt;p&gt;Carl, it turns out bash 3.00 is complaining:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;hive/bin/ext/cli.sh: line 19: syntax error in conditional expression: unexpected token `(&apos;
hive/bin/ext/cli.sh: line 19: syntax error near `^([&apos;
hive/bin/ext/cli.sh: line 19: `  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; [[ $version =~ ^([[:digit:]]+)\.([[:digit:]]+)\.([[:digit:]]+).*$ ]]; then&apos;
hive/bin/hive: line 173: cli: command not found
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I tried both:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; [[ $version =~ &lt;span class=&quot;code-quote&quot;&gt;&quot;^([0-9]+)\.([[:digit:]]+)\.([[:digit:]]+).*$&quot;&lt;/span&gt; ]]; then
  major_ver=${BASH_REMATCH[1]};
  minor_ver=${BASH_REMATCH[2]};
  patch_ver=${BASH_REMATCH[3]};
  echo $major_ver $minor_ver $patch_ver;
fi
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; [[ &lt;span class=&quot;code-quote&quot;&gt;&quot;$version&quot;&lt;/span&gt; =~ ^&lt;span class=&quot;code-quote&quot;&gt;&quot;([0-9]+)\.([[:digit:]]+)\.([[:digit:]]+).*&quot;&lt;/span&gt;$ ]]; then
  major_ver=${BASH_REMATCH[1]};
  minor_ver=${BASH_REMATCH[2]};
  patch_ver=${BASH_REMATCH[3]};
  echo $major_ver $minor_ver $patch_ver;
fi
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Both of these worked on bash 3.00.&lt;/p&gt;

&lt;p&gt;Can you try these out on bash 4.00? Once it works, can you post another patch on top of the first one (since the first one is already committed..&lt;/p&gt;</comment>
                            <comment id="12772641" author="zshao" created="Mon, 2 Nov 2009 20:13:43 +0000"  >&lt;p&gt;Please replace &lt;span class=&quot;error&quot;&gt;&amp;#91;0-9&amp;#93;&lt;/span&gt;+ with [&lt;span class=&quot;error&quot;&gt;&amp;#91;:digit:&amp;#93;&lt;/span&gt;]+ in the previous comment - that was for debugging.&lt;/p&gt;</comment>
                            <comment id="12772643" author="zshao" created="Mon, 2 Nov 2009 20:15:58 +0000"  >&lt;p&gt;By the way, we also want to make it compatible with internal versions like &quot;0.20&quot;. Can you make the third argument (patch var) optional in the regex?&lt;/p&gt;</comment>
                            <comment id="12772647" author="zshao" created="Mon, 2 Nov 2009 20:24:10 +0000"  >&lt;p&gt;Carl, can you help test &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-902&quot; title=&quot;cli.sh can not correctly identify Hadoop minor version numbers less than 20&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-902&quot;&gt;&lt;del&gt;HIVE-902&lt;/del&gt;&lt;/a&gt;.repair.patch on bash 4.00?&lt;/p&gt;</comment>
                            <comment id="12772693" author="cwsteinbach" created="Mon, 2 Nov 2009 21:09:39 +0000"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-902&quot; title=&quot;cli.sh can not correctly identify Hadoop minor version numbers less than 20&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-902&quot;&gt;&lt;del&gt;HIVE-902&lt;/del&gt;&lt;/a&gt;.repair.patch does not work on Bash 4.0.33&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Underlying issue is described here: &lt;a href=&quot;http://stackoverflow.com/questions/218156/bash-regex-with-quotes&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://stackoverflow.com/questions/218156/bash-regex-with-quotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Tested &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-902&quot; title=&quot;cli.sh can not correctly identify Hadoop minor version numbers less than 20&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-902&quot;&gt;&lt;del&gt;HIVE-902&lt;/del&gt;&lt;/a&gt;.repair.2.patch on Bash 4.0.33 and 3.2.17&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12772695" author="cwsteinbach" created="Mon, 2 Nov 2009 21:10:55 +0000"  >&lt;p&gt;Zheng, can you test the updated patch on Bash 3.1? I can&apos;t get 3.1 to build because of readline problems. Thanks.&lt;/p&gt;</comment>
                            <comment id="12772705" author="cwsteinbach" created="Mon, 2 Nov 2009 21:40:53 +0000"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Fix version comparison logic.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                    <attachments>
                            <attachment id="12423160" name="HIVE-902.1.patch" size="931" author="cwsteinbach" created="Mon, 26 Oct 2009 02:15:37 +0000"/>
                            <attachment id="12423792" name="HIVE-902.2.patch" size="1009" author="cwsteinbach" created="Mon, 2 Nov 2009 05:08:01 +0000"/>
                            <attachment id="12423851" name="HIVE-902.repair.2.patch" size="802" author="cwsteinbach" created="Mon, 2 Nov 2009 21:09:39 +0000"/>
                            <attachment id="12423853" name="HIVE-902.repair.3.patch" size="1089" author="cwsteinbach" created="Mon, 2 Nov 2009 21:40:53 +0000"/>
                            <attachment id="12423849" name="HIVE-902.repair.patch" size="673" author="zshao" created="Mon, 2 Nov 2009 20:24:10 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 2 Nov 2009 00:19:52 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73229</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 13 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0lccn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122651</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-902&quot; title=&quot;cli.sh can not correctly identify Hadoop minor version numbers less than 20&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-902&quot;&gt;&lt;strike&gt;HIVE-902&lt;/strike&gt;&lt;/a&gt;. Fix cli.sh to work with hadoop versions less than 20. (Carl Steinbach via zshao)</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-903] msck: output not deterministic</title>
                <link>https://issues.apache.org/jira/browse/HIVE-903</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;As a followup of &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-874&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-874&lt;/a&gt;, I was getting diffs sometime since the order of the message:&lt;/p&gt;

&lt;p&gt;part 1 added&lt;br/&gt;
part 2 added&lt;/p&gt;

&lt;p&gt;is non-deterministic.&lt;/p&gt;

&lt;p&gt;CheckResult should be enhanced to support new APIs which will return sorted lists, these new APIs should be used by DDLTask to print anything.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12439107">HIVE-903</key>
            <summary>msck: output not deterministic</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="katrakc">Cyrus Katrak</assignee>
                                    <reporter username="namit">Namit Jain</reporter>
                        <labels>
                    </labels>
                <created>Mon, 26 Oct 2009 20:43:29 +0000</created>
                <updated>Sat, 17 Dec 2011 00:06:02 +0000</updated>
                            <resolved>Tue, 27 Oct 2009 19:05:04 +0000</resolved>
                                                    <fixVersion>0.5.0</fixVersion>
                                    <component>Query Processor</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12770543" author="katrakc" created="Tue, 27 Oct 2009 15:44:01 +0000"  >&lt;p&gt;Was unable to recreate the issue, but I think this should solve it...&lt;/p&gt;</comment>
                            <comment id="12770577" author="namit" created="Tue, 27 Oct 2009 17:10:25 +0000"  >&lt;p&gt;Looks good - running the tests right now&lt;/p&gt;</comment>
                            <comment id="12770608" author="namit" created="Tue, 27 Oct 2009 19:05:04 +0000"  >&lt;p&gt;Committed. Thanks Cyrus&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12439119">HIVE-904</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12423327" name="HIVE-903.patch" size="1884" author="katrakc" created="Tue, 27 Oct 2009 15:40:46 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 27 Oct 2009 15:44:01 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73228</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 13 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0lccv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122652</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-904] unit test failure in repair.q</title>
                <link>https://issues.apache.org/jira/browse/HIVE-904</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;It seems that the order of the output partitions are not deterministic.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    [junit] Begin query: repair.q
    [junit] diff -a -I \(file:\)\|\(/tmp/.*\) -I lastUpdateTime -I lastAccessTime -I owner -I transient_lastDdlTime /data/users/zshao/tools/deploy-trunk-apache-hive/.ptest_0/build/ql/test/l\
ogs/clientpositive/repair.q.out /data/users/zshao/tools/deploy-trunk-apache-hive/.ptest_0/ql/src/test/results/clientpositive/repair.q.out
    [junit] 18c18
    [junit] &amp;lt; Partitions not in metastore:      repairtable:p1=b/p2=a   repairtable:p1=a/p2=a
    [junit] ---
    [junit] &amp;gt; Partitions not in metastore:      repairtable:p1=a/p2=a   repairtable:p1=b/p2=a
    [junit] 23,24c23
    [junit] &amp;lt; Partitions not in metastore:      repairtable:p1=b/p2=a   repairtable:p1=a/p2=a
    [junit] &amp;lt; Repair: Added partition to metastore repairtable:p1=b/p2=a
    [junit] ---
    [junit] &amp;gt; Partitions not in metastore:      repairtable:p1=a/p2=a   repairtable:p1=b/p2=a
    [junit] 25a25
    [junit] &amp;gt; Repair: Added partition to metastore repairtable:p1=b/p2=a
    [junit] Exception: Client execution results failed with error code = 1
    [junit] junit.framework.AssertionFailedError: Client execution results failed with error code = 1
    [junit]     at junit.framework.Assert.fail(Assert.java:47)
    [junit]     at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_repair(TestCliDriver.java:3442)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at junit.framework.TestCase.runTest(TestCase.java:154)
    [junit]     at junit.framework.TestCase.runBare(TestCase.java:127)
    [junit]     at junit.framework.TestResult$1.protect(TestResult.java:106)
    [junit]     at junit.framework.TestResult.runProtected(TestResult.java:124)
    [junit]     at junit.framework.TestResult.run(TestResult.java:109)
    [junit]     at junit.framework.TestCase.run(TestCase.java:118)
    [junit]     at junit.framework.TestSuite.runTest(TestSuite.java:208)
    [junit]     at junit.framework.TestSuite.run(TestSuite.java:203)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12439119">HIVE-904</key>
            <summary>unit test failure in repair.q</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="3">Duplicate</resolution>
                                        <assignee username="katrakc">Cyrus Katrak</assignee>
                                    <reporter username="zshao">Zheng Shao</reporter>
                        <labels>
                    </labels>
                <created>Mon, 26 Oct 2009 22:08:19 +0000</created>
                <updated>Tue, 27 Oct 2009 19:05:37 +0000</updated>
                            <resolved>Tue, 27 Oct 2009 16:20:20 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12770249" author="prasadc" created="Mon, 26 Oct 2009 23:11:21 +0000"  >&lt;p&gt;The problem was not in the repair command per se but the test that added has more than one non-existing partition and the msck output was not sorted.&lt;/p&gt;</comment>
                            <comment id="12770250" author="prasadc" created="Mon, 26 Oct 2009 23:12:30 +0000"  >&lt;p&gt;Just ran the repair.q test and it seems to pass fine on my system but that doesn&apos;t tell anything since this test was passing for me even before. Can someone check whether this works on where it is currently failing?&lt;/p&gt;</comment>
                            <comment id="12770552" author="prasadc" created="Tue, 27 Oct 2009 16:20:20 +0000"  >&lt;p&gt;Duplicate of &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-903&quot; title=&quot;msck: output not deterministic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-903&quot;&gt;&lt;del&gt;HIVE-903&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12770610" author="namit" created="Tue, 27 Oct 2009 19:05:37 +0000"  >&lt;p&gt;The duplicate jira got fixed&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="12439107">HIVE-903</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12423260" name="HIVE-904.patch" size="2928" author="prasadc" created="Mon, 26 Oct 2009 23:11:21 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 26 Oct 2009 23:11:21 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73227</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 13 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0lcd3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122653</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-905] smallint not working for typedbyteserde</title>
                <link>https://issues.apache.org/jira/browse/HIVE-905</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description></description>
                <environment></environment>
        <key id="12439211">HIVE-905</key>
            <summary>smallint not working for typedbyteserde</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="namit">Namit Jain</assignee>
                                    <reporter username="namit">Namit Jain</reporter>
                        <labels>
                    </labels>
                <created>Tue, 27 Oct 2009 20:10:40 +0000</created>
                <updated>Sat, 17 Dec 2011 00:06:14 +0000</updated>
                            <resolved>Tue, 27 Oct 2009 23:44:05 +0000</resolved>
                                                    <fixVersion>0.5.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12770652" author="zshao" created="Tue, 27 Oct 2009 21:31:37 +0000"  >&lt;p&gt;+1. Will commit after test.&lt;/p&gt;</comment>
                            <comment id="12770704" author="zshao" created="Tue, 27 Oct 2009 23:44:05 +0000"  >&lt;p&gt;Committed. Thanks Namit!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12423356" name="hive.905.1.patch" size="18157" author="namit" created="Tue, 27 Oct 2009 21:11:31 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 27 Oct 2009 21:31:37 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73226</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 13 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0lcdb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122654</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-905&quot; title=&quot;smallint not working for typedbyteserde&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-905&quot;&gt;&lt;strike&gt;HIVE-905&lt;/strike&gt;&lt;/a&gt;. Smallint not working for typedbyteserde. (Namit Jain via zshao)</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>
</channel>
</rss>
