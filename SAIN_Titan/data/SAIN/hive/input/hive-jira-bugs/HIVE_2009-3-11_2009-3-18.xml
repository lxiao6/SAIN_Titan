<!--
RSS generated by JIRA (7.6.3#76005-sha1:8a4e38d34af948780dbf52044e7aafb13a7cae58) at Tue Jan 22 15:13:03 UTC 2019

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<!-- If you wish to do custom client-side styling of RSS, uncomment this:
<?xml-stylesheet href="https://issues.apache.org/jira/styles/jiraxml2html.xsl" type="text/xsl"?>
-->
<rss version="0.92">
    <channel>
        <title>ASF JIRA</title>
        <link>https://issues.apache.org/jira/issues/?jql=project+%3D+HIVE+AND+created+%3E%3D+2009-3-11+AND+created+%3C%3D+2009-3-18+ORDER+BY+key+ASC</link>
        <description>An XML representation of a search request</description>
                <language>en-uk</language>
                        <issue start="0" end="16" total="16"/>
                <build-info>
            <version>7.6.3</version>
            <build-number>76005</build-number>
            <build-date>09-01-2018</build-date>
        </build-info>

<item>
            <title>[HIVE-338] Executing cli commands into thrift server</title>
                <link>https://issues.apache.org/jira/browse/HIVE-338</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Let thrift server support set, add/delete file/jar and normal HSQL query.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12416601">HIVE-338</key>
            <summary>Executing cli commands into thrift server</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21140&amp;avatarType=issuetype">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="4">Incomplete</resolution>
                                        <assignee username="coderplay">Min Zhou</assignee>
                                    <reporter username="coderplay">Min Zhou</reporter>
                        <labels>
                    </labels>
                <created>Wed, 11 Mar 2009 01:44:47 +0000</created>
                <updated>Sat, 17 Dec 2011 00:07:39 +0000</updated>
                            <resolved>Wed, 24 Jun 2009 04:08:33 +0000</resolved>
                                    <version>0.3.0</version>
                                    <fixVersion>0.4.0</fixVersion>
                                    <component>Server Infrastructure</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="12681621" author="coderplay" created="Fri, 13 Mar 2009 05:51:54 +0000"  >&lt;p&gt;support add file/jar now&lt;/p&gt;


&lt;p&gt;python example&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;#!/usr/bin/env python

&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; sys

from hive &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; ThriftHive
from hive.ttypes &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; HiveServerException
from thrift &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; Thrift
from thrift.transport &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; TSocket
from thrift.transport &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; TTransport
from thrift.protocol &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; TBinaryProtocol

&lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
    transport = TSocket.TSocket(&lt;span class=&quot;code-quote&quot;&gt;&apos;localhost&apos;&lt;/span&gt;, 10000)
    transport = TTransport.TBufferedTransport(transport)
    protocol = TBinaryProtocol.TBinaryProtocol(transport)

    client = ThriftHive.Client(protocol)
    transport.open()

    client.execute(&lt;span class=&quot;code-quote&quot;&gt;&apos;ADD FILE /home/zhoumin/py/foo&apos;&lt;/span&gt;)
    client.execute(&lt;span class=&quot;code-quote&quot;&gt;&apos;ADD FILE /home/zhoumin/py/streaming.py&apos;&lt;/span&gt;)
    query = &apos;&apos;&apos;
        INSERT OVERWRITE TABLE streaming_pokes
        MAP (pokes.foo, pokes.bar)         
            USING &lt;span class=&quot;code-quote&quot;&gt;&apos;streaming.py&apos;&lt;/span&gt;
        AS new_foo, new_bar
        FROM pokes                                &apos;&apos;&apos;

    client.execute(query)
    row = client.fetchOne()
    print row
    
    transport.close()

except Thrift.TException, tx:
    print &lt;span class=&quot;code-quote&quot;&gt;&apos;%s&apos;&lt;/span&gt; % (tx.message)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;java example:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt; zhoumin.example;

&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.util.ArrayList;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.util.concurrent.Callable;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.util.concurrent.ExecutionException;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.util.concurrent.ExecutorService;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.util.concurrent.Executors;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.util.concurrent.Future;

&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.hive.metastore.api.MetaException;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.hive.service.HiveClient;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.hadoop.hive.service.HiveServerException;

&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.facebook.thrift.TException;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.facebook.thrift.protocol.TBinaryProtocol;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.facebook.thrift.protocol.TProtocol;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.facebook.thrift.transport.TSocket;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.facebook.thrift.transport.TTransport;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.facebook.thrift.transport.TTransportException;


&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;MyClient {
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; THREADS_NUMBER = 10;
  
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;Worker &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; Callable&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; {
    
    TTransport transport;
    TProtocol protocol;
    HiveClient client;
    
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Worker() {
      transport = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TSocket(&lt;span class=&quot;code-quote&quot;&gt;&quot;localhost&quot;&lt;/span&gt;, 10000);
      protocol = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TBinaryProtocol(transport);
      client = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HiveClient(protocol); 
    }

    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; call() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception {
      transport.open();
      client.execute(&lt;span class=&quot;code-quote&quot;&gt;&quot;add jar /home/zhoumin&lt;span class=&quot;code-comment&quot;&gt;//hadoop/mapreduce/zhoumin/dist/zhoumin-0.00.1.jar&quot;&lt;/span&gt;);
&lt;/span&gt;      client.execute(&lt;span class=&quot;code-quote&quot;&gt;&quot;CREATE TEMPORARY FUNCTION strlen AS &lt;span class=&quot;code-quote&quot;&gt;&apos;hadoop.hive.udf.UdfStringLength&apos;&lt;/span&gt;&quot;&lt;/span&gt;);
      client.execute(&lt;span class=&quot;code-quote&quot;&gt;&quot;select strlen(mid) from log_data&quot;&lt;/span&gt;);
      &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; row = client.fetchOne();
      transport.close();
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; row;
    }
    
  }
  
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; void main(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;[] args) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; TTransportException,
      TException, HiveServerException, MetaException {
    ExecutorService exec = Executors.newCachedThreadPool();
    
    ArrayList&amp;lt;Future&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt;&amp;gt; results = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;Future&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt;&amp;gt;();
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0; i &amp;lt; THREADS_NUMBER; i++) {
      results.add(exec.submit(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Worker()));
    }
    
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;(Future&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; fs : results) {
      &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
        &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(fs.get());
      }&lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException e) {
        &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(e);
      } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt;(ExecutionException e) {
        &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(e);
      } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
        exec.shutdown();
      }
    }
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;the add jar command is also supported on CLI now.&lt;/p&gt;</comment>
                            <comment id="12682212" author="jsensarma" created="Mon, 16 Mar 2009 03:27:07 +0000"  >&lt;p&gt;some comments:&lt;/p&gt;

&lt;p&gt;HiveServer.java: &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;where is &apos;sp&apos; constructed?&lt;/li&gt;
	&lt;li&gt;can you encapsulate the &apos;add&apos;/&apos;delete&apos;/&apos;list&apos; commands in a new processor and call that from both CliDriver and HiveServer. Even though the logic is trivial - duplicating code sucks.&lt;/li&gt;
	&lt;li&gt;SessionState.java: addToClassPath()  - this looks like the same as the one in ExecDriver.java - can you just make the latter public static and invoke that&lt;/li&gt;
	&lt;li&gt;metadata/Hive.java: can you tell why this change was made?&lt;/li&gt;
	&lt;li&gt;exec/FunctionTask.java: is it necessary to specify the loader in the Class.forName call? I thought that that the current thread context loader was the always the first loader to be tried anyway during name resolution.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;This is missing one change in MapRedTask.java - take a look at the execute() that generates a command line that executes ExecDriver in a separate jvm (we use this mode in tests) - here we are setting -libjars option and this needs to add the ones from the jar resources as well.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;One problem is that this will not work for hadoop-17 (at least local mode) - see ExecDriver:main() - where addToClassPath is invoked on auxjars as a workaround for hadoop-17. this would need to be done for other jars added via &apos;add jar&apos; as well - except there would be no way to do this unless the list of jar file resources was also passed in as a conf variable.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Related point - we need a test for this. some dummy udf in a separate jar file that is added and then invoked from a query would be great (and would have revealed the above two issues).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;finally - &apos;delete jar&apos; doesn&apos;t seem to get rid of the jar from the classpath. perhaps this was not required at this time - but would be good to add this just for sake of completeness. The delete resource codepath is missing a callback (hook) - that would need to be added as well.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;thanks for taking this on - too many small hadoop related complexities here ..&lt;/p&gt;

</comment>
                            <comment id="12682548" author="coderplay" created="Tue, 17 Mar 2009 03:48:49 +0000"  >&lt;p&gt;oops, Some mistakes was made when migrate code from another repository. &lt;br/&gt;
I was considering running CLI as client of thrift server,  thus junior user can run sql every where,  that would not be  limited execution on the server node not opened to anybody. Thrift server would run like a multi-user interface,   cli clients could respectively sumbit commands with different privileges, they can do set , add/delete commands, even upload resource download results if  it&apos;s too huge to display .&lt;/p&gt;


&lt;ul&gt;
	&lt;li&gt;metadata/Hive.java: can you tell why this change was made?&lt;br/&gt;
pls see &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-324&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-324&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;ll upgrade this patch for solving problem you metioned later. &lt;/p&gt;</comment>
                            <comment id="12715316" author="jsensarma" created="Tue, 2 Jun 2009 00:37:34 +0000"  >&lt;p&gt;can we complete this one? or at least get the changes required to HiveServer in as part of some other Jira? (since the server is pretty broken without these thread related fixes)&lt;/p&gt;</comment>
                            <comment id="12716515" author="coderplay" created="Fri, 5 Jun 2009 05:53:54 +0000"  >&lt;ol&gt;
	&lt;li&gt;I thought we need a more efficient RPC rather than thrift. Lots of tries has been done here, and reflected it&apos;s not very suitable for a multi-user server.&lt;/li&gt;
	&lt;li&gt;SesstionState in hive must an abusage of  ThreadLocal  by treating its thread confinement property as a license to use global variables or as a means of creating &quot;hidden&quot; method arguments. Like global variables, thread-local variables can detract from reusability and introduce hidden.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="12716655" author="jsensarma" created="Fri, 5 Jun 2009 15:55:36 +0000"  >&lt;p&gt;there&apos;s a singleton sessionstate per thread that captures session settings. thread can switch from one session to another.&lt;/p&gt;

&lt;p&gt;yes - the variables in SessionState are pseudo global variables - but this is easier to organize than passing those properties to each and every method. Do you consider the System object in Java to be an abuse of global variables? Or do you consider environment variables and system properties available globally to processes as things that prevent reuse? If you look at the things that are part of sessionstate - they are similar in nature (input/output streams, list of systemwide resources and so on).&lt;/p&gt;

&lt;p&gt;this is of course open-source and re-writes are as welcome as any other contribution. i have not been a heavy user of thrift myself so cannot comment on its speed. it would be easy enough to write more rpc layers on top of hive if required. however - the current problems in the threaded server code need immediate fixing since users are trying to use it right now. probably we will have to do this as part of a separate jira.&lt;/p&gt;
</comment>
                            <comment id="12717651" author="coderplay" created="Tue, 9 Jun 2009 11:46:19 +0000"  >&lt;ul&gt;
	&lt;li&gt;exec/FunctionTask.java: is it necessary to specify the loader in the Class.forName call? I thought that that the current thread context loader was the always the first loader to be tried anyway during name resolution.&lt;br/&gt;
Yes, of course. the class loader holding by HiveConf is older than that of current thread.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;this pacth support dfs, add/delete file/jar, set now.  &lt;/p&gt;

&lt;p&gt;btw, Joydeep, would you do me a favor writing some test code that I am not familiar with?  you know, &apos; add jar&apos;  need a separate jar, and i not quite sure how to organize them.&lt;/p&gt;</comment>
                            <comment id="12720241" author="jsensarma" created="Tue, 16 Jun 2009 17:04:26 +0000"  >&lt;p&gt;thanks! - this looks pretty good. i will add a test case - and there&apos;s also an extra addtoclasspath in execdriver.main() (for hadoop-17) that needs to get added.&lt;/p&gt;

&lt;p&gt;will commit after adding/running some tests.&lt;/p&gt;</comment>
                            <comment id="12720393" author="jsensarma" created="Tue, 16 Jun 2009 22:33:04 +0000"  >&lt;p&gt;modified version:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;clean up commandprocessors to use factory and put into separate directory&lt;/li&gt;
	&lt;li&gt;added test for add jar (reusing existing jar - TestSerDe.jar)&lt;/li&gt;
	&lt;li&gt;added jars should augment -libjars in command line (required for local mode execution)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;i didn&apos;t understand the comment about the FunctionTask change. &lt;/p&gt;</comment>
                            <comment id="12720462" author="coderplay" created="Wed, 17 Jun 2009 01:47:16 +0000"  >&lt;p&gt;I think you should take a look at these lines of org.apache.hadoop.conf.Configuration&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt; classLoader;
  {
    classLoader = &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.currentThread().getContextClassLoader();
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (classLoader == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
      classLoader = Configuration.&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;getClassLoader();
    }
  }
...

  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;&amp;lt;?&amp;gt; getClassByName(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; name) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; ClassNotFoundException {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName(name, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, classLoader);
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ClassLoader of current thread changed  when adding jars into ClassPath,  conf hasnot synchronously get that change. &lt;/p&gt;</comment>
                            <comment id="12721392" author="jsensarma" created="Thu, 18 Jun 2009 18:15:12 +0000"  >&lt;p&gt;ok - this is making a little more sense to me now. aside from looking at the thread classloader - we should look at the current classloader (if the former is null). the tests probably don&apos;t catch this since the thread classloader is always set.&lt;/p&gt;</comment>
                            <comment id="12722353" author="jsensarma" created="Sun, 21 Jun 2009 12:55:14 +0000"  >&lt;p&gt;final version:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;add test for delete jar&lt;/li&gt;
	&lt;li&gt;use thread class loader preferentially in whole bunch of places (wherever configurable classes are used)&lt;/li&gt;
	&lt;li&gt;remove ^Ms&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;also forgot to mention - addtoclasspath logic was only adding new jars and not retaining existing jars in the classpath&lt;/p&gt;</comment>
                            <comment id="12722356" author="jsensarma" created="Sun, 21 Jun 2009 13:14:57 +0000"  >&lt;p&gt;committed - thanks Min!&lt;/p&gt;</comment>
                            <comment id="12722950" author="zshao" created="Tue, 23 Jun 2009 04:06:09 +0000"  >&lt;p&gt;This patch fixes the unit test failure as in:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;ant -lib testlibs test -Dtestcase=TestCliDriver -Dqfile=input16_cc.q
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It basically adds the resource jars to auxjars, so that in local mode, hive can add those jars to class path.&lt;/p&gt;</comment>
                            <comment id="12723168" author="jsensarma" created="Tue, 23 Jun 2009 16:31:29 +0000"  >&lt;p&gt;hmmm - the MapRedTask changes don&apos;t seem good to me ..&lt;/p&gt;

&lt;p&gt;the conf variable that gets passed to MapRedTask is a shared global object i think. if we set added jars into it - then delete jar command will not work.&lt;/p&gt;

&lt;p&gt;also - -libjars is the proper way of passing jars to hadoop based commands. it also sets the supplied jars into the sub-process&apos;s classpath - something that we depend on hadoop to do. so if the execdriver subprocess needs to access (by any chance) classes in the added/aux jars - then things will not work. bin/hive also uses -libjars to make sure jars are added to the process classpath.&lt;/p&gt;

&lt;p&gt;if the problem is primarily with the &lt;a href=&quot;file:/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;file:/&lt;/a&gt; uri - why do we need the mapredtask changes?&lt;/p&gt;</comment>
                            <comment id="12723314" author="zshao" created="Tue, 23 Jun 2009 21:26:32 +0000"  >&lt;p&gt;There are two problems:&lt;/p&gt;

&lt;p&gt;1. The jars added by the the &quot;add jar&quot; command only goes to -libjars but not &quot;-jobconf hive.aux.jars&quot;&lt;br/&gt;
2. For hadoop 0.17.0 local mode, ExecDriver reads &quot;hive.aux.jars&quot; and we set the classpath ourselves (instead of depending on hadoop).&lt;/p&gt;

&lt;p&gt;In order to let the hadoop 0.17.0 local mode work, I have to add the added jars to hive.aux.jars and pass it to ExecDriver through the command line jobconf.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;ExecDriver.java:526
    &lt;span class=&quot;code-comment&quot;&gt;// workaround &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; hadoop-17 - libjars are not added to classpath. &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// affects local
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// mode execution
&lt;/span&gt;    &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; localMode = HiveConf.getVar(conf, HiveConf.ConfVars.HADOOPJT)
        .equals(&lt;span class=&quot;code-quote&quot;&gt;&quot;local&quot;&lt;/span&gt;);
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (localMode) {
      &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; auxJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEAUXJARS);
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (StringUtils.isNotBlank(auxJars)) {
        &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
          Utilities.addToClassPath(StringUtils.split(auxJars, &lt;span class=&quot;code-quote&quot;&gt;&quot;,&quot;&lt;/span&gt;));
        } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Exception e) {
          &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HiveException(e.getMessage(), e);
        }
      }
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Several way to fix the global object problem:&lt;br/&gt;
A. copy-create a new conf variable&lt;br/&gt;
B. Add a new jobconf to replicate the value of -libjars (e.g. hive.hadoop.libjars)&lt;br/&gt;
C. Set the auxjars before starting the MapRed job, and reset it after the job started.&lt;/p&gt;

&lt;p&gt;I prefer approach B. Thoughts?&lt;/p&gt;</comment>
                            <comment id="12723322" author="jsensarma" created="Tue, 23 Jun 2009 21:55:12 +0000"  >&lt;p&gt;B sounds good to me as well.&lt;/p&gt;

&lt;p&gt;once we deprecate support for older versions of hadoop - we can get rid of this special case code in execdriver then.&lt;/p&gt;</comment>
                            <comment id="12723400" author="coderplay" created="Wed, 24 Jun 2009 02:20:46 +0000"  >&lt;p&gt;Can you exlain why you made a change at FunctionTask .java? It caused a java.lang.ClassNotFoundException when I executing my udf where mr jobs were submitted by hive cli. &lt;br/&gt;
ClassLoader didnot work.&lt;/p&gt;</comment>
                            <comment id="12723402" author="jsensarma" created="Wed, 24 Jun 2009 02:30:24 +0000"  >&lt;p&gt;the initial change only used the thread context classloader for looking up the class.&lt;/p&gt;

&lt;p&gt;the change i made was to (in addition) use the current (default) class loader if the thread context loader was null. this is in sync with the example you pointed out in Configuration.java (and in general seems like the right way after reading a few documents on the web). Note that it&apos;s not just FunctionTask - wherever we pick up user configured classes (inputformat being another obvious place) - we need to use this logic (and this was missing from the patch as well).&lt;/p&gt;

&lt;p&gt;it&apos;s possible there&apos;s some other issue if we are getting classnotfound. Zheng just committed the postfix patch that he has posted here earlier in the day - it might make sense to revert to an earlier version (in case u are synced to the latest trunk) prior to this commit and check if that works.&lt;/p&gt;</comment>
                            <comment id="12723419" author="jsensarma" created="Wed, 24 Jun 2009 03:47:58 +0000"  >&lt;p&gt;it turns out that the way we did the classloader (thread and then current) is not working for hadoop-20. The issue is that local job runner runs in a separate thread.  Setting the thread context classloader is useless. Based on the changes for hadoop-4612 - it seems that the way to set the classloader is to set it in the Conf object and then to always use the Conf objects classloader for locating classes. Yuck.&lt;/p&gt;

&lt;p&gt;For some reason it works (only) on hadoop-19. I have no idea how this stuff works in hadoop-19 (and why setting the thread context loader makes a difference in 17/18). &lt;/p&gt;

&lt;p&gt;it might be best if we just fixed this as part of hive-487 (so that we could address 20 related issues as well).&lt;/p&gt;</comment>
                            <comment id="12723422" author="zshao" created="Wed, 24 Jun 2009 04:08:33 +0000"  >&lt;p&gt;It seems this one is getting bigger. I will open a new jira for this.&lt;/p&gt;</comment>
                            <comment id="12723459" author="coderplay" created="Wed, 24 Jun 2009 06:00:25 +0000"  >&lt;p&gt;@Joydeep&lt;br/&gt;
even 0.19 didnot work here appling your patch.&lt;/p&gt;</comment>
                            <comment id="12723461" author="jsensarma" created="Wed, 24 Jun 2009 06:09:36 +0000"  >&lt;p&gt;i added a test for add jar (input16*.q) - that seems to work mysteriously in hadoop-19 (where of course i ran all the tests) - and nowhere else.&lt;/p&gt;

&lt;p&gt;since i don&apos;t really understand why the heck its working in 19 - let&apos;s just try this once more time via 574.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12428732">HIVE-574</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="12416074">HIVE-322</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12411492" name="HIVE-338.postfix.1.patch" size="2993" author="zshao" created="Tue, 23 Jun 2009 04:06:09 +0000"/>
                            <attachment id="12411340" name="hive-338.final.patch" size="56123" author="jsensarma" created="Sun, 21 Jun 2009 12:55:14 +0000"/>
                            <attachment id="12402116" name="hiveserver-v1.patch" size="11434" author="coderplay" created="Fri, 13 Mar 2009 05:51:54 +0000"/>
                            <attachment id="12410218" name="hiveserver-v2.patch" size="22184" author="coderplay" created="Tue, 9 Jun 2009 11:58:58 +0000"/>
                            <attachment id="12410860" name="hiveserver-v3.patch" size="43150" author="jsensarma" created="Tue, 16 Jun 2009 22:33:04 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 16 Mar 2009 03:27:07 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73603</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 31 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l99b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122150</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>- a&amp;#39;dd/delete jar&amp;#39; commands to add/remove jars from classpath used during query compilation and execution&lt;br/&gt;
- cli commands (set/add/delete/dfs) commands are now executable on hive server</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-339] [Hive] problem in count distinct in 1mapreduce job with map side aggregation</title>
                <link>https://issues.apache.org/jira/browse/HIVE-339</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description></description>
                <environment></environment>
        <key id="12416609">HIVE-339</key>
            <summary>[Hive] problem in count distinct in 1mapreduce job with map side aggregation</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="namit">Namit Jain</assignee>
                                    <reporter username="namit">Namit Jain</reporter>
                        <labels>
                    </labels>
                <created>Wed, 11 Mar 2009 05:50:38 +0000</created>
                <updated>Sat, 17 Dec 2011 00:08:39 +0000</updated>
                            <resolved>Thu, 12 Mar 2009 20:37:38 +0000</resolved>
                                                    <fixVersion>0.3.0</fixVersion>
                                    <component>Query Processor</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12680765" author="namit" created="Wed, 11 Mar 2009 06:25:05 +0000"  >&lt;p&gt;fixed some testparse logs also&lt;/p&gt;</comment>
                            <comment id="12680779" author="rsm" created="Wed, 11 Mar 2009 07:41:09 +0000"  >&lt;p&gt;It&apos;s not clear what adding a new group by mode would do. There are no changes to GroupByOperator which seem to use this mode. Is that correct? If so, can you add a comment what the mergepartial mode is supposed to indicate?&lt;/p&gt;</comment>
                            <comment id="12680797" author="jsensarma" created="Wed, 11 Mar 2009 08:24:39 +0000"  >&lt;p&gt;man - this code is unparseable at this point. seems like 99% of the code in the old and new function (gengroupbyplanxxx) is common. is any refactoring and better documentation possible here at all? &lt;/p&gt;

&lt;p&gt;i also don&apos;t understand the new groupbydesc mode. it seems the effect is to choose &apos;iterate&apos; method in the group by operator for the distinct case (is that correct?). why is this necessary?&lt;/p&gt;</comment>
                            <comment id="12680892" author="namit" created="Wed, 11 Mar 2009 15:42:31 +0000"  >&lt;p&gt;will add comments and load a new patch&lt;/p&gt;</comment>
                            <comment id="12680928" author="namit" created="Wed, 11 Mar 2009 17:07:50 +0000"  >&lt;p&gt;added a lot more comments.&lt;/p&gt;

&lt;p&gt;Need iterate at reduce-side because it is possible that the map-aide aggregation was turned off at runtime &lt;br/&gt;
and then distinct need to be eliminated&lt;/p&gt;</comment>
                            <comment id="12680958" author="jsensarma" created="Wed, 11 Mar 2009 18:08:31 +0000"  >&lt;p&gt;not able to understand. regardless of whether map-side aggregation is turned off dynamically - the aggregation output type of the mapside will be a partial aggregate. i don&apos;t understand how we can iterate over the results of partial aggregate.&lt;/p&gt;

&lt;p&gt;i think problems may be getting masked because the output type of count partial aggregate is the same as count full aggregate. if we were doing &apos;avg(distinct column)&apos; - then doing iterate over partial aggregate output &lt;em&gt;should&lt;/em&gt; cause problems. this would be a good test case as well.&lt;/p&gt;</comment>
                            <comment id="12680984" author="namit" created="Wed, 11 Mar 2009 18:59:48 +0000"  >&lt;p&gt;Say the query is: select col2, count(distinct col2) from T group by col1;&lt;/p&gt;

&lt;p&gt;the mapper emits:&lt;/p&gt;

&lt;p&gt;col1   col2  col3&lt;br/&gt;
Key    19   1&lt;br/&gt;
Key    19   1&lt;br/&gt;
Key    22   1&lt;br/&gt;
Key    22   1&lt;br/&gt;
Key    23   1&lt;/p&gt;

&lt;p&gt;The expected answer is:&lt;/p&gt;

&lt;p&gt;Key  3&lt;/p&gt;

&lt;p&gt;So, you walk over the key+sort (which is col1 + col2), which makes sure that in the above example, 2nd and 4th rows are ignored,&lt;br/&gt;
and then iterate is called 3 times, which result in (key 3)&lt;/p&gt;

&lt;p&gt;However,if you called merge in the reducer: it would result in:&lt;/p&gt;

&lt;p&gt;key (19+22+23) --&amp;gt; key 64&lt;/p&gt;


&lt;p&gt;The same should work for avg also. I am running a query on mobile_mms for february currently to validate.&lt;br/&gt;
Partial results do not mean anything for distincts whereas they can be aggregated for non-distincts.&lt;/p&gt;




</comment>
                            <comment id="12681002" author="zshao" created="Wed, 11 Mar 2009 19:26:27 +0000"  >&lt;p&gt;Agree.&lt;/p&gt;

&lt;p&gt;In short, for DINSTICT aggregations in 1-map/reduce plan, we do &quot;de-duplicate&quot; as much as we can in the map-phase, and the whole aggregation is done in the reduce phase.&lt;br/&gt;
That&apos;s why reduce phase also needs iterate().&lt;/p&gt;</comment>
                            <comment id="12681015" author="rsm" created="Wed, 11 Mar 2009 20:16:57 +0000"  >&lt;p&gt;Can you rename Reduction key to &apos;Partitioning Key&apos; and &apos;Partitioning Key&apos; to &apos;Sorting Key&apos;. I&apos;m guessing that&apos;s what you mean.&lt;/p&gt;</comment>
                            <comment id="12681020" author="namit" created="Wed, 11 Mar 2009 20:46:09 +0000"  >&lt;p&gt;incorporated Raghu&apos;s comments&lt;/p&gt;</comment>
                            <comment id="12681030" author="jsensarma" created="Wed, 11 Mar 2009 21:13:09 +0000"  >&lt;p&gt;the explanations and comments make this much easier to understand. am just thumbing through the code one last time.&lt;/p&gt;
</comment>
                            <comment id="12681060" author="zshao" created="Wed, 11 Mar 2009 22:02:59 +0000"  >&lt;p&gt;hive.339.4.patch: 1073:&lt;br/&gt;
+   *   Partitioning Key: &lt;br/&gt;
+   *      grouping key if no DISTINCT&lt;br/&gt;
+   *      grouping + distinct key if DISTINCT&lt;/p&gt;

&lt;p&gt;should be:&lt;br/&gt;
+   *   Partitioning Key: &lt;br/&gt;
+   *      random() if no DISTINCT&lt;br/&gt;
+   *      grouping + distinct key if DISTINCT&lt;/p&gt;

&lt;p&gt;The same is true for line 1158.&lt;/p&gt;


&lt;p&gt;Line 1017: Can you add the comment for MERGEPARTIAL?&lt;/p&gt;</comment>
                            <comment id="12681069" author="namit" created="Wed, 11 Mar 2009 22:35:52 +0000"  >&lt;p&gt;incorporated Zheng&apos;s comments&lt;/p&gt;</comment>
                            <comment id="12681454" author="zshao" created="Thu, 12 Mar 2009 19:21:25 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="12681481" author="zshao" created="Thu, 12 Mar 2009 20:37:37 +0000"  >&lt;p&gt;Committed revision 752996. Thanks Namit!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12401897" name="hive.339.1.patch" size="36775" author="namit" created="Wed, 11 Mar 2009 05:55:30 +0000"/>
                            <attachment id="12401899" name="hive.339.2.patch" size="41493" author="namit" created="Wed, 11 Mar 2009 06:24:45 +0000"/>
                            <attachment id="12401943" name="hive.339.3.patch" size="46788" author="namit" created="Wed, 11 Mar 2009 17:06:12 +0000"/>
                            <attachment id="12401965" name="hive.339.4.patch" size="46777" author="namit" created="Wed, 11 Mar 2009 20:45:51 +0000"/>
                            <attachment id="12401980" name="hive.339.5.patch" size="47107" author="namit" created="Wed, 11 Mar 2009 22:35:29 +0000"/>
                            <attachment id="12401997" name="hive.339.6.patch" size="33010" author="namit" created="Thu, 12 Mar 2009 00:33:19 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 11 Mar 2009 07:41:09 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73602</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 46 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l99j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122151</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-339&quot; title=&quot;[Hive] problem in count distinct in 1mapreduce job with map side aggregation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-339&quot;&gt;&lt;strike&gt;HIVE-339&lt;/strike&gt;&lt;/a&gt;: Fix count distinct in 1 map-reduce job with map side aggregation. (Namit Jain via zshao)</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-340] [hive] null pointer exception with nulls in map-side aggregation</title>
                <link>https://issues.apache.org/jira/browse/HIVE-340</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description></description>
                <environment></environment>
        <key id="12416612">HIVE-340</key>
            <summary>[hive] null pointer exception with nulls in map-side aggregation</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="namit">Namit Jain</assignee>
                                    <reporter username="namit">Namit Jain</reporter>
                        <labels>
                    </labels>
                <created>Wed, 11 Mar 2009 06:25:36 +0000</created>
                <updated>Sat, 17 Dec 2011 00:08:28 +0000</updated>
                            <resolved>Wed, 11 Mar 2009 21:31:04 +0000</resolved>
                                                    <fixVersion>0.3.0</fixVersion>
                                    <component>Query Processor</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12680767" author="prasadc" created="Wed, 11 Mar 2009 06:41:56 +0000"  >&lt;p&gt;null check seems ok. don&apos;t see any other problem since it is used only in sampling.&lt;br/&gt;
+1&lt;/p&gt;</comment>
                            <comment id="12681037" author="zshao" created="Wed, 11 Mar 2009 21:31:04 +0000"  >&lt;p&gt;Committed revision 752624. Thanks Namit!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12401900" name="hive.340.1.patch" size="717" author="namit" created="Wed, 11 Mar 2009 06:33:16 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 11 Mar 2009 06:41:56 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73601</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 46 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l99r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122152</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-340&quot; title=&quot;[hive] null pointer exception with nulls in map-side aggregation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-340&quot;&gt;&lt;strike&gt;HIVE-340&lt;/strike&gt;&lt;/a&gt;. Fixed null pointer exception with nulls in map-side aggregation. (Namit Jain via zshao)</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-341] Specifying partition column without table alias causes unknown exception</title>
                <link>https://issues.apache.org/jira/browse/HIVE-341</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Created two tables - tmp_rsm_abc and tmp_rsm_abc1. The latter is partitioned on ds. Query on first table succeeds, but query on second fails. See the session below.&lt;/p&gt;

&lt;p&gt;hive&amp;gt; describe tmp_rsm_abc;                                                                                                         &lt;br/&gt;
a	string&lt;br/&gt;
b	int&lt;br/&gt;
Time taken: 0.116 seconds&lt;br/&gt;
hive&amp;gt; select a, b from tmp_rsm_abc where b &amp;gt; 5;  &amp;lt;------------------------------------- this query succeeds&lt;br/&gt;
Unknown	19&lt;br/&gt;
Unknown	29&lt;br/&gt;
Unknown	29&lt;br/&gt;
Unknown	29&lt;br/&gt;
Unknown	30&lt;br/&gt;
Unknown	25&lt;br/&gt;
Unknown	15&lt;br/&gt;
Unknown	17&lt;br/&gt;
Unknown	28&lt;br/&gt;
Unknown	17&lt;br/&gt;
Time taken: 8.198 seconds&lt;br/&gt;
hive&amp;gt; create table tmp_rsm_abc1(a string, b int) partitioned by (ds string);&lt;br/&gt;
OK&lt;br/&gt;
Time taken: 0.118 seconds&lt;br/&gt;
hive&amp;gt; insert overwrite table tmp_rsm_abc1 partition (ds = &apos;10&apos;) select a, b from tmp_rsm_abc where b &amp;gt; 5;&lt;br/&gt;
10 Rows loaded to tmp_rsm_abc1&lt;br/&gt;
OK&lt;br/&gt;
Time taken: 9.319 seconds&lt;br/&gt;
hive&amp;gt; select a, b from tmp_rsm_abc1 where ds = &apos;10&apos;; &amp;lt;--------------------------------------------- this query fails&lt;br/&gt;
FAILED: Unknown exception : null&lt;br/&gt;
Time taken: 0.053 seconds&lt;br/&gt;
hive&amp;gt; &lt;/p&gt;</description>
                <environment></environment>
        <key id="12416616">HIVE-341</key>
            <summary>Specifying partition column without table alias causes unknown exception</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="rsm">Raghotham Murthy</reporter>
                        <labels>
                    </labels>
                <created>Wed, 11 Mar 2009 08:23:07 +0000</created>
                <updated>Sat, 17 Dec 2011 00:03:45 +0000</updated>
                            <resolved>Fri, 11 Sep 2009 22:18:45 +0000</resolved>
                                    <version>0.6.0</version>
                                    <fixVersion>0.6.0</fixVersion>
                                    <component>Query Processor</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12680800" author="prasadc" created="Wed, 11 Mar 2009 08:29:38 +0000"  >&lt;p&gt;it is the partition pruning code that is bailing out here. if we fix that code few other JIRAs also will be closed.&lt;/p&gt;</comment>
                            <comment id="12754382" author="prasadc" created="Fri, 11 Sep 2009 22:18:45 +0000"  >&lt;p&gt;fixed with the new partition pruner code.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 11 Mar 2009 08:29:38 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73600</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 20 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l99z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122153</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-342] TestMTQueries is broken</title>
                <link>https://issues.apache.org/jira/browse/HIVE-342</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;It has been broken for quite sometime but the build is not failing.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12416662">HIVE-342</key>
            <summary>TestMTQueries is broken</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="prasadc">Prasad Chakka</assignee>
                                    <reporter username="prasadc">Prasad Chakka</reporter>
                        <labels>
                    </labels>
                <created>Wed, 11 Mar 2009 17:37:25 +0000</created>
                <updated>Sat, 17 Dec 2011 00:09:05 +0000</updated>
                            <resolved>Wed, 25 Mar 2009 21:14:48 +0000</resolved>
                                    <version>0.6.0</version>
                                    <fixVersion>0.3.0</fixVersion>
                                    <component>Build Infrastructure</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12680941" author="prasadc" created="Wed, 11 Mar 2009 17:42:14 +0000"  >&lt;p&gt;need to fix this otherwise bugs might be introduced&lt;/p&gt;</comment>
                            <comment id="12680954" author="prasadc" created="Wed, 11 Mar 2009 17:59:00 +0000"  >&lt;p&gt;&lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hive-trunk-h0.19/lastBuild/testReport/org.apache.hadoop.hive.ql/TestMTQueries/testMTQueries1/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hive-trunk-h0.19/lastBuild/testReport/org.apache.hadoop.hive.ql/TestMTQueries/testMTQueries1/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12681567" author="athusoo" created="Fri, 13 Mar 2009 01:07:04 +0000"  >&lt;p&gt;Assigning to Prasad.&lt;/p&gt;</comment>
                            <comment id="12681834" author="prasadc" created="Fri, 13 Mar 2009 17:29:58 +0000"  >&lt;p&gt;Session is being initialized in wrong thread thus SessionState.get() in the test thread gets spawned.&lt;/p&gt;</comment>
                            <comment id="12681879" author="athusoo" created="Fri, 13 Mar 2009 20:00:15 +0000"  >&lt;p&gt;Running tests on this and will commit once they succeed.&lt;/p&gt;</comment>
                            <comment id="12681881" author="athusoo" created="Fri, 13 Mar 2009 20:08:34 +0000"  >&lt;p&gt;Sorry, did not realize that you can commit this yourself now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Anyway&lt;/p&gt;

&lt;p&gt;+1 to the code.&lt;/p&gt;</comment>
                            <comment id="12681917" author="athusoo" created="Fri, 13 Mar 2009 22:22:43 +0000"  >&lt;p&gt;Getting follow errors in test...  &lt;/p&gt;

&lt;p&gt;  &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; Hive history file=/data/users/athusoo/commits/hive_trunk_ws7/ql/../build/ql/tmp/hive_job_log_athusoo_200903131519_1514028075.txt&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; Exception: 0&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; java.lang.ArrayIndexOutOfBoundsException: 0&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input_dfs(TestCliDriver.java:67)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at junit.framework.TestCase.runTest(TestCase.java:154)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at junit.framework.TestCase.runBare(TestCase.java:127)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at junit.framework.TestResult$1.protect(TestResult.java:106)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at junit.framework.TestResult.runProtected(TestResult.java:124)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at junit.framework.TestResult.run(TestResult.java:109)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at junit.framework.TestCase.run(TestCase.java:118)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at junit.framework.TestSuite.runTest(TestSuite.java:208)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at junit.framework.TestSuite.run(TestSuite.java:203)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt;     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 8.928 sec&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; Test org.apache.hadoop.hive.cli.TestCliDriver FAILED&lt;/p&gt;</comment>
                            <comment id="12688528" author="prasadc" created="Tue, 24 Mar 2009 01:48:01 +0000"  >&lt;p&gt;once 349 is committed, the latest patch for this will work without any test failures.&lt;/p&gt;</comment>
                            <comment id="12689242" author="namit" created="Wed, 25 Mar 2009 21:14:48 +0000"  >&lt;p&gt;committed. Thanks Prasad&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12416882">HIVE-349</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12402154" name="hive-342.patch" size="898" author="prasadc" created="Fri, 13 Mar 2009 17:29:58 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 13 Mar 2009 01:07:04 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73599</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 44 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l9a7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122154</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-343] Include py packages for thrift service </title>
                <link>https://issues.apache.org/jira/browse/HIVE-343</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description></description>
                <environment></environment>
        <key id="12416675">HIVE-343</key>
            <summary>Include py packages for thrift service </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rsm">Raghotham Murthy</assignee>
                                    <reporter username="rsm">Raghotham Murthy</reporter>
                        <labels>
                    </labels>
                <created>Wed, 11 Mar 2009 19:05:09 +0000</created>
                <updated>Sat, 17 Dec 2011 00:08:58 +0000</updated>
                            <resolved>Thu, 26 Mar 2009 21:12:54 +0000</resolved>
                                    <version>0.6.0</version>
                                    <fixVersion>0.3.0</fixVersion>
                                    <component>Server Infrastructure</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12689665" author="zshao" created="Thu, 26 Mar 2009 21:12:54 +0000"  >&lt;p&gt;Committed revision 758857. Thanks Raghu!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12401958" name="hive-py.patch" size="178975" author="rsm" created="Wed, 11 Mar 2009 19:07:12 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 26 Mar 2009 21:12:54 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73598</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 44 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l9af:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122155</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-343&quot; title=&quot;Include py packages for thrift service &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-343&quot;&gt;&lt;strike&gt;HIVE-343&lt;/strike&gt;&lt;/a&gt;. Include py packages for thrift service. (Raghotham Murthy via zshao)</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-344] Fix the Hive CLI to run on cygwin under windows</title>
                <link>https://issues.apache.org/jira/browse/HIVE-344</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Yes, I am guilty as charged, I don&apos;t use mac like the rest of you &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;The Hive CLI currently doesn&apos;t work with cygwin under windows, this is due to the dual path model that cygwin employs (there are file paths relative to the cygwin virtual root, and file paths relative to the windows root)&lt;/p&gt;

&lt;p&gt;Since Sun&apos;s JDK is installed under the windows environment, if the paths are not converted to windows format before being passed along then java will be at a loss to where the files are. The solution is to use the cygpath command to convert the paths to windows format before passing along to java world.&lt;/p&gt;

&lt;p&gt;I have a fix for this already, still doing some further testing (to make sure it works under both unix and windows environments), then I will submit patch to this bug (should submit before end of this week).&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;&amp;#8211; amr&lt;/p&gt;</description>
                <environment>&lt;p&gt;windows xp&lt;/p&gt;</environment>
        <key id="12416704">HIVE-344</key>
            <summary>Fix the Hive CLI to run on cygwin under windows</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="aaa">Amr Awadallah</assignee>
                                    <reporter username="aaa">Amr Awadallah</reporter>
                        <labels>
                    </labels>
                <created>Thu, 12 Mar 2009 00:45:17 +0000</created>
                <updated>Sat, 17 Dec 2011 00:07:29 +0000</updated>
                            <resolved>Fri, 7 Aug 2009 20:36:45 +0000</resolved>
                                    <version>0.3.0</version>
                                    <fixVersion>0.4.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12706803" author="aaa" created="Thu, 7 May 2009 11:11:20 +0000"  >&lt;p&gt;&lt;br/&gt;
I upgraded the fix to work with Hive 0.3. Would be great if a couple of folks can try this under cygwin and let me know if it works fine. It shouldn&apos;t affect non-windows installations, and for windows installations it will change the relevant paths/dirs using  cygpath (I added the fix to all services: cli.sh, hwi.sh, hiveserver.sh, and lineage.sh)&lt;/p&gt;</comment>
                            <comment id="12707043" author="athusoo" created="Thu, 7 May 2009 19:05:33 +0000"  >&lt;p&gt;Thanks for the patch Amr. Like you I am a PC user as well. Will try it out.&lt;/p&gt;

&lt;p&gt;Sometime back I do remember seeing some email on the hadoop mailing list that they are going to deprecate support on cygwin. Don&apos;t know if that is true or not...&lt;/p&gt;

&lt;p&gt;Also you should try coLinux (it is a very lightweight linux vm that runs on windows). I have been able to do all my hive development on the FC7 image with that.&lt;/p&gt;
</comment>
                            <comment id="12708929" author="johanoskarsson" created="Wed, 13 May 2009 13:58:17 +0000"  >&lt;p&gt;Moving to 0.3.1 as 0.3.0 is released&lt;/p&gt;</comment>
                            <comment id="12709930" author="athusoo" created="Fri, 15 May 2009 18:42:05 +0000"  >&lt;p&gt;I tried this but it does not seem to work with auxilary jars. I get the following error&lt;/p&gt;

&lt;p&gt;bash-3.2$ hive/bin/hive&lt;br/&gt;
java.net.URISyntaxException: Illegal character in scheme name at index 4: file;c:/Users/Ashish/dev/hive/hive/auxlib/fbhive_hooks.jar&lt;/p&gt;

&lt;p&gt;AUX_PARAM has URIs in it so cygpath will probably cause problems there. The cygpath code for AUX_PARAMS needs to go at all the places where aux params are getting created from file names as opposed to at the end. I think that should fix this issue.&lt;/p&gt;
</comment>
                            <comment id="12713395" author="aaa" created="Wed, 27 May 2009 03:54:14 +0000"  >&lt;p&gt;@Ashish, sorry for delay replying and thanks for testing. I thought I tracked down all of the path params, I will take a closer look at AUX_PARAMS next week and submit another patch.&lt;/p&gt;

&lt;p&gt; &amp;#8211; amr&lt;/p&gt;</comment>
                            <comment id="12733174" author="johanoskarsson" created="Mon, 20 Jul 2009 09:56:00 +0000"  >&lt;p&gt;Since there&apos;s been no activity on this ticket for a while I suggest we move it to 0.3.2 so that 0.3.1 can be released.&lt;/p&gt;</comment>
                            <comment id="12734533" author="aaa" created="Thu, 23 Jul 2009 09:44:52 +0000"  >&lt;p&gt;@Ashish&lt;/p&gt;

&lt;p&gt;Can&apos;t believe it has been two months already, but please find a new patch attached (&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-344&quot; title=&quot;Fix the Hive CLI to run on cygwin under windows&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-344&quot;&gt;&lt;del&gt;HIVE-344&lt;/del&gt;&lt;/a&gt;-v2.patch). &lt;/p&gt;

&lt;p&gt;I think I tracked down all places where AUX_PARAM and AUX_CLASSPATH get set. &lt;/p&gt;

&lt;p&gt;Please give a try and let me know.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;&amp;#8211; amr&lt;/p&gt;</comment>
                            <comment id="12735725" author="athusoo" created="Mon, 27 Jul 2009 19:19:57 +0000"  >&lt;p&gt;Thanks Amr. Will try it out today.&lt;/p&gt;</comment>
                            <comment id="12738777" author="aaa" created="Tue, 4 Aug 2009 02:57:46 +0000"  >&lt;p&gt;ashish, worked?&lt;/p&gt;</comment>
                            <comment id="12740106" author="athusoo" created="Thu, 6 Aug 2009 15:25:46 +0000"  >&lt;p&gt;The first tests seem to be ok. Will do a bit more today and checkin if it works. I have one question though about the patch. The cygpath thingy is done in the cli.sh and lineage.sh files after HIVE_LIB is used. Should it not be before HIVE_LIB is used?&lt;/p&gt;</comment>
                            <comment id="12740479" author="aaa" created="Fri, 7 Aug 2009 09:32:56 +0000"  >&lt;p&gt;unfortunately HIVE_LIB is checked for validity inside these files using normal bash commands, so the cygpath translation for HIVE_LIB has to be done this late as opposed to in the main hive script (otherwise the verification in sub-scripts would fail).&lt;/p&gt;</comment>
                            <comment id="12740687" author="athusoo" created="Fri, 7 Aug 2009 19:41:06 +0000"  >&lt;p&gt;I am having problems with show tables.&lt;/p&gt;

&lt;p&gt;hive&amp;gt; show tables;&lt;br/&gt;
FAILED: Error in metadata: javax.jdo.JDODataStoreException: Exception thrown performing schema operation : Add classes to Catalog &quot;&quot;, Schema &quot;APP&quot;&lt;br/&gt;
NestedThrowables:&lt;br/&gt;
java.sql.SQLNonTransientConnectionException: No current connection.&lt;br/&gt;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask&lt;/p&gt;

&lt;p&gt;Prasad can you comment on this?&lt;/p&gt;</comment>
                            <comment id="12740704" author="athusoo" created="Fri, 7 Aug 2009 20:17:49 +0000"  >&lt;p&gt;Got it working. Looks all good to me.&lt;/p&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;Will  checkin once the unit tests pass.&lt;/p&gt;</comment>
                            <comment id="12740716" author="athusoo" created="Fri, 7 Aug 2009 20:36:45 +0000"  >&lt;p&gt;committed. Thanks Amr!!&lt;/p&gt;</comment>
                            <comment id="12740730" author="prasadc" created="Fri, 7 Aug 2009 20:57:10 +0000"  >&lt;p&gt;what was the problem?&lt;/p&gt;</comment>
                            <comment id="12858863" author="rymm" created="Tue, 20 Apr 2010 13:03:50 +0000"  >&lt;p&gt;Hi&lt;br/&gt;
I&apos;ve installed hadoop-0.19.2 and hive under cygwin&lt;br/&gt;
am trying to follow the tutorial &lt;a href=&quot;http://wiki.apache.org/hadoop/Hive/GettingStarted#Downloading_and_building&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/hadoop/Hive/GettingStarted#Downloading_and_building&lt;/a&gt;&lt;br/&gt;
everything is working fine in the tutorial until I tried to execute an sql stmt &lt;br/&gt;
after performing following settings&lt;br/&gt;
hive&amp;gt;SET mapred.job.tracker=myhost.mycompany.com:50030;&lt;br/&gt;
hive&amp;gt;SET -v ;&lt;br/&gt;
then I get &quot;Number of reduce tasks is set to 0 since there&apos;s no reduce operator&quot;&lt;br/&gt;
first does the patch apply to this error&lt;br/&gt;
am a new to linux, how to apply the patch&lt;br/&gt;
isn&apos;t worth to work with co-linux?&lt;br/&gt;
thankx&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12414326" name="HIVE-344-v2.patch" size="3219" author="aaa" created="Thu, 23 Jul 2009 09:44:52 +0000"/>
                            <attachment id="12407452" name="HIVE-344.patch" size="2360" author="aaa" created="Thu, 7 May 2009 11:12:14 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 7 May 2009 19:05:33 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73597</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 41 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l9an:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122156</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>* Fixes for running hive under cygwin</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-345] Extend Date UDFs to support time zone and full specs as in MySQL</title>
                <link>https://issues.apache.org/jira/browse/HIVE-345</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Most of the Date UDF in Hive now are based on String instead of Date objects, and they have limited functionality compared with MySQL.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_from-unixtime&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_from-unixtime&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_date-add&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_date-add&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_date-sub&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_date-sub&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_datediff&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_datediff&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We should make it fully compliant with what MySQL offers.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12416719">HIVE-345</key>
            <summary>Extend Date UDFs to support time zone and full specs as in MySQL</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21140&amp;avatarType=issuetype">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="zshao">Zheng Shao</reporter>
                        <labels>
                    </labels>
                <created>Thu, 12 Mar 2009 07:04:08 +0000</created>
                <updated>Sat, 8 Apr 2017 19:51:00 +0000</updated>
                                            <version>0.3.0</version>
                                                    <component>UDF</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>10</watches>
                                                                <comments>
                            <comment id="13471395" author="mgrover" created="Mon, 8 Oct 2012 04:23:24 +0000"  >&lt;p&gt;Presently Hive doesn&apos;t seem to provide functionality to specify the timezone when performing date/time conversions. The pre-set timezone is used by default. I would like to propose addition of such functionality (by means of this JIRA) that allows users to optionally specify a timezone when performing date/time conversions.&lt;/p&gt;

&lt;p&gt;There are 2 ways I can think of implementing this:&lt;br/&gt;
1) Add an optional parameter to functions like from_unixtime and unix_timestamp to specify the timezone to be used during conversion.&lt;br/&gt;
2) Create a new UDF, convert_tz with a prototype: CONVERT_TZ(dt,src_tz,dest_tz) that will perform the timezone conversion. In this case, folks would have to convert the argument&apos;s timezone before passing it in to unix_timestamp() or convert the result&apos;s timezone when using from_unixtime().&lt;/p&gt;

&lt;p&gt;Advantage of the first approach is that it provides a direct way of performing date/time conversions with a particular timezone. Disadvantage is that there is a risk of confusing users. For example, from_unixtime takes a 2nd optional parameter that be used to specify the format. If we go with approach one and make timezone third parameter, we force people to specify a format (which is a string just like timezone) if they wish you to use the timezone parameter.&lt;/p&gt;

&lt;p&gt;Advantage of the second approach is that it&apos;s a more generic method of converting timezones and can potentially be used with other date/time related UDFs. It&apos;s also similar to MySQL does (MySQL doesn&apos;t have parameters in from_unixtime and unix_timestamp to specify timezone). Disadvantage is that people using from_unixtime and unix_timestamp would have to make a nested UDF call e.g. convert_tz(from_unixtime()) or unix_timestamp(convert_tz()) in order to accomplish what they want.&lt;/p&gt;

&lt;p&gt;Based on the above analysis, I am inclined towards the second approach (i.e. creating a convert_tz UDF). What do you folks think? Anyone out there that prefers approach 1 or a completely different approach?&lt;/p&gt;</comment>
                            <comment id="15647129" author="nmahadevuni" created="Tue, 8 Nov 2016 10:06:10 +0000"  >&lt;p&gt;Hi. I&apos;m starting as a contributor. I would like to work on this. Is anyone else working on similar changes? &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-6009&quot; title=&quot;Add from_unixtime UDF that has controllable Timezone&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-6009&quot;&gt;HIVE-6009&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-2867&quot; title=&quot;Timestamp is defined to be timezoneless but timestamps appear to be processed in the current timezoneql&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-2867&quot;&gt;HIVE-2867&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-6040&quot; title=&quot;to_utc_timestamp() not intuitive when cluster timezone is not set to UTC&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-6040&quot;&gt;HIVE-6040&lt;/a&gt; are also related to this. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12459949">HIVE-1269</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12415739">HIVE-313</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 8 Oct 2012 04:23:24 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>42858</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 11 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i06247:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>33274</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>


<item>
            <title>[HIVE-346] Support ENUM types</title>
                <link>https://issues.apache.org/jira/browse/HIVE-346</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;ENUM types would be useful for reducing data size as well as for mapping constants to meaningful names.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12416865">HIVE-346</key>
            <summary>Support ENUM types</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21140&amp;avatarType=issuetype">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="indigoviolet">Venky Iyer</reporter>
                        <labels>
                    </labels>
                <created>Fri, 13 Mar 2009 19:52:22 +0000</created>
                <updated>Fri, 13 Mar 2009 19:52:22 +0000</updated>
                                                                                <due></due>
                            <votes>1</votes>
                                    <watches>3</watches>
                                                                        <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>42857</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 46 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i08ofj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>48557</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>


<item>
            <title>[HIVE-347] [hive] lot of mappers due to a user error while specifying the partitioning column</title>
                <link>https://issues.apache.org/jira/browse/HIVE-347</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;A common scenario when the table is partitioned on &apos;ds&apos; column which is of type &apos;string&apos; of a certain format &apos;yyyy-mm-dd&apos;&lt;/p&gt;

&lt;p&gt;However, if the user forgets to add quotes while specifying the query:&lt;/p&gt;

&lt;p&gt;select ... from T where ds = 2009-02-02&lt;/p&gt;


&lt;p&gt;2009-02-02 is a valid integer expression. So, partition pruning makes all partitions unknown, since 2009-02-02 to double conversion is null.&lt;/p&gt;

&lt;p&gt;If all partitions are unknown, in strict mode, we should thrown an error&lt;/p&gt;

</description>
                <environment></environment>
        <key id="12416875">HIVE-347</key>
            <summary>[hive] lot of mappers due to a user error while specifying the partitioning column</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="namit">Namit Jain</assignee>
                                    <reporter username="namit">Namit Jain</reporter>
                        <labels>
                    </labels>
                <created>Fri, 13 Mar 2009 22:38:36 +0000</created>
                <updated>Sat, 17 Dec 2011 00:08:57 +0000</updated>
                            <resolved>Thu, 19 Mar 2009 21:48:20 +0000</resolved>
                                                    <fixVersion>0.3.0</fixVersion>
                                    <component>Query Processor</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12681978" author="prasadc" created="Sat, 14 Mar 2009 03:14:06 +0000"  >&lt;p&gt;The changes look good. I think the Error Message will confuse users since they see a partition predicate. I think error message should say something about incorrect type or quotes.&lt;/p&gt;</comment>
                            <comment id="12682524" author="rsm" created="Mon, 16 Mar 2009 23:59:52 +0000"  >&lt;p&gt;+1 looks good.&lt;/p&gt;

&lt;p&gt;Can you change the comment to remove mention of &apos;date string&apos; since that&apos;s not always the case?&lt;/p&gt;</comment>
                            <comment id="12682528" author="namit" created="Tue, 17 Mar 2009 00:16:28 +0000"  >&lt;p&gt;done&lt;/p&gt;</comment>
                            <comment id="12682808" author="namit" created="Tue, 17 Mar 2009 21:15:10 +0000"  >&lt;p&gt;committed.&lt;/p&gt;</comment>
                            <comment id="12683570" author="namit" created="Thu, 19 Mar 2009 19:04:59 +0000"  >&lt;p&gt;There was a problem with the above patch. TRUE and unknown is unknown, therefore the patch was breaking for the following case:&lt;/p&gt;

&lt;p&gt;select .. from T where partCol = value and col = 1;&lt;/p&gt;


&lt;p&gt;col = 1 will be null even for &apos;value&apos; partition, therefore &apos;value&apos; partition will also be unknown.&lt;/p&gt;


&lt;p&gt;The basic problem is that there is a implicit int to string conversion I dont think this can/should be fixed.&lt;br/&gt;
will upload a new patch undoing the patch&lt;/p&gt;</comment>
                            <comment id="12683604" author="athusoo" created="Thu, 19 Mar 2009 20:48:06 +0000"  >&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;looks good to me.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12402174" name="hive.347.1.patch" size="2814" author="namit" created="Fri, 13 Mar 2009 22:48:47 +0000"/>
                            <attachment id="12402299" name="hive.347.2.patch" size="3429" author="namit" created="Mon, 16 Mar 2009 17:58:41 +0000"/>
                            <attachment id="12402334" name="hive.347.3.patch" size="3296" author="namit" created="Tue, 17 Mar 2009 00:16:17 +0000"/>
                            <attachment id="12402600" name="hive.347.4.patch" size="3276" author="namit" created="Thu, 19 Mar 2009 20:22:29 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 14 Mar 2009 03:14:06 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73596</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 45 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l9av:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122157</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-348] Provide type information to custom mappers and reducers.</title>
                <link>https://issues.apache.org/jira/browse/HIVE-348</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Either by setting an environment variable with the schema, or (ideally), configurably passing JSON data to custom mappers/reducers. This would be more data to pump into the pipe on each mapper/reducer, but it would be generated on the mapper/reducer, and it may mean much less processing for the custom script (using a fast json library compared to casting each column that needs casting based on the schema). &lt;/p&gt;</description>
                <environment></environment>
        <key id="12416878">HIVE-348</key>
            <summary>Provide type information to custom mappers and reducers.</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21140&amp;avatarType=issuetype">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="indigoviolet">Venky Iyer</reporter>
                        <labels>
                    </labels>
                <created>Fri, 13 Mar 2009 23:38:13 +0000</created>
                <updated>Mon, 3 Aug 2009 18:43:48 +0000</updated>
                                                                                <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12733835" author="zshao" created="Tue, 21 Jul 2009 21:22:27 +0000"  >&lt;p&gt;I think we should pass type information in the environment variable.&lt;/p&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12407836">HIVE-51</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 21 Jul 2009 21:22:27 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>42856</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 27 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i08ofr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>48558</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>


<item>
            <title>[HIVE-349] HiveHistory: TestCLiDriver fails if there are test cases with  no tasks</title>
                <link>https://issues.apache.org/jira/browse/HIVE-349</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;TestCLIDriver Fails for some test cases.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12416882">HIVE-349</key>
            <summary>HiveHistory: TestCLiDriver fails if there are test cases with  no tasks</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="santony73">Suresh Antony</assignee>
                                    <reporter username="santony73">Suresh Antony</reporter>
                        <labels>
                    </labels>
                <created>Sat, 14 Mar 2009 00:54:21 +0000</created>
                <updated>Sat, 17 Dec 2011 00:08:29 +0000</updated>
                            <resolved>Wed, 25 Mar 2009 04:55:21 +0000</resolved>
                                    <version>0.3.0</version>
                                    <fixVersion>0.3.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12688529" author="prasadc" created="Tue, 24 Mar 2009 01:48:32 +0000"  >&lt;p&gt;looks good. +1&lt;/p&gt;</comment>
                            <comment id="12689001" author="zshao" created="Wed, 25 Mar 2009 04:55:21 +0000"  >&lt;p&gt;Committed revision 758139. Thanks Suresh!&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12416662">HIVE-342</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12402184" name="patch_349_1.txt" size="955" author="santony73" created="Sat, 14 Mar 2009 00:58:32 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 24 Mar 2009 01:48:32 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73595</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 44 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l9b3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122158</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-349&quot; title=&quot;HiveHistory: TestCLiDriver fails if there are test cases with  no tasks&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-349&quot;&gt;&lt;strike&gt;HIVE-349&lt;/strike&gt;&lt;/a&gt;. Fix TestCliDriver when there are test cases with no tasks. (Suresh Antony via zshao)</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-350] [Hive] wrong order in explain plan</title>
                <link>https://issues.apache.org/jira/browse/HIVE-350</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;In case of multiple aggregations, the explain plan might be wrong -the order of aggregations since AbParseInfo maintains the information in a hashmap, which does the guarantee the results to be returned in order&lt;/p&gt;</description>
                <environment></environment>
        <key id="12416987">HIVE-350</key>
            <summary>[Hive] wrong order in explain plan</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="namit">Namit Jain</assignee>
                                    <reporter username="namit">Namit Jain</reporter>
                        <labels>
                    </labels>
                <created>Mon, 16 Mar 2009 18:00:17 +0000</created>
                <updated>Sat, 17 Dec 2011 00:08:38 +0000</updated>
                            <resolved>Wed, 18 Mar 2009 22:36:41 +0000</resolved>
                                                    <fixVersion>0.3.0</fixVersion>
                                    <component>Query Processor</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12682816" author="namit" created="Tue, 17 Mar 2009 21:35:54 +0000"  >&lt;p&gt;The explain plan is not wrong perse - the order is random, and eventually the select corrects the order. But, it was very difficult to understand&lt;/p&gt;</comment>
                            <comment id="12683200" author="athusoo" created="Wed, 18 Mar 2009 22:05:45 +0000"  >&lt;p&gt;+1.&lt;/p&gt;

&lt;p&gt;Looks good to me.&lt;/p&gt;</comment>
                            <comment id="12683215" author="namit" created="Wed, 18 Mar 2009 22:36:41 +0000"  >&lt;p&gt;committed.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12402422" name="hive.350.1.patch" size="54845" author="namit" created="Tue, 17 Mar 2009 21:34:48 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 18 Mar 2009 22:05:45 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73594</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 45 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l9bb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122159</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-351] Dividing by zero raises an exception.</title>
                <link>https://issues.apache.org/jira/browse/HIVE-351</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Can I just have a NULL instead?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12417008">HIVE-351</key>
            <summary>Dividing by zero raises an exception.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="2">Won&apos;t Fix</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="sasmith">S. Alex Smith</reporter>
                        <labels>
                    </labels>
                <created>Mon, 16 Mar 2009 23:10:16 +0000</created>
                <updated>Thu, 17 Dec 2009 23:27:25 +0000</updated>
                            <resolved>Thu, 17 Dec 2009 23:27:25 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12792218" author="zshao" created="Thu, 17 Dec 2009 23:27:25 +0000"  >&lt;p&gt;It&apos;s easy to use: &quot;IF(col = 0, NULL, col)&quot; to replace &quot;col&quot;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 17 Dec 2009 23:27:25 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73593</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 6 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l9bj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122160</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-352] Make Hive support column based storage</title>
                <link>https://issues.apache.org/jira/browse/HIVE-352</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;column based storage has been proven a better storage layout for OLAP. &lt;br/&gt;
Hive does a great job on raw row oriented storage. In this issue, we will enhance hive to support column based storage. &lt;br/&gt;
Acctually we have done some work on column based storage on top of hdfs, i think it will need some review and refactoring to port it to Hive.&lt;/p&gt;


&lt;p&gt;Any thoughts?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12417027">HIVE-352</key>
            <summary>Make Hive support column based storage</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21141&amp;avatarType=issuetype">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="he yongqiang">He Yongqiang</assignee>
                                    <reporter username="he yongqiang">He Yongqiang</reporter>
                        <labels>
                    </labels>
                <created>Tue, 17 Mar 2009 07:15:44 +0000</created>
                <updated>Tue, 14 Aug 2012 06:02:16 +0000</updated>
                            <resolved>Fri, 1 May 2009 06:35:56 +0000</resolved>
                                                    <fixVersion>0.4.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>14</watches>
                                                                <comments>
                            <comment id="12682716" author="jsensarma" created="Tue, 17 Mar 2009 16:41:29 +0000"  >&lt;p&gt;thanks for taking this on. this could be pretty awesome.&lt;/p&gt;

&lt;p&gt;traditionally the arguments for columnar storage has been limited &apos;scan bandwidth&apos; and compression. In practice - we see that scan bandwidth has two components:&lt;br/&gt;
1. disk/file-system bandwidth to read data&lt;br/&gt;
2. compute cost to scan data&lt;/p&gt;

&lt;p&gt;most columnar stores optimize for both (especially because in shared disk architectures - #1 is at premium). However - our limited experience says is that in Hadoop #1 is almost infinite. #2 can still be a bottleneck though. (it is possible that this observation applies because of high hadoop/java compute overheads - regardless - this seems to be reality).&lt;/p&gt;

&lt;p&gt;Given this - i like the idea of a scheme where columns are stored as independent streams inside a block oriented file format (each file block contains a set of rows, however - the organization inside blocks is by column). This does not optimize for #1 - but does optimize for #2 (potentially in conjunction with Hive&apos;s interfaces to get one column at a time from IO Libraries). It also gives us nearly equivalent compression.&lt;/p&gt;

&lt;p&gt;(The alternative scheme of having  different file(s) per column is also complicated by the fact that locality is almost impossible to ensure and there is no reasonable ways of asking hdfs to colocate different file segments in the near future).&lt;/p&gt;

&lt;p&gt;&amp;#8211;&lt;/p&gt;

&lt;p&gt;i would love to understand how you are planning to approach this. will we still use sequencefiles as a container - or should we ditch it? (it wasn&apos;t a great fit for hive - given that we don&apos;t use the key field - but the best thing we could find). We have seen that having a number of open codecs can hurt in memory usage - that&apos;s one open question for me - can we actually afford to open N concurrent compressed streams (assuming each column is stored compressed separately).&lt;/p&gt;

&lt;p&gt;It also seems that one could define a ColumnarInputFormat/OutputFormat as a generic api with different implementations and different pluggable containers underneath - and a scheme of either file per column or columnar in a block approach. in that sense we could build something more generic for hadoop (and then just make sure that hive&apos;s lazy serde uses the columnar api for data access - instead of the row based api exposed by current inputformat).&lt;/p&gt;</comment>
                            <comment id="12682740" author="he yongqiang" created="Tue, 17 Mar 2009 18:16:10 +0000"  >&lt;p&gt;Thanks, Joydeep Sen Sarma. Your feedback is really important.&lt;/p&gt;

&lt;p&gt;1. store schema.  block-wise column store or one file per column.&lt;br/&gt;
Our current implementation stores each column in one file. And the most annoying part for us, just as you said, is that currently and even in near future, hdfs does not support to colocate different file segements for columns in a same table.  So some operations need to fetch data from a new file(like a mapside hash join, a join with CompositeInputFormat) or need to add new map reduce job to merge data together.  Some operations are pretty good for this. &lt;br/&gt;
I think block-wise column is a good point. I will try to imprement it nearly. With different columns collocated in a single block, some operations do not need a reduce part(which is really time-consuming).&lt;/p&gt;

&lt;p&gt;2. compression&lt;br/&gt;
With different columns in different files, some light weight compressions,such as RLE, dictionay and bit vector encoding, can be used. One benefit of these light weight compression algorithms is that some operations does not need to decompression the data.&lt;br/&gt;
If we implement the block-wise column storage, should we also need to specify the light weight compression algorithm for each column or we choose one( like RLE) internally if the data is of good cluster nature? Since dictionary and bit vector should also be supported, the comlumns with these compression algorithms should be also placed in the block-wise columnar file? I think placing these columns in seperate files can be handled more easily? But i do not know whether it can fit into Hive. I am new to Hive.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;having a number of open codecs can hurt in memory usage&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;currently I can not think up a solution to avoid this for column per file store.&lt;/p&gt;

&lt;p&gt;3.file format&lt;br/&gt;
yeah. i think we need to add new file formats and their corresponding InputFormats. Currently, we have implemented the VFile(Value File, we do not need to store a key part), and BitMapFile. We have not implemented a DictionayFile, instead we use a header file for VFile to store dictionary entries. The header file for VFile is not needed for some columns and sometimes it is must. &lt;br/&gt;
I think the refactor of file formats should be the start for this issue.&lt;/p&gt;

&lt;p&gt;Thanks again.&lt;/p&gt;</comment>
                            <comment id="12683293" author="zshao" created="Thu, 19 Mar 2009 03:39:21 +0000"  >&lt;p&gt;Hi Yongqiang,&lt;/p&gt;

&lt;p&gt;Sorry for jumping on this issue late.&lt;/p&gt;


&lt;p&gt;Let me summaries the choices that we have to make:&lt;/p&gt;

&lt;p&gt;A. Put different columns in different files (we can still have column-set - a bunch of columns in the same file)&lt;/p&gt;

&lt;p&gt;B. Put different columns in the same file, but organize it in a block-based way. In a single block, the first column of all rows are in the front, then the second column, etc. &lt;br/&gt;
B1: Write a new FileFormat&lt;br/&gt;
B2: Continue to use SequenceFileFormat&lt;br/&gt;
B2.1: Store a block in multiple records, one record for each column. Use the key to label the beginning of a block (or column id).&lt;br/&gt;
B2.2: Store a block in a single record&lt;/p&gt;


&lt;p&gt;Comparing A and B: 1. B is much easier to implement than A. Hadoop jobs take files as input. If the data is stored in a single file, it&apos;s much easier to either read or write to the file. 2. B may have the advantage of locality. 3. B may require a little bit more memory buffer for writing. 4. B may not be as efficient as A in reading since all data need to be read (unless the FileFormat supports &quot;skip&quot; but that might create more random seeks depending on block size).&lt;/p&gt;

&lt;p&gt;Comparing B1 and B2: 1. B1 is much more flexible since we can do whatever we want (especially skip-reading etc); 2. B2 is much easier to do and we naturally enjoy all benefits of SequenceFile: splittable, customizable compression codec.&lt;/p&gt;

&lt;p&gt;Comparing B2.1 and B2.2: 1. B2.2 is easier to implement, because we don&apos;t have the problem of splitting different columns of the same block into multiple mappers. 2. B2.1 is potentially more efficient when we allow SequenceFile to skip record and ask Hive to tell us which of the columns can be skipped.&lt;/p&gt;

&lt;p&gt;As a result, I would suggest to try B2.2 as the first exercise, then try B2.1, then B1, then A.&lt;/p&gt;

&lt;p&gt;The amount of work for each level (B2.2, B2.1, B1, A) will probably differ by a factor of 3-5. So it does not hurt much by starting from B2.2, and also the first steps will be good learning steps for the next ones.&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="12683348" author="jsensarma" created="Thu, 19 Mar 2009 07:17:09 +0000"  >&lt;p&gt;&amp;gt;B2.2 is easier to implement, because we don&apos;t have the problem of splitting different columns of the same block into multiple mappers.&lt;/p&gt;

&lt;p&gt;for B2.1 - we may be able to control when sequencefile writes out sync markers (or at least we should investigate if that&apos;s easy enough to do by extending SequenceFile). the advantage of avoiding reading specific columns seems pretty significant.&lt;/p&gt;

&lt;p&gt;OTOH - one can also easily imagine that  SequenceFile does not copy data into a BytesWritable - rather that we have a special Writable structure such that when the read on it is invoked - it just copies the reference to the underlying byte buffer. that way there are no copies of data in sequencefile reader and the application (in this case the columnar format reader) - is able to skip to the relevant sections of data without touching the irrelevant columns. if we do it this way - B2.2 has no performance downside. &lt;/p&gt;

&lt;p&gt;regarding the compression related questions raised by Yongqiang - it seems to me that trying out the most generic compression algorithm (gzip) is better - trying to specify or infer best compression technique per column much harder and something that can be done later. one thing we could do to mitigate the number of open codecs is to simply accumulate all the data uncompressed in a buffer per column and then do the compression in one shot at the end (once we think enough data is accumulated) using just one codec object.  this obviously seems non optimal from the point of view of having to scan data multple times - OTOH - there were known issues with older versions of hadoop with lots of open codecs. &lt;/p&gt;</comment>
                            <comment id="12683353" author="zshao" created="Thu, 19 Mar 2009 07:53:45 +0000"  >&lt;p&gt;Let&apos;s do B2.2 first. I guess there will need to be some interface change to make it possible (SerDe now only deserializes one row out of one Writable, while we are looking for multiple rows per Writable). We can use the sequencefile compression support transparently.&lt;/p&gt;

&lt;p&gt;Once B2.2 is done, we can move to B2.1. As Joydeep said, we may need to extend SequenceFile to make split work. At the same time we might want to use SequenceFile record-compression (instead of SequenceFile block-compression) if we can make relatively big records. That will save us the time of decompressing unnecessary columns. Or we can disable SequenceFile compression, and compress record by record by ourselves. As Joydeep said, we will have to decide whether we want to open a big number of codecs at the same time, or buffer all uncompressed data and compress one column by one column when writing out. BZip2Codec needs 100KB to 900KB per compression codec.&lt;/p&gt;</comment>
                            <comment id="12683783" author="he yongqiang" created="Fri, 20 Mar 2009 07:40:00 +0000"  >&lt;p&gt;Thanks, Joydeep and Zheng. The advises are really helpful.&lt;br/&gt;
I have written a draft document according to suggestions from Zheng and Joydeep.&lt;br/&gt;
Here is the link: &lt;a href=&quot;http://docs.google.com/Doc?id=dc9jpfdr_3ft7w3hc4&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://docs.google.com/Doc?id=dc9jpfdr_3ft7w3hc4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I agree with you guys, we can start from B2, and then B1. And finally find out should we need to add the VFile in.&lt;br/&gt;
BTW, yestoday i also took a look on MapFile, which i found VFile has a same with MapFile in that VFlie sometimes also need an index file. The main difference is that VFile does not need a key part and sometimes even the value&apos;s length part. Because a VFile stores one column, each column has a type, and if the data type of that column is fix lengthed, it only needs to store the raw value bytes.&lt;/p&gt;</comment>
                            <comment id="12688397" author="jsensarma" created="Mon, 23 Mar 2009 19:57:20 +0000"  >&lt;p&gt;if u are doing B2.2 - i think it&apos;s still pretty easy to make sure that we don&apos;t decompress all columns when we only want a few. using sequencefile record compression - that&apos;s what will happen - and i think the performance gain might be much less (the benefit would be reduced primarily to better compression of the data due to columnar format)&lt;/p&gt;

&lt;p&gt;In this past i have written a dummywritable class that doesn&apos;t deserialize - but just passes the inputstream passed in by hadoop to the application. (the serialization framework does this in a less hacky way - and we could do that as well). if u do it this way - hive serde can get a massive blob of binary data - and then based on header metadata - only decompress the relevant parts of it. &lt;/p&gt;

&lt;p&gt;ie - i don&apos;t think we ever need to do B2.1 if we do B2.2 this way. &lt;/p&gt;</comment>
                            <comment id="12688596" author="prasadc" created="Tue, 24 Mar 2009 07:20:31 +0000"  >&lt;p&gt;joydeep, do you mean impose our own record structure within a sequence file record? That is, a sequence file record value contains array of compressed column groups for N rows (N mini-records) and sequence file key contains array of length of compressed column groups(another N mini-records). &lt;/p&gt;</comment>
                            <comment id="12688606" author="he yongqiang" created="Tue, 24 Mar 2009 08:16:29 +0000"  >&lt;p&gt;Thank you for the advices, joydeep.  &lt;br/&gt;
yeah,i am working on B2.2. And i am hoping i can finish a draft version in the coming few days.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;don&apos;t decompress all columns when we only want a few&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Using the SequenceFile to implement B2.2( columnar storage in one record), if we use RecordCompression, I think SequenceFile will decompress the whole value part thus all the data. It is really tough, since decompression needs to touch all the data, and if no compression at all then one advantage of columnar storage will lost. Even without compression, SequenceFile still needs to read the whole value part into memory, and disallow skipping data (Hadoop-5553). &lt;br/&gt;
So i guess i may need to discard SequenceFile, which can support skipping data and compression. &lt;/p&gt;

&lt;p&gt;Any comments?&lt;/p&gt;</comment>
                            <comment id="12688611" author="he yongqiang" created="Tue, 24 Mar 2009 08:27:17 +0000"  >&lt;p&gt;By &quot;So i guess i may need to discard SequenceFile, which can support skipping data and compression. &quot; I mean:&lt;/p&gt;

&lt;p&gt;i may need to discard SequenceFile, and implement a new one, which has many similarities with SequenceFile but can support skipping data and fine grained compression (in record each column compression). &lt;/p&gt;</comment>
                            <comment id="12688725" author="jsensarma" created="Tue, 24 Mar 2009 15:58:44 +0000"  >&lt;p&gt;it&apos;s not clear to me that we need to ditch Sequencefile for the short term. Like Prasad said - we can impose our own structure on the sequencefile record which can allow skipping unnecessary data.&lt;/p&gt;

&lt;p&gt;we cannot use record compression obviously. There are two approaches you can take:&lt;/p&gt;

&lt;p&gt;1. keep using a BytesWritable (or Text) for the &apos;value&apos; part and impose ur own layout inside this so that the ColumnarSerDe only needs to seek to and decompress the relevant column). This does require one copy of the entire data from sequencefile &apos;value&apos; to the BytesWritable&lt;br/&gt;
2. use the Hadoop serializer framework (see src/core/org/apache/hadoop/io/serializer) - and get Hadoop to pass u the input stream directly (for reading the &apos;value&apos; part). The custom deserializer can then be configured via Hive&apos;s plan to only copy out the bytes that are of interest to the Hive plan.&lt;/p&gt;

&lt;p&gt;#2 is obviously more complicated - and in practice straighline data copies of hot data is not that expensive (since Hadoop has already done a crc check on all this data and it&apos;s typically already in processor caches and fast to scan again).&lt;/p&gt;

&lt;p&gt;So i would try out #1 to begin with. &lt;/p&gt;</comment>
                            <comment id="12689041" author="he yongqiang" created="Wed, 25 Mar 2009 09:15:13 +0000"  >&lt;p&gt;Thanks, Joydeep and Prasad.&lt;br/&gt;
First i would like to make an update to the recent work:&lt;br/&gt;
I had implemented an initial RCFile which was just a wrapper of SequenceFile, and it relied on Hadoop-5553. Since it seems Hadoop-5553 will not be resolved, I have implemented another RCFile, which copies many code form SequenceFile( especially the Writer code), and provides the same on-disk data layout as SequenceFile.&lt;/p&gt;

&lt;p&gt;Here is a draft description of the new RCFile:&lt;br/&gt;
1) Only record compression or no compression at all. &lt;/p&gt;

&lt;p&gt;    In B2.2 we store a bunch of raw rows into one record in a columnar way. So there is no need for block compression, because block compression will decompress all the data.&lt;/p&gt;

&lt;p&gt;2) In-record compression.&lt;br/&gt;
    If the writer is created with compress flag, then the value part in one record is compressed but with a column compression style. The layout is like this:&lt;/p&gt;

&lt;p&gt;Record length&lt;br/&gt;
Key length&lt;/p&gt;
{the below is the Key part} 
&lt;p&gt;number_of_rows_in_this_record(vint)&lt;br/&gt;
column_1_ondisk_length(vint),column_1_row_1_value_plain_length, column_1_row_2_value_plain_length,....&lt;br/&gt;
column_2_ondisk_length(vint),column_2_row_1_value_plain_length, column_2_row_2_value_plain_length,....&lt;br/&gt;
..........&lt;/p&gt;
{the end of the key part}
{the begin of the value part}
&lt;p&gt;Compressed data or plain data of &lt;span class=&quot;error&quot;&gt;&amp;#91;column_1_row_1_value, column_1_row_2_value,....&amp;#93;&lt;/span&gt;&lt;br/&gt;
Compressed data or plain data of &lt;span class=&quot;error&quot;&gt;&amp;#91;column_2_row_1_value, column_2_row_2_value,....&amp;#93;&lt;/span&gt;&lt;/p&gt;
{the end of the value part}

&lt;p&gt;The key part: KeyBuffer&lt;br/&gt;
The value part : ValueBuffer&lt;/p&gt;

&lt;p&gt;3) the reader&lt;/p&gt;

&lt;p&gt;It now only provides 2 API:&lt;br/&gt;
next(LongWritable rowID): returns the next rowid number. I think it should be refined, because the rowid maybe not real rowid, and it is only the already passed rows from the beginning of the reader.&lt;/p&gt;

&lt;p&gt;List&amp;lt;Bytes&amp;gt; getCurrentRow() will return all the columns raw bytes of one row. Because the reader can let use specify the column ids which should be skipped, so the returned List&amp;lt;Bytes&amp;gt; only contains the unskipped columns bytes. Maybe it is better to store a NullBytes in the returned list to represent a skipped column.&lt;/p&gt;</comment>
                            <comment id="12689151" author="he yongqiang" created="Wed, 25 Mar 2009 16:34:32 +0000"  >&lt;p&gt;One problem with this RCFile is that it needs to know the needed columns in advance, so it can skip and avoid decompress unneeded columns. &lt;br/&gt;
I took a look at Hive&apos;s operators and SerDe, it seems that they all take a whole row object as input and do not know which column is needed before processing. &lt;br/&gt;
Like with LazyStruct and StructObjectInspector, they only know which column is needed when getField/getStructFieldData is invoked by operators&apos; evalators( like ExprNodeColumnEvaluator).&lt;/p&gt;</comment>
                            <comment id="12689221" author="zshao" created="Wed, 25 Mar 2009 20:13:54 +0000"  >&lt;p&gt;@Yongqiang: The reason that we do that &quot;lazy&quot; operation is that there will be &quot;CASE&quot;, &quot;IF&quot; and short-circuiting boolean operations which will allow us to skip different columns for different rows.&lt;/p&gt;

&lt;p&gt;We need a new top-level StructObjectInspector which can deserialize the column only when getField/getStructFieldData is called on that column. For all levels below that, we can reuse the current ObjectInspector.&lt;/p&gt;</comment>
                            <comment id="12689492" author="he yongqiang" created="Thu, 26 Mar 2009 14:04:41 +0000"  >&lt;blockquote&gt;
&lt;p&gt;impose our own structure on the sequencefile record which can allow skipping unnecessary data&lt;br/&gt;
impose ur own layout inside this so that the ColumnarSerDe only needs to seek to and decompress the relevant column&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can you give a detailed intro about the &quot;ur own layout&quot;? Thanks!!&lt;/p&gt;</comment>
                            <comment id="12689705" author="rsm" created="Thu, 26 Mar 2009 22:33:40 +0000"  >&lt;p&gt;Here are a few insightful articles about using a row-based query processor along with a column-store. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.databasecolumn.com/2008/12/debunking-yet-another-myth-col.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.databasecolumn.com/2008/12/debunking-yet-another-myth-col.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.databasecolumn.com/2008/07/debunking-another-myth-columns.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.databasecolumn.com/2008/07/debunking-another-myth-columns.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Before we go ahead and implement everything, it would be a good idea to do some performance analysis on a small prototype.&lt;/p&gt;</comment>
                            <comment id="12689794" author="he yongqiang" created="Fri, 27 Mar 2009 03:03:30 +0000"  >&lt;p&gt;Thanks, Raghotham Murthy.&lt;br/&gt;
Besides these two posts, there are also several useful papers,like&lt;br/&gt;
C-Store: A Column-oriented DBMS  &lt;br/&gt;
Column-Stores vs. Row-Stores- How Different Are They Really-sigmod08&lt;br/&gt;
A Comparison of C-Store and Row-Store in a Common Framework&lt;br/&gt;
Materialization Strategies in a Column-Oriented DBMS.&lt;br/&gt;
Integrating compression and execution in column-oriented database systems&lt;/p&gt;

&lt;p&gt;In these papers, which are written mostly(all?) by people in vertica, they place most emphasis on the column-oriented execution layer together with a column storage layer. I totally agree with these opinions. And actually we observed that operators with map-reduce approach have many differences with the ones implemented in systems like CStore.  And we also found that bitmap compression can extremely reduce the execution time.&lt;br/&gt;
So i guess we can first try to support a column storage layer, and then we can add some column oriented operators and column-specific compression algorithms.&lt;br/&gt;
I will try to provide a small prototype of the storage layer as soon as possible.&lt;/p&gt;</comment>
                            <comment id="12689796" author="he yongqiang" created="Fri, 27 Mar 2009 03:21:25 +0000"  >&lt;p&gt;Also the cost of tuple reconstruction accounts for a large proportion of the whole execution time. In our initial exprements, the reconstruction cost is much higher than the benefit of intergreting the column-execution and the underlying column-storage. The reconstruction is a Map-Reduce join operation. The cost can be extremely reduced in some queries when we can reduce the number of tuples needed to reconstruct. The key to this is a late materialization.&lt;br/&gt;
But in the current B2.2, the localize rows in a single file and adopt a record-level columnar storage, it does not have the tuple reconstruction cost. But it needs a more specific and more flexble compression algorithms, and i strongly recommed to support bitmap file in future. As the main benefit of a columnar strategy, it needs us to add some columnar operators in the next.&lt;br/&gt;
But now let us make the first step, and then add more optimizations.&lt;/p&gt;</comment>
                            <comment id="12690176" author="he yongqiang" created="Sat, 28 Mar 2009 05:22:31 +0000"  >&lt;p&gt;A draft version of B2.2.&lt;br/&gt;
Thank you a lot for comments.&lt;/p&gt;</comment>
                            <comment id="12693664" author="zshao" created="Mon, 30 Mar 2009 05:25:57 +0000"  >&lt;p&gt;Haven&apos;t looked it completely through yet.&lt;/p&gt;

&lt;p&gt;Some initial comments:&lt;/p&gt;

&lt;p&gt;BytesRefWritable.java: You might want to replace all BytesWritable to BytesRefWritable.&lt;br/&gt;
86: I don&apos;t understand why equal() can be implemented like this.&lt;/p&gt;

&lt;p&gt;ColumnarSerDe.java: You might want to refactor LazySimpleSerDe to extract out the common functionality, (and reuse them in ColumnarSerDe), instead of inheriting from LazySimpleSerDe.  This will give you much better control -  the current implementation won&apos;t work unless you also override initialize(), serialize() - basically all methods from LazySimpleSerDe.&lt;/p&gt;

&lt;p&gt;If you would like to refactor LazySimpleSerDe to extract out the common functionality for ColumnarSerDe, make sure you follow &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-375&quot; title=&quot;LazySimpleSerDe to directly serialize (append) int/long/byte/short etc to UTF-8 buffer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-375&quot;&gt;&lt;del&gt;HIVE-375&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;Since &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-375&quot; title=&quot;LazySimpleSerDe to directly serialize (append) int/long/byte/short etc to UTF-8 buffer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-375&quot;&gt;&lt;del&gt;HIVE-375&lt;/del&gt;&lt;/a&gt; is not committed yet, you might want to work on &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-360&quot; title=&quot;Generalize the FileFormat Interface in Hive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-360&quot;&gt;&lt;del&gt;HIVE-360&lt;/del&gt;&lt;/a&gt; first.&lt;/p&gt;</comment>
                            <comment id="12693665" author="zshao" created="Mon, 30 Mar 2009 05:29:29 +0000"  >&lt;p&gt;@Raghu: Thanks for the references. For this issue, we are working in a fast-prototyping mode. We try the simplest approach (B2.2) first, gain some experiences, and then go for more time-consuming approaches. So right now we are just building out a prototype.&lt;/p&gt;</comment>
                            <comment id="12693674" author="he yongqiang" created="Mon, 30 Mar 2009 06:50:41 +0000"  >&lt;p&gt;An update version according Zheng&apos;s suggestions.&lt;br/&gt;
1) remove all usage of BytesWritable, and replace them with BytesRefWritable&lt;br/&gt;
2) change modifier of rowTypeInfo in LazySimpleSerDe to default&lt;br/&gt;
3) change modifer of uncheckedGetField in LazyStruct from private to protected, which can allow subclasses to override its behaviour&lt;br/&gt;
4) several fixes of initial RCFile&lt;br/&gt;
5) a runnable TestRCFile (the TestRCFile in the previous attatched file can not even pass the compile, my mistake.)&lt;/p&gt;

&lt;p&gt;Thanks,Zheng.&lt;/p&gt;</comment>
                            <comment id="12695220" author="he yongqiang" created="Fri, 3 Apr 2009 03:26:38 +0000"  >&lt;p&gt;Need to generalize the FileFormat Interface in Hive first. Then it can allow RCFile Format be added more easier.&lt;/p&gt;</comment>
                            <comment id="12699045" author="he yongqiang" created="Wed, 15 Apr 2009 04:18:24 +0000"  >&lt;p&gt;This is the latest work on this issue.&lt;br/&gt;
1) added new columnar serde&lt;br/&gt;
2) new columnar struct&lt;br/&gt;
3) RCFile now includes a partial read test&lt;br/&gt;
4) RCFileOutputFormat now implements HiveOutputForamt&lt;/p&gt;

&lt;p&gt;I think next step, more tests are needed, especaill on HiveOutputFormat. &lt;/p&gt;</comment>
                            <comment id="12699046" author="he yongqiang" created="Wed, 15 Apr 2009 04:19:08 +0000"  >&lt;p&gt;&amp;gt;&amp;gt;more tests are needed, especaill on HiveOutputFormat. &lt;br/&gt;
I mean RCFileOutputFormat.&lt;/p&gt;</comment>
                            <comment id="12699050" author="zshao" created="Wed, 15 Apr 2009 04:40:58 +0000"  >&lt;p&gt;Please try a simple test for writing/reading using the new SerDe and file format:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;CREATE table columnTable (key STRING, value STRING)
ROW FORMAT SERDE
  &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.hadoop.hive.serde.columnar.ColumnarSerDe&apos;&lt;/span&gt;
STORED AS
  INPUTFORMAT &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.hadoop.hive.serde.columnar.RCFIleInputFormat&apos;&lt;/span&gt;
  OUTPUTFORMAT &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.hadoop.hive.serde.columnar.RCFIleOutputFormat&apos;&lt;/span&gt;;

INSERT OVERWRITE TABLE columnTable
SELECT src.*
FROM src;

SELECT *
FROM columnTable;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12699719" author="he yongqiang" created="Thu, 16 Apr 2009 14:35:02 +0000"  >&lt;p&gt;against the latest truck.&lt;/p&gt;

&lt;p&gt;1) added a simple rcfile_columar.q file for test.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;DROP TABLE columnTable;
CREATE table columnTable (key STRING, value STRING)
ROW FORMAT SERDE
  &apos;org.apache.hadoop.hive.serde2.lazy.ColumnarSerDe&apos;
STORED AS
  INPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.RCFileInputFormat&apos;
  OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.RCFileOutputFormat&apos;;

FROM src
INSERT OVERWRITE TABLE columnTable SELECT src.key, src.value LIMIT 10;
describe columnTable;

SELECT columnTable.* FROM columnTable;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2) let ColumnarSerDe&apos;s serialize returns BytesRefArrayWritable instead of Text&lt;/p&gt;

&lt;p&gt;BTW, it seems rcfile_columar.q.out does not contain results of SELECT columnTable.* FROM columnTable; &lt;br/&gt;
but after the test, i saw file ql/test/data/warehouse/columntable/attempt_local_0001_r_000000_0, and it did contain the data inserted. &lt;br/&gt;
Why the select got nothing?&lt;/p&gt;</comment>
                            <comment id="12700025" author="he yongqiang" created="Fri, 17 Apr 2009 06:28:08 +0000"  >&lt;p&gt;Fixed the select problem.&lt;br/&gt;
And refatored the TestRCFile class.&lt;/p&gt;</comment>
                            <comment id="12700098" author="zshao" created="Fri, 17 Apr 2009 09:27:22 +0000"  >&lt;p&gt;hive-352-2009-4-17.patch:&lt;/p&gt;

&lt;p&gt;Very nice job!&lt;/p&gt;

&lt;p&gt;2 more tests to add:&lt;br/&gt;
1. Big data test. Take a look at ql/src/test/queries/clientpositive/groupby_bigdata.q to see how we generate big data sets.&lt;br/&gt;
2. Complex column types: Take a look at ./ql/src/test/queries/clientpositive/input_lazyserde.q&lt;/p&gt;

&lt;p&gt;Some other improvements:&lt;br/&gt;
1. ObjectInspectorFactory.getColumnarStructObjectInspector: I think you don&apos;t need byte separator and boolean lastColumnTakesRest. Just remove them.&lt;br/&gt;
2. ColumnarStruct.init: Can you cache/reuse the ByteArrayRef() instead of doing ByteArrayRef br = new ByteArrayRef() every time? The assumption in Hive is that data is already owned by creator, and whoever wants to keep the data for later use needs to get a deep copy of the Object by calling ObjectInspectorUtils.copyToStandardObject.&lt;br/&gt;
3. ColumnarStruct: comments should mention the difference against LazyStruct is that it reads data through init(BytesRefArrayWritable cols).&lt;br/&gt;
4. Can you put all changes to serde2.lazy package into a new package called serde2.columnar?&lt;br/&gt;
5. It seems there are a lot of shared code between LazySimpleSerDe and ColumnarSerDe, e.g. a lot of functionalities in init and serialize. Can you refactor LazySimpleSerde and put those common functionalities into public static methods, so that ColumnarSerDe can directly call? You might also want to put the configurations of the LazySimpleSerDe (nullString, separators, etc) into a public static Class, so that the public static methods will return it.&lt;/p&gt;</comment>
                            <comment id="12700116" author="zshao" created="Fri, 17 Apr 2009 10:23:42 +0000"  >&lt;p&gt;hive-352-2009-4-17.patch: &lt;/p&gt;

&lt;p&gt;Talked with Yongqiang offline. Two more things:&lt;/p&gt;

&lt;p&gt;1. RCFile.readFields is not very efficient (see below). I think we should lazily decompress the stream instead of decompress all of it and return the decompressor. The reason is that decompressed data can be very big and easily go out-of-memory (if we consider 1:10 or more compression ratio)&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;           &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (deflatFilter.available() &amp;gt; 0)
              valBuf.write(valueIn, 1);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2. Also we need to think about how we can pass the information of which columns are needed to Hive. Yongqiang is working on designing that. If anybody have good ideas, please chime in.&lt;/p&gt;</comment>
                            <comment id="12700542" author="zshao" created="Sun, 19 Apr 2009 00:02:48 +0000"  >&lt;p&gt;2 major approaches for the RCFileFormat to work are:&lt;br/&gt;
1. Lazy deserialization (and decompression): The Objects passed around in the Hive Operators can be wrappers of handles to underlying decompression streams which will decompress the data on the fly.&lt;br/&gt;
2. Column-hinting: Let Hive tell the FileFormat which columns are neede and which are not.&lt;/p&gt;

&lt;p&gt;There is a major benefit of Option 1 in a common case like this:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;SELECT key, value1, value2, value3, value4 from columnarTable where key = &lt;span class=&quot;code-quote&quot;&gt;&apos;xxyyzz&apos;&lt;/span&gt;;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;if the selectivity of &quot;key = &apos;xxyyzz&apos;&quot; is really high, we will end up decompressing very few blocks of value1 to value4.&lt;br/&gt;
This is not possible with Option 2.&lt;/p&gt;</comment>
                            <comment id="12700556" author="he yongqiang" created="Sun, 19 Apr 2009 07:14:46 +0000"  >&lt;p&gt;Agreed.&lt;br/&gt;
Can we have both?&lt;br/&gt;
1 is absolutely better for high selectivity filter clauses. With 2, we can skip loading unnecessary (compressed) columns into memory. &lt;br/&gt;
I have done a simple RCFile perform test in my local single machine. It seems RCFile perform much better in reading than block-compressed sequence file. I think the performance improvements should attribute to the skip strategy.&lt;br/&gt;
The below is a coarse results of comparing RCFile with SequenceFile (in local):&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Write RCFile with 10 random string columns and 100000 rows cost 9851 milliseconds. And the file&apos;s on disk size is 50527070
Read only one column of a RCFile with 10 random string columns and 100000 rows cost 448 milliseconds.
Write SequenceFile with 10  random string columns and 100000 rows cost 18405 milliseconds. And the file&apos;s on disk size is 52684063
Read SequenceFile with 10  random string columns and 100000 rows cost 9418 milliseconds.
Write RCFile with 25 random string columns and 100000 rows cost 15112 milliseconds. And the file&apos;s on disk size is 126262141
Read only one column of a RCFile with 25 random string columns and 100000 rows cost 467 milliseconds.
Write SequenceFile with 25  random string columns and 100000 rows cost 45586 milliseconds. And the file&apos;s on disk size is 131355387
Read SequenceFile with 25  random string columns and 100000 rows cost 22013 milliseconds.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I will post more detailed test results together with next patch.&lt;/p&gt;</comment>
                            <comment id="12700557" author="zshao" created="Sun, 19 Apr 2009 07:21:39 +0000"  >&lt;p&gt;I am surprised that RCFile is at least 2 times faster than SequenceFile in writing. What do you think is the reason?&lt;/p&gt;</comment>
                            <comment id="12700559" author="he yongqiang" created="Sun, 19 Apr 2009 07:38:10 +0000"  >&lt;p&gt;I am not sure. However, i observed that SequenceFile does much better in writing and in comression ratio if all rows data are the same.&lt;br/&gt;
I will post a patch now, although it is not finished. I have only finished adding big data test and complex column data test. The big data test is added in 2 ways:&lt;br/&gt;
1) added a rcfile_bigdata.q and &lt;br/&gt;
2) add some test codes in TestRCFile. And in it there are also comparison code of SequenceFile and RCFile, RCFile does not perform better in writing and compression ratio,  but much better in reading.&lt;/p&gt;

&lt;p&gt;The test results in previous post is generated by class PerformTestRCFileAndSeqFile.&lt;/p&gt;</comment>
                            <comment id="12700764" author="he yongqiang" created="Mon, 20 Apr 2009 11:15:12 +0000"  >&lt;p&gt;When I am testing RCFile&apos;s read performance, I notice severe read performance degradation when the column number get bigger. &lt;br/&gt;
I tentatively doubt that is caused by the DFSClient&apos;s DFSInputStream &apos;s skip code as shown below.  &lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;int diff = (int)(targetPos - pos);
        if (diff &amp;lt;= TCP_WINDOW_SIZE) {
          try {
            pos += blockReader.skip(diff);
            if (pos == targetPos) {
              done = true;
            }
          } catch (IOException e) {//make following read to retry
            LOG.debug(&quot;Exception while seek to &quot; + targetPos + &quot; from &quot;
                      + currentBlock +&quot; of &quot; + src + &quot; from &quot; + currentNode + 
                      &quot;: &quot; + StringUtils.stringifyException(e));
          }
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It seems that if I remove this piece of code, everything still works correctly ( in my test code). &lt;br/&gt;
Got an offline discussion with Zheng, and made several draft improvements:&lt;br/&gt;
1. compress each column directly, it means keep one codec for each column and write data directly to the column&apos;s corresponding compression stream. Currently RCFile buffers all the data first. When buffered data is greater than a config, compress each column separately and  flush them out. The direct compression strategy can increase the compression ratio.(this is not related the severe read performance degradation problem)&lt;/p&gt;

&lt;p&gt;To overcome the bad skip performance caused by TCP_WINDOW_SIZE(128K and not changeable at all), &lt;br/&gt;
2. make continuous  skips to a single skip, that way it would increase the bytes need skipped and increase the probability of not executing statements in the above if block. &lt;br/&gt;
3. Enhance the RCFile writer code to be aware of TCP_WINDOW_SIZE, and let most columns be greater than TCP_WINDOW_SIZE as far as possible. &lt;br/&gt;
    To do this, we need add a SLOP variable to allow buffer size be greater than configured size (default 4M) and less than SIZE*SLOP. With this we can let most columns&apos; buffer data be greater TCP_WINDOW_SIZE  as possible as we can.&lt;br/&gt;
    This method has many limitations when columns are getting more and more (we guess &amp;gt;100). &lt;br/&gt;
    Another way to do this is to let the buffer size be TCP_WINDOW_SIZE(128K) *columnNumber. &lt;br/&gt;
    Anyway, all the solutions I can think up can only ease the situation. &lt;/p&gt;

&lt;p&gt;Any thoughts on this? Thanks!&lt;/p&gt;</comment>
                            <comment id="12700765" author="he yongqiang" created="Mon, 20 Apr 2009 11:22:42 +0000"  >&lt;p&gt;More explaination to the read sharp decrease problem:&lt;br/&gt;
In our test, we use string columns. the data is randomly produced.&lt;br/&gt;
When column number is only a few, and the buffer size is 4M default. So every column&apos;s buffer is more than TCP_WINDOW_SIZE. So when skipping columns, the if block will not executed. But when columns are getting more, the buffer size each column can get become less. And finally, most columns&apos; buffer is less than TCP_WINDOW_SIZE. So the sharp decrease problem appears.&lt;/p&gt;</comment>
                            <comment id="12700941" author="zshao" created="Mon, 20 Apr 2009 21:22:12 +0000"  >&lt;p&gt;I agree we should do 1 and 2, but I don&apos;t feel 3 is worth to do.&lt;br/&gt;
As you mentioned, 3 will have memory problems when the number of columns grows very big - in that case, performance degradation is better than out-of-memory problem.&lt;/p&gt;

&lt;p&gt;For 3 the eventual solution would be to fix DFSClient to use a smaller number than TCP_WINDOW_SIZE. But if it is not easy to change DFSClient, and we continue to see this problem, we may want to switch to other solutions (B1 etc). Anyway, if TCP_WINDOW_SIZE is 128KB, and we have 20MB buffer (compressed size), we should be able to handle 160 columns, which is usually good enough for most applications.&lt;/p&gt;</comment>
                            <comment id="12701074" author="zshao" created="Tue, 21 Apr 2009 06:51:56 +0000"  >&lt;p&gt;Yongqiang talked with me offline since 1 conflicts with Joydeep&apos;s earlier comment: &quot;We have seen that having a number of open codecs can hurt in memory usage - that&apos;s one open question for me - can we actually afford to open N concurrent compressed streams (assuming each column is stored compressed separately).&quot;&lt;/p&gt;

&lt;p&gt;I think we should do an experiment to see how much memory each concurrent compressing stream takes - in reality, most tables will have less than 100 columns, so I guess if each codec takes 100K-500K memory it&apos;s affordable (total memory usage: 10MB-50MB), otherwise we need to rethink about 1. I &lt;/p&gt;</comment>
                            <comment id="12701437" author="he yongqiang" created="Wed, 22 Apr 2009 08:32:12 +0000"  >&lt;p&gt;4-22 progress.txt describes latest modifications hive-352-2009-4-22.patch made.&lt;br/&gt;
4-22 performance.txt gives a simple test results, which can be reproduced by running org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile (it uses LocalFileSystem). I am not quite confident with the RCFile&apos;s read performance, the results show it does much better than SequenceFile. Are there some bugs?  I tested with TestRCFile and three added .q files, they all passed in my local.&lt;/p&gt;</comment>
                            <comment id="12701512" author="he yongqiang" created="Wed, 22 Apr 2009 12:44:41 +0000"  >&lt;p&gt;According to Zheng&apos;s suggestions, hive-352-2009-4-22-2.patch made several improvements againds hive-352-2009-4-22.patch:&lt;br/&gt;
1) let each row data randomly produced in the test be string bytes( the previous one produces binary bytes) &lt;br/&gt;
2) add correctness parameter in performance test to allow test what we read are what we wrote ( in PerformTestRCFileAndSeqFile).&lt;/p&gt;

&lt;p&gt;4-22 performace2.txt added more detailed test results:&lt;br/&gt;
1. local using bulk decompression,in RCFile-&amp;gt;ValueBuffer-&amp;gt;readFields(), like:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;bufferRef.write(valueIn, columnBlockPlainDataLength);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;2. locak not using bulk decompression,in RCFile-&amp;gt;ValueBuffer-&amp;gt;readFields(), like:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;while(deflateFilter.available()&amp;gt;0)
bufferRef.write(valueIn, 1);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;3. using DistributedFileSystem and bulk decompression, the tests are still run on my local machine&lt;/p&gt;

&lt;p&gt;Here are the brief results(more detail, pls take a look attached 4-22 performace2.txt)&lt;/p&gt;

&lt;p&gt;1.&lt;br/&gt;
(LocalFileSystem)Use Bulk decompression in RCFile-&amp;gt;ValueBuffer-&amp;gt;ReadFileds, and adds some noisy between two RCFile reading, and after written to avoid disk cache.&lt;/p&gt;

&lt;p&gt;column number| RCFile size | RCFile read 1 column | RCFile read 2 column | RCFile read all columns |Sequence file size | sequence file read all&lt;/p&gt;

&lt;p&gt;10| 11501112| 259| 181| 498| 13046020| 7002&lt;br/&gt;
25| 28725817| 233| 269| 1082| 32246409| 16539&lt;br/&gt;
40| 45940679| 261| 301| 1698| 51436799| 25415&lt;/p&gt;

&lt;p&gt;2.&lt;br/&gt;
(LocalFileSystem)Not bulk decompression in RCFile-&amp;gt;ValueBuffer-&amp;gt;readFileds, and the test adds some noisy between two RCFile reading, and after written to avoid disk cache.&lt;/p&gt;

&lt;p&gt;column number| RCFile size | RCFile read 1 column | RCFile read 2 column | RCFile read all columns |Sequence file size | sequence file read all&lt;br/&gt;
10| 11501112| 1804 | 3262 | 15956 | 13046020| 6927&lt;br/&gt;
25| 28725817| 1761 | 3310 | 39492 | 32246409| 15983&lt;br/&gt;
40| 45940679| 1843 | 3386 | 63759 | 51436799| 25256&lt;/p&gt;

&lt;p&gt;3.&lt;br/&gt;
(DistributedFileSystem)Use Bulk decompression in RCFile-&amp;gt;ValueBuffer-&amp;gt;readFileds, and adds some noisy between two RCFile reading, and after written to avoid disk cache.&lt;/p&gt;

&lt;p&gt;column number| RCFile size | RCFile read 1 column | RCFile read 2 column | RCFile read all columns |Sequence file size | sequence file read all&lt;/p&gt;

&lt;p&gt;10| 11501112| 2381| 3516| 9898| 13046020| 18053&lt;br/&gt;
25| 28725817| 3754| 5254| 22521| 32246409| 43258&lt;br/&gt;
40| 45940679| 5597| 8225| 40304| 51436799| 69278&lt;/p&gt;</comment>
                            <comment id="12701829" author="zshao" created="Thu, 23 Apr 2009 07:45:55 +0000"  >&lt;p&gt;The numbers look much reasonable than before. 1.7s to read and decompress 46MB data is plausible. But the sequence file&apos;s speed - 25s to read and decompress 51MB data looks a bit too low.&lt;/p&gt;

&lt;p&gt;0. Did you try that with hadoop 0.17.0? &quot;ant -Dhadoop.version=0.17.0 test&quot; etc.&lt;/p&gt;

&lt;p&gt;1. Can you add your tests to ant, or post the testing scripts so that everybody can easily reproduce the test results that you have got?&lt;/p&gt;

&lt;p&gt;2. For DistributedFileSystem, how big is the cluster? Is the file (the file size is small so it&apos;s clearly a single block) local?&lt;/p&gt;

&lt;p&gt;3. It seems SequenceFile&apos;s compression is not as good as RCFile, although the data is the same and also random. What is the exact record format in sequencefile? Did you put delimitors or you put length of Strings?&lt;/p&gt;

&lt;p&gt;4. 40MB to 50MB is too small for testing. Let&apos;s double it to ~100MB but less than 128MB to simulate a single file system block.&lt;/p&gt;

&lt;p&gt;I think we should compare the following 2 approaches:&lt;br/&gt;
BULK. When creating file, store uncompressed data in memory, when limit reached, compress and write out; when reading file, do bulk decompression.  This won&apos;t go out of memory because decompressed size is bounded by the limit at the file creation;&lt;br/&gt;
NONBULK: When creating file, store compressed data in memory, when (compressed size) limit reached, compress and write out; when reading file, do small chunk decompression to make sure we don&apos;t go out of memory.&lt;/p&gt;

&lt;p&gt;The approach of store compressed data at creation, and do bulk decompression at reading is not practical because it&apos;s very easy to go out of memory.&lt;/p&gt;

&lt;p&gt;We&apos;ve done BULK, and it showed great performance (1.6s to read and decompress 40MB local file), but I suspect the compression ratio will be lower than NONBULK.&lt;br/&gt;
Can you compare the compression ratio of BULK and NONBULK, given different buffer sizes and column numbers?&lt;br/&gt;
Also, with NONBULK, we might be able to get bigger compressed blocks, so that for each skip we can skip more than BULK, but this is just a minor issue I think.&lt;/p&gt;

&lt;p&gt;If the compression ratio didn&apos;t turn out to be too different, we may just go the BULK approach.&lt;/p&gt;</comment>
                            <comment id="12701837" author="he yongqiang" created="Thu, 23 Apr 2009 08:12:24 +0000"  >&lt;p&gt;Thanks, Zheng.&lt;br/&gt;
&amp;gt;&amp;gt;0. Did you try that with hadoop 0.17.0? &quot;ant -Dhadoop.version=0.17.0 test&quot; etc.&lt;br/&gt;
yes.&lt;br/&gt;
&amp;gt;&amp;gt;1. Can you add your tests to ant, or post the testing scripts so that everybody can easily reproduce the test results that you have got?&lt;br/&gt;
I will do that with next patch&lt;br/&gt;
&amp;gt;&amp;gt;2. For DistributedFileSystem, how big is the cluster? Is the file (the file size is small so it&apos;s clearly a single block) local?&lt;br/&gt;
The cluster is of six nodes. The file is not local. The test was run on my local machine, and use HDFS.&lt;br/&gt;
&amp;gt;&amp;gt;3. It seems SequenceFile&apos;s compression is not as good as RCFile, although the data is the same and also random. What is the exact record format in sequencefile? Did you &amp;gt;&amp;gt;put delimitors or you put length of Strings?&lt;br/&gt;
yes, it has length of Strings.However, RCFile also has the length of strings&lt;br/&gt;
&amp;gt;&amp;gt;The approach of store compressed data at creation, and do bulk decompression at reading is not practical because it&apos;s very easy to go out of memory.&lt;br/&gt;
Yes, I encountered Out of memory error. So i added some trick in RCFile.Writer&apos;s append. Like&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;if ((columnBufferSize + (this.bufferedRecords * this.columnNumber * 2) &amp;gt; COLUMNS_BUFFER_SIZE)
          || (this.bufferedRecords &amp;gt;= this.RECORD_INTERVAL)) {
        flushRecords();
      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&amp;gt;&amp;gt;We&apos;ve done BULK, and it showed great performance (1.6s to read and decompress 40MB local file), but I suspect the compression ratio will be lower than NONBULK.&lt;br/&gt;
&amp;gt;&amp;gt;Can you compare the compression ratio of BULK and NONBULK, given different buffer sizes and column numbers?&lt;br/&gt;
BULK and NONBULK( they mean decompress) are only for Read, they have nothing to do with Write, so I guess it will not influence compression ratio.&lt;/p&gt;</comment>
                            <comment id="12701839" author="zshao" created="Thu, 23 Apr 2009 08:18:10 +0000"  >&lt;p&gt;@Yongqiang: I found a place in the SequenceFile reader test that may improve the performance a lot - BytesRefWritable.readFields is creating a new array for each row!! This is bad and I would say this is not a fair comparison between RCFile and SequenceFile.&lt;/p&gt;

&lt;p&gt;There are 3 ways to fix BytesRefWritable:&lt;br/&gt;
1. Add a boolean member &quot;owned&quot;, set it to true every time we create an array in readFields, and don&apos;t create another array if owned is true and the current record is equal or smaller than the current owned array. Also, set it to false every time set(...) is called.&lt;br/&gt;
2. Directly change the semantics of readFields - we always reuse the bytes array if length of bytes array is equal or greater to the current record, otherwise create a new one. This is OK because for people who uses set(...) they probably won&apos;t use readFields at all. Of course, we need to put a comment at readFields and set() says readFields will corrupt the array, so don&apos;t call readFields.&lt;br/&gt;
3. Use a completely different class hierarchy.&lt;/p&gt;

&lt;p&gt;I would prefer to do 2 since it&apos;s the simplest way to go.&lt;/p&gt;

&lt;p&gt;I hope this will improve the sequencefile read performance a lot, and give RCFile and SeqFile a fair comparison.&lt;/p&gt;


&lt;p&gt;Also, you might want to modify the write code to use the same logic - reuse the bytes array if possible. Then the writes will be much faster as well.&lt;/p&gt;</comment>
                            <comment id="12701840" author="zshao" created="Thu, 23 Apr 2009 08:21:26 +0000"  >&lt;p&gt;&amp;gt;&amp;gt; Yes, I encountered Out of memory error. So i added some trick in RCFile.Writer&apos;s append. Like &lt;br/&gt;
Can you explain what the formula means?&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt; BULK and NONBULK( they mean decompress) are only for Read, they have nothing to do with Write, so I guess it will not influence compression ratio.&lt;br/&gt;
I mean the BULK and NONBULK in my comments - they have different ways to determine when to flush the records at write as well, so the compression ratio will be different.&lt;/p&gt;</comment>
                            <comment id="12701843" author="he yongqiang" created="Thu, 23 Apr 2009 08:29:40 +0000"  >&lt;p&gt;More explaination for the fomular used: &lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;(columnBufferSize + (this.bufferedRecords * this.columnNumber * 2) &amp;gt; COLUMNS_BUFFER_SIZE)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;columnBufferSize(compressed size) refers to number of bytes all columns value data occupied.&lt;br/&gt;
(this.bufferedRecords * this.columnNumber * 2) &amp;gt; COLUMNS_BUFFER_SIZE) is an approximate of bytes number of Key part. We record each value&apos;s length in the key part.&lt;br/&gt;
So the number of records in a value buffer can not be too much large.&lt;/p&gt;</comment>
                            <comment id="12701876" author="zshao" created="Thu, 23 Apr 2009 10:07:16 +0000"  >&lt;p&gt;Running Yongqiang&apos;s tests with hadoop native library.The file is on local file system. Each column is a random string length uniformly from 0 to 30, containing random uppercase and lowercase alphabets.&lt;/p&gt;

&lt;p&gt;Using DefaultCodec &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Write RCFile with 80 random string columns and 100000 rows cost 25464 milliseconds. And the file&apos;s on disk size is 91874941
Write SequenceFile with 80 random string columns and 100000 rows cost 35711 milliseconds. And the file&apos;s on disk size is 102521005
Read only one column of a RCFile with 80 random string columns and 100000 rows cost 594 milliseconds.
Read only first and last columns of a RCFile with 80 random string columns and 100000 rows cost 600 milliseconds.
Read all columns of a RCFile with 80 random string columns and 100000 rows cost 2227 milliseconds.
Read SequenceFile with 80  random string columns and 100000 rows cost 4343 milliseconds.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Using GzipCodec. Not much difference.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Write RCFile with 80 random string columns and 100000 rows cost 26358 milliseconds. And the file&apos;s on disk size is 91931563
Write SequenceFile with 80 random string columns and 100000 rows cost 35802 milliseconds. And the file&apos;s on disk size is 102528154
Read only one column of a RCFile with 80 random string columns and 100000 rows cost 593 milliseconds.
Read only first and last columns of a RCFile with 80 random string columns and 100000 rows cost 626 milliseconds.
Read all columns of a RCFile with 80 random string columns and 100000 rows cost 2401 milliseconds.
Read SequenceFile with 80  random string columns and 100000 rows cost 4601 milliseconds.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These result look reasonable.  RCFile&apos;s read performance is around 2 times of that of SequenceFiles, probably because we do bulk decompression and one less copy of data.&lt;/p&gt;</comment>
                            <comment id="12701930" author="athusoo" created="Thu, 23 Apr 2009 13:30:18 +0000"  >&lt;p&gt;Can we also get some numbers on the amount of memory usage?&lt;/p&gt;

&lt;p&gt;Also can you give more details about the experiment. Was this just a hdfs read or the measurement of a Hive query?&lt;/p&gt;</comment>
                            <comment id="12702146" author="he yongqiang" created="Thu, 23 Apr 2009 22:39:40 +0000"  >&lt;p&gt;&amp;gt;&amp;gt;Can we also get some numbers on the amount of memory usage? &lt;br/&gt;
I rerun the test(the same test as Zheng&apos;s,but with no native codec) in my local using local fs and DefaultCodec, and it read all columns of a rc file with 80 columns and 100000 rows(size:91849881 Bytes).&lt;br/&gt;
And the maximum memory usages is shown below( i do couple of command &apos;ps -o vsz,rss,rsz,%mem -p 549&apos; every minute),&lt;br/&gt;
     VSZ    RSS    RSZ %MEM&lt;br/&gt;
  766732  63472  63472 -3.0&lt;br/&gt;
BTW, my physical memory is 3GB.&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;Was this just a hdfs read or the measurement of a Hive query?&lt;br/&gt;
The test was just a file read test.&lt;/p&gt;

&lt;p&gt;However, with no native codec and my results shows a much diff from Zheng&apos;s in that SequenceFile does much worse in my test.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Write RCFile with 80 random string columns and 100000 rows cost 30643 milliseconds. And the file&apos;s on disk size is 91849881
Write SequenceFile with 80 random string columns and 100000 rows cost 62034 milliseconds. And the file&apos;s on disk size is 102521005
Read only one column of a RCFile with 80 random string columns and 100000 rows cost 703 milliseconds.
Read only first and last columns of a RCFile with 80 random string columns and 100000 rows cost 526 milliseconds.
Read all columns of a RCFile with 80 random string columns and 100000 rows cost 3131 milliseconds.
Read SequenceFile with 80  random string columns and 100000 rows cost 47876 milliseconds.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Why native codec matters so much for sequece file and not for RCFile? It should influence both RCFile and SequenceFile in the same way.&lt;/p&gt;</comment>
                            <comment id="12702267" author="zshao" created="Fri, 24 Apr 2009 07:16:35 +0000"  >&lt;p&gt;@Yongqiang,&lt;/p&gt;

&lt;p&gt;The reason that native codec matters more for SequenceFile is probably because seqfile is using compression differently from rcfile, for example, incremental compression/decompression.&lt;/p&gt;

&lt;p&gt;I had a test on our data set which mainly contains around 40 columns of string, the length of string is usually fixed for that column, from length 1 to 10. The result is that seqfile is much smaller than rcfile - seqfile is only around 55% of the size of rcfile. However inside the rcfile I see a lot of repeated bytes - that&apos;s the length of the field for each row.  Also rcfile is slower probably because it&apos;s writing out more data than seqfile.&lt;/p&gt;

&lt;p&gt;1. Can you also compress the field length columns? I tried to compress the rcfile again using gzip command line, and it becomes 41% of the current size - this is a lot smaller than the seqfile, which means in general, RCfile can save a lot of space because it&apos;s easier for compression algorithm to compress the length and the content of each column separately.&lt;/p&gt;

&lt;p&gt;2. Also, I remember you changed the the compression to be incremental, so the current solution is a mix of BULK and NONBULK as I described above. which has memory problems  Since as we discussed we would like to leave the NONBULK mode for later because of the amount of additional work, can you change the code back to BULK compression?  There is probably a performance loss due to incremental compression, which can be avoided by bulk compression.&lt;/p&gt;</comment>
                            <comment id="12703086" author="he yongqiang" created="Mon, 27 Apr 2009 11:50:01 +0000"  >&lt;p&gt;hive-352-2009-4-27.patch changed back to bulk compression and now also compress the key part.&lt;/p&gt;

&lt;p&gt;Here is a result on TPCH&apos;s lineitem:&lt;br/&gt;
Direct(incremental) compression, and does not compress key part:&lt;br/&gt;
274982705   hdfs://10.61.0.160:9000/user/hdfs/tpch1G_rc&lt;br/&gt;
First Buffered then compress(Bulk Compression), and compress key part:&lt;br/&gt;
188401365   hdfs://10.61.0.160:9000/user/hdfs/tpch1G_newRC&lt;/p&gt;


&lt;p&gt;BTW, I also tried to implement direct(incremental) compression, and tried to decompress a value buffer&apos;s columns part by part. But at the last step( when implementing ValueBuffer&apos;s readFields), i noticed that it is not very easy to implement it. Because we only hold on InputStream to the underlying file, and we need to seek back and forth to decompress part of each columns, and also we need to hold one decompress stream for each column. If we seek the inputstream, the decompress stream is corrupt. &lt;br/&gt;
To avoid all these, we need to read all needed columns&apos; compressed data into memory, and do in memory decompress. But we stil need one decompress stream for each column. I stop implementing this at the last step, if it is needed i can finish it.&lt;/p&gt;</comment>
                            <comment id="12703606" author="zshao" created="Tue, 28 Apr 2009 11:38:59 +0000"  >&lt;p&gt;Nice work Yongqiang!&lt;/p&gt;

&lt;p&gt;I totally agree with your analysis of the NONBULK mode - I was thinking of caching all compressed data in memory.&lt;/p&gt;

&lt;p&gt;The good thing about NONBULK is that we might be able to bypass the column hinting - because we might be able to omit the disk read cost since the decompression cost usually takes much more time.&lt;/p&gt;

&lt;p&gt;Can you add a simple option to your code so that I can easily test out whether that&apos;s true? Basically, I want to compare the difference of reading and decompressing 1 column (while skipping all other columns), and reading all columns (while decompressing only 1 column). If the difference is small, then we can implement NONBULK, and skip the column hinting implementation for now.&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;</comment>
                            <comment id="12703616" author="zshao" created="Tue, 28 Apr 2009 12:04:49 +0000"  >&lt;p&gt;Good news: A test on some of our internal data shows that the column-based storage is saving 20%+ space for us. However it is about 1.5x slower than seqfile in writing. Not sure why yet. Will do more profiling tomorrow.&lt;/p&gt;</comment>
                            <comment id="12703647" author="athusoo" created="Tue, 28 Apr 2009 14:02:55 +0000"  >&lt;p&gt;That is very encouraging from the storage perspective. However 1.5x is a bit concerning. Profile output would be awesome to understand this.&lt;/p&gt;</comment>
                            <comment id="12703649" author="jsensarma" created="Tue, 28 Apr 2009 14:06:21 +0000"  >&lt;p&gt;how does read performance look like?&lt;/p&gt;</comment>
                            <comment id="12703980" author="he yongqiang" created="Wed, 29 Apr 2009 03:51:24 +0000"  >&lt;p&gt;Zheng, can you post your profiling results?&lt;br/&gt;
I did a test with PerformTestRCFileAndSeqFile, and it seems RCFile does better in both reading and writing. Can you also does a test with PerformTestRCFileAndSeqFile? &lt;br/&gt;
I am refactoring BytesRefArrayWritable today. BytesRefArrayWritable is a public class, and oncethe number of BytesRefWritable it holds is changed, currently we have to recreate a new list for that. And i am trying to avoid recreating a new list by able to narrow or enlarge the underlying list. It needs more work than I initially thought.&lt;/p&gt;</comment>
                            <comment id="12703993" author="zshao" created="Wed, 29 Apr 2009 05:39:33 +0000"  >&lt;p&gt;The following numbers are all for 128MB gzip compressed block (for seqfile, and 20% smaller for rcfile because of difference compression ratio)&lt;br/&gt;
A. Read from seqfile + Write to seqfile: 2m 05s&lt;br/&gt;
B. Read from seqfile + Write to rcfile: 2m 45s&lt;br/&gt;
C. Read from rcfile + Write to seqfile: 2m 20s&lt;br/&gt;
D. Read from rcfile + Write to rcfile: 3m 00s&lt;/p&gt;

&lt;p&gt;@Joydeep: The good compression ratio is mainly because we are compressing column length and column data (without delimiters) separately.  In an earlier experiment I did, column-based compression only showed 7-8% improvements because I was compressing column data with delimiters.&lt;/p&gt;

&lt;p&gt;@Yongqiang: Did you turn on native compression when testing?&lt;/p&gt;

&lt;p&gt;Some performance improvement tips from the profiling:&lt;br/&gt;
1. BytesRefArrayWritable to use Java Array (BytesRefWritable[]) instead of List&amp;lt;BytesRefWritable&amp;gt;&lt;br/&gt;
2. RCFile$Writer.columnBuffers to use Java Array(ColumnBuffer[]) instead of List&amp;lt;ColumnBuffer&amp;gt;&lt;br/&gt;
3. Add a method in BytesRefArrayWritable to return the BytesRefWritable[] so that RCFile$Writer.append can operator on it directly.&lt;br/&gt;
1-3 will save us 10-15 seconds from B and D.&lt;br/&gt;
4. RCFIle$Writer$ColumnBuffer.append should directly call DataOutputStream.write and WritableUtils.writeVLong&lt;br/&gt;
     public void append(BytesRefWritable data) throws IOException &lt;/p&gt;
{
        data.writeDataTo(columnValBuffer);
        WritableUtils.writeVInt(valLenBuffer, data.getLength());
      }
&lt;p&gt;4 will save 5-10 seconds from B and D.&lt;/p&gt;

&lt;p&gt;Following the same route, if there are any Lists that the number of elements do not usually change, we should use Java Array ([]) instead of List.&lt;/p&gt;

&lt;p&gt;Yongqiang, can you do step 1-4 and try to replace List with Array?&lt;/p&gt;</comment>
                            <comment id="12704005" author="zshao" created="Wed, 29 Apr 2009 06:27:27 +0000"  >&lt;p&gt;BTW, for my previous comment, all times measured are for reading all columns (about 30 in total) out from the table. RCFile might have much better read performance for a single column, but we have not integrated RCFile column pruning with Hive yet.&lt;/p&gt;</comment>
                            <comment id="12704238" author="he yongqiang" created="Wed, 29 Apr 2009 18:32:51 +0000"  >&lt;p&gt;hive-352-2009-4-30-2.patch changs all lists to java arrays,&lt;br/&gt;
1), 2) and 3) are done. If we change 4), it will needs extra func call. &lt;br/&gt;
The attached patch also changes some fields in the key part to from vint to int (decode not-one-byte vint will needs extra work).&lt;/p&gt;</comment>
                            <comment id="12704329" author="zshao" created="Wed, 29 Apr 2009 20:47:28 +0000"  >&lt;p&gt;hive-352-2009-4-30-2.patch&lt;br/&gt;
1. It seems you compiled against hadoop 0.19? Can you do that with hadoop 0.17? Otherwise it won&apos;t compile at my workspace.&lt;br/&gt;
2. I think vint is actually pretty good for saving space. I would prefer to have vint for key length instead of int, but if you can do an experiment to show that vint and int has similar size after compression, then I am also OK with int.&lt;/p&gt;</comment>
                            <comment id="12704361" author="he yongqiang" created="Wed, 29 Apr 2009 21:53:12 +0000"  >&lt;p&gt;compiled with hadoop-0.17&lt;/p&gt;</comment>
                            <comment id="12704365" author="he yongqiang" created="Wed, 29 Apr 2009 21:57:09 +0000"  >&lt;p&gt;Sorry for the wrong file attached earlier. &lt;br/&gt;
hive-352-2009-4-30-3.patch can be compiled with hadoop-0.17.0. hive-352-2009-4-30-3.patch use vint for some fields of key part. I tested with vint and int, and can not see their differences in my test.&lt;/p&gt;</comment>
                            <comment id="12704397" author="zshao" created="Wed, 29 Apr 2009 23:02:15 +0000"  >&lt;p&gt;hive-352-2009-4-30-3.patch&lt;br/&gt;
It seems there is a bug - only 10 columns are saved. Please have a try.&lt;/p&gt;</comment>
                            <comment id="12704497" author="he yongqiang" created="Thu, 30 Apr 2009 06:32:09 +0000"  >&lt;p&gt;hive-352-2009-4-30-4.patch does some refactor work to BytesRefArrayWritable. Maybe the problem is caused by it. I did not see the problem. please make sure BytesRefArrayWritable can work well since it is a public class. Because many tests are integrated in TestRCFile and PerformTestRCFileAndSeqFile, so i does not include seperate tests for BytesRefArrayWritable, ColumnarSerDe. If they should be provided, i can add them.&lt;/p&gt;

&lt;p&gt;Also hive-352-2009-4-30-4.patch added a short cut for creating table with &quot;STORED AS RCFILE&quot; and includes a test .q file &quot;columnarserde_create_shortcut.q&quot;. &lt;/p&gt;</comment>
                            <comment id="12704617" author="zshao" created="Thu, 30 Apr 2009 12:39:07 +0000"  >&lt;p&gt;hive-352-2009-4-30-4.patch:&lt;/p&gt;

&lt;p&gt;Thanks Yongqiang. I tried it and it works now.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;exec/Utilities.java:createRCFileWriter:&lt;br/&gt;
parameter fs never used.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;RCFile.java:&lt;br/&gt;
Can you add some comments in the header to explain how you determine &quot;row split&quot;?&lt;br/&gt;
What does this mean?  &amp;lt;li&amp;gt;A sync-marker every few &amp;lt;code&amp;gt;100&amp;lt;/code&amp;gt; bytes or so.&amp;lt;/li&amp;gt;&lt;br/&gt;
KeyBuffer.write(DataInput in, int length): length never used. Is this method ever used? If not, remove it.&lt;br/&gt;
ColumnBuffer.getLength(): Let&apos;s remove the synchronized keyword for the sake of performance.&lt;br/&gt;
ColumnBuffer.ColumnBuffer(Class valClass): Let&apos;s remove valClass since it&apos;s never used.&lt;br/&gt;
Writer: how do you pass the column number from Hive to the configuration and then to the RCFIle.Writer?&lt;br/&gt;
+      this.columnNumber = conf.getInt(COLUMN_NUMBER_CONF_STR, 0);&lt;br/&gt;
Writer: Per Java convention, COLUMNS_BUFFER_SIZE should be columnsBufferSize.&lt;br/&gt;
+      this.COLUMNS_BUFFER_SIZE = conf.getInt(COLUMNS_BUFFER_SIZE_CONF_STR,&lt;br/&gt;
+          4 * 1024 * 1024);&lt;br/&gt;
Reader: add javadoc for &quot;public synchronized boolean nextColumnsBatch()&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;RCFileRecordReader.java&lt;br/&gt;
Remove &quot;synchronized&quot; in all methods for the sake of performance&lt;br/&gt;
+  protected synchronized boolean next(LongWritable key) throws IOException 
{

* TestRCFile.java
You might want to use Text.encode() instead of &quot;String&quot;.getBytes(). At least we should use getBytes(&quot;UTF-8&quot;).

* ql/src/test/queries/clientpositive/columnarserde_create_shortcut.q
For all tests, please drop the created tables again at the end of the .q file. This helps to make &quot;show tables&quot; in other .q files return a deterministic result.

* BytesRefArrayWritable.java
What&apos;s the difference between get() and unCheckedGet()? The code is the same
BytesRefArrayWritable.resetValid: Let&apos;s call it resize. 
set(..): should be valid &amp;lt;= index
+    if (valid &amp;lt; index)
+      valid = index + 1;

* BytesRefWritable.java
Shall we rename getBytes() to getBytesCopy()?

* ColumnarStruct.java
init(...): Cleaning out the object and recreate LazyObject is not efficient.
{code}
&lt;p&gt;Current:&lt;br/&gt;
+      if (field.length &amp;gt; 0) &lt;/p&gt;
{
+        if (fields[fieldIndex] == null)
+          fields[fieldIndex] = LazyFactory.createLazyObject(fieldTypeInfos
+              .get(fieldIndex));
+        fields[fieldIndex].init(cachedByteArrayRef[fieldIndex], field
+            .getStart(), field.getLength());
+      }
&lt;p&gt; else if (fields&lt;span class=&quot;error&quot;&gt;&amp;#91;fieldIndex&amp;#93;&lt;/span&gt; != null&lt;br/&gt;
+          &amp;amp;&amp;amp; fields&lt;span class=&quot;error&quot;&gt;&amp;#91;fieldIndex&amp;#93;&lt;/span&gt;.getObject() != null) &lt;/p&gt;
{
+        fields[fieldIndex] = null;
+      }
&lt;p&gt;Let&apos;s change it to (Please have a test)&lt;br/&gt;
+      if (fields&lt;span class=&quot;error&quot;&gt;&amp;#91;fieldIndex&amp;#93;&lt;/span&gt; == null)&lt;br/&gt;
+        fields&lt;span class=&quot;error&quot;&gt;&amp;#91;fieldIndex&amp;#93;&lt;/span&gt; = LazyFactory.createLazyObject(fieldTypeInfos&lt;br/&gt;
+            .get(fieldIndex));&lt;br/&gt;
+      fields&lt;span class=&quot;error&quot;&gt;&amp;#91;fieldIndex&amp;#93;&lt;/span&gt;.init(cachedByteArrayRef&lt;span class=&quot;error&quot;&gt;&amp;#91;fieldIndex&amp;#93;&lt;/span&gt;, field&lt;br/&gt;
+          .getStart(), field.getLength());&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
* Can you add one more test &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; adding/deleting columns? The value of the newly added columns should be NULL. RCFile should also be able to ignore the extra columns from the data.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;CREAT TABLE xxx  (a string, b string) STORED AS RCFILE;&lt;br/&gt;
INSERT OVERWRITE TABLE xxx ...;&lt;br/&gt;
ALTER TABLE xxx ADD COLUMNS (c string);&lt;br/&gt;
SELECT * FROM xxx;&lt;br/&gt;
ALTER TABLE xxx REPLACE COLUMNS (a string);&lt;br/&gt;
SELECT * FROM xxx;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
* General comments:
1 We don&apos;t need to put &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.&quot;&lt;/span&gt; in the code unless there is a local &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; with the same name.
2 Please add more javadoc - the general rule &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Hive is that every &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class/&lt;/span&gt;method should have javadoc, except getters and setters.

** Very important: Eclipse automatically generates a lot of empty javadocs &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; variables etc. Please remove them &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; you don&apos;t have a comment &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; that variable, otherwise &lt;span class=&quot;code-quote&quot;&gt;&quot;ant -Dhadoop.version=0.17.0 javadoc&quot;&lt;/span&gt; will have warnings. Please make sure to remove all warnings.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefWritable.java:95: warning - @return tag has no arguments.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefWritable.java:122: warning - Tag @see: missing &apos;#&apos;: &quot;set(byte[] newData, int offset, int length)&quot;&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/serde/src/java/org/apache/hadoop/hive/serde2/columnar/BytesRefWritable.java:105: warning - Tag @see: missing &apos;#&apos;: &quot;readFields(DataInput in)&quot;&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarStruct.java:38: warning - Tag @link: missing &apos;#&apos;: &quot;init(BytesRefArrayWritable cols)&quot;&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:522: warning - @param argument &quot;keyClass&quot; is not a parameter name.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:522: warning - @param argument &quot;valClass&quot; is not a parameter name.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:149: warning - Tag @see: malformed: &quot;CompressionCodec, SequenceFile&quot;&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:149: warning - Tag @see: reference not found: CompressionCodec, SequenceFile&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:1214: warning - @return tag has no arguments.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:1214: warning - Tag @link: missing &apos;#&apos;: &quot;nextColumnBatch()&quot;&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:1214: warning - Tag @link: can&apos;t find nextColumnBatch() in org.apache.hadoop.hive.ql.io.RCFile.Reader&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:1288: warning - @return tag has no arguments.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:1288: warning - @return tag cannot be used in method with void return type.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:543: warning - @param argument &quot;keyClass&quot; is not a parameter name.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:543: warning - @param argument &quot;valClass&quot; is not a parameter name.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:564: warning - @param argument &quot;keyClass&quot; is not a parameter name.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:564: warning - @param argument &quot;valClass&quot; is not a parameter name.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:591: warning - @param argument &quot;keyClass&quot; is not a parameter name.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:591: warning - @param argument &quot;valClass&quot; is not a parameter name.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFileOutputFormat.java:105: warning - @return tag has no arguments.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/io/RCFileOutputFormat.java:105: warning - @param argument &quot;tableInfo&quot; is not a parameter name.&lt;br/&gt;
  &lt;span class=&quot;error&quot;&gt;&amp;#91;javadoc&amp;#93;&lt;/span&gt; /data/users/zshao/tools/352-trunk-apache-hive/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:745: warning - @param argument &quot;dest&quot; is not a parameter name.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12704751" author="he yongqiang" created="Thu, 30 Apr 2009 19:10:29 +0000"  >&lt;p&gt;Thanks for the detailed review and the improvement instructions, Zheng.&lt;/p&gt;

&lt;p&gt;hive-352-2009-5-1.patch does these improvements according to Zheng&apos;s suggestions.&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;Writer: how do you pass the column number from Hive to the configuration and then to the RCFIle.Writer?&lt;br/&gt;
The code is in RCFileOutputFormat&apos;s getHiveRecordWriter(). It tries to parse the columns from passed in Properties.&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;init(...): Cleaning out the object and recreate LazyObject is not efficient. &lt;br/&gt;
If we change it, it will not pass the TestRCFile test. The final extra else if  statements are rarely reached, and when reached, most time it only needs one instruction to determine whether fields&lt;span class=&quot;error&quot;&gt;&amp;#91;fieldIndex&amp;#93;&lt;/span&gt; is null.&lt;/p&gt;

&lt;p&gt;Other suggestions are all done. Thanks!!&lt;/p&gt;</comment>
                            <comment id="12704765" author="zshao" created="Thu, 30 Apr 2009 19:52:49 +0000"  >&lt;p&gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;Writer: how do you pass the column number from Hive to the configuration and then to the RCFIle.Writer?&lt;br/&gt;
&amp;gt;&amp;gt;The code is in RCFileOutputFormat&apos;s getHiveRecordWriter(). It tries to parse the columns from passed in Properties.&lt;br/&gt;
Thanks. I understand it now.&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;init(...): Cleaning out the object and recreate LazyObject is not efficient.&lt;br/&gt;
&amp;gt;&amp;gt;If we change it, it will not pass the TestRCFile test. The final extra else if statements are rarely reached, and when reached, most time it only needs one instruction to determine whether fields&lt;span class=&quot;error&quot;&gt;&amp;#91;fieldIndex&amp;#93;&lt;/span&gt; is null.&lt;/p&gt;

&lt;p&gt;Can you add a boolean[] fieldIsNull to mark whether a field is null, instead of throwing away and recreating the LazyObject?&lt;br/&gt;
Then getField can check fieldIsNull to decide whether to return null or the LazyObject.&lt;/p&gt;</comment>
                            <comment id="12704789" author="he yongqiang" created="Thu, 30 Apr 2009 20:50:30 +0000"  >&lt;p&gt;Fixes a small mistake in previous file, and adds in a boolean array fieldIsNull in ColumnarStruct. &lt;/p&gt;</comment>
                            <comment id="12704807" author="zshao" created="Thu, 30 Apr 2009 21:50:54 +0000"  >&lt;p&gt;hive-352-2009-5-1-3.patch&lt;/p&gt;

&lt;p&gt;Can you remove the extra message &quot;FileSplit&apos;s start is 0, its length is 299&quot;?&lt;br/&gt;
Or use LOG.info/LOG.debug.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;hive&amp;gt; select * from zshao_rc;
OK
FileSplit&apos;s start is 0, its length is 299
123     456     NULL
Time taken: 0.09 seconds
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Can you find the error messsage in the code, and fix it?&lt;br/&gt;
You probably just need to add your ColumnarSerDe to the internal SerDe list.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;hive&amp;gt; alter table zshao_rc replace columns(a &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;);
Replace columns is not supported &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; table. SerDe may be incompatible.
FAILED: Execution Error, &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Can you allow extra columns in the metadata? Just assign NULLs to the columns in the metadata but NOT in the data.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;hive&amp;gt; alter table zshao_rc add columns(a &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;);
Column &lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt; exists
FAILED: Execution Error, &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
hive&amp;gt; alter table zshao_rc add columns(d &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;);
hive&amp;gt; select * from zshao_rc;
FileSplit&apos;s start is 0, its length is 299
Failed with exception This BytesRefArrayWritable only has 3 valid values.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12704902" author="zshao" created="Fri, 1 May 2009 06:35:56 +0000"  >&lt;p&gt;Corrected a few javadoc warnings and remove one debug message.&lt;br/&gt;
Committed to trunk. Thanks for the hard work Yongqiang He!&lt;/p&gt;</comment>
                            <comment id="12706103" author="athusoo" created="Tue, 5 May 2009 17:01:59 +0000"  >&lt;p&gt;Very cool contribution Yongqiang!!&lt;/p&gt;</comment>
                            <comment id="12711054" author="zshao" created="Wed, 20 May 2009 08:15:20 +0000"  >&lt;p&gt;Some more test results using some real log data. I also tried 2 levels of gzip by adding a new GzipCodec1 (level 1). GzipCodec is taking the default which should be level 6.&lt;br/&gt;
Gzip compression levels have a big impact on both the running time and compressed size:&lt;/p&gt;

&lt;p&gt;The time and output_file_size shown is the average map task running time and output_file_size. I&apos;ve changed mapred.min.split.size to make sure a single mapper is always processing the same data in different tests.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;InputFileFormat -&amp;gt; OutputFileFormat: time output_file_size
Seqfile GZIP 6 -&amp;gt; Seqfile GZIP 1: 1&lt;span class=&quot;code-quote&quot;&gt;&apos;25&apos;&lt;/span&gt;&apos; 182MB
Seqfile GZIP 1 -&amp;gt; Seqfile GZIP 6: 2&lt;span class=&quot;code-quote&quot;&gt;&apos;05&apos;&lt;/span&gt;&apos; 134MB

set hive.io.rcfile.record.buffer.size=4194304;
Rcfile GZIP 6 -&amp;gt; Rcfile GZIP 6: 2&lt;span class=&quot;code-quote&quot;&gt;&apos;50&apos;&lt;/span&gt;&apos; 104MB
Rcfile GZIP 6 -&amp;gt; Rcfile GZIP 1: 1&lt;span class=&quot;code-quote&quot;&gt;&apos;55&apos;&lt;/span&gt;&apos; 130MB
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From this, Rcfile GZIP 1 beats Seqfile GZIP 6 in both time and space: 8% less time and 3% less space.&lt;br/&gt;
I believe there are still time performance potentials in Rcfile (removing synchronized methods, etc) that we can exploit.&lt;/p&gt;

&lt;p&gt;These results shows that Rcfile can still beat Seqfile even if we select all fields.&lt;br/&gt;
If we only select a subset of the fields, then Rcfile will definitely beat Seqfile badly after &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-460&quot; title=&quot;Improve ColumnPruner to prune more aggressively and keep column information for input tables&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-460&quot;&gt;&lt;del&gt;HIVE-460&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-461&quot; title=&quot;Optimize RCFile reading by using column pruning results&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-461&quot;&gt;&lt;del&gt;HIVE-461&lt;/del&gt;&lt;/a&gt; are in.&lt;/p&gt;



&lt;p&gt;A local test with the same data (uncompressed text, about 910MB) shows GZIP 1 compression takes about half time of GZIP 6 compression .&lt;br/&gt;
The compression/decompression is using the command line utility gzip and gunzip.&lt;br/&gt;
Note that I&apos;ve warmed up the cache, and the disk reading time is less than 1&apos;&apos; (by doing cat file &amp;gt; /dev/null)&lt;br/&gt;
All times reported are wall time, but user time is within 1&apos;&apos;-2&apos;&apos; difference.&lt;br/&gt;
CPU is dual AMD Opteron 270 (2 x 2 core at 2GHz)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;GZIP 1 compression: 22&lt;span class=&quot;code-quote&quot;&gt;&apos;&apos; decompression: 7.3&apos;&lt;/span&gt;s 
GZIP 6 compression: 49&lt;span class=&quot;code-quote&quot;&gt;&apos;&apos; decompression: 6.4&apos;&lt;/span&gt;s
time wc uncompressed: 9.3&apos;&apos;
time awk &lt;span class=&quot;code-quote&quot;&gt;&apos;END {print NR;}&apos;&lt;/span&gt; uncompressed: 2.8&apos;&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These numbers are probably the lower bounds of the running time we can ever achieve.&lt;/p&gt;</comment>
                            <comment id="13432931" author="umamaheswararao" created="Mon, 13 Aug 2012 05:46:32 +0000"  >&lt;blockquote&gt;
&lt;p&gt;trying to specify or infer best compression technique per column much harder and something that can be done later&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Here we mentioned that, this improvement will be implemented later.&lt;br/&gt;
Could you please point me the right JIRA, where we started implementing this point or discuss?&lt;/p&gt;</comment>
                            <comment id="13433928" author="umamaheswararao" created="Tue, 14 Aug 2012 06:02:16 +0000"  >&lt;p&gt;Looks like &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-2097&quot; title=&quot;Explore mechanisms for better compression with RC Files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-2097&quot;&gt;HIVE-2097&lt;/a&gt; is talking about same stuff. &lt;br/&gt;
Linked that JIRA here as related.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12424250">HIVE-461</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12416592">HIVE-337</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12419283">HIVE-360</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12504731">AVRO-806</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12503539">HIVE-2097</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12424247">HIVE-460</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12433139">HIVE-756</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12406133" name="4-22 performace2.txt" size="8595" author="he yongqiang" created="Wed, 22 Apr 2009 12:44:41 +0000"/>
                            <attachment id="12406113" name="4-22 performance.txt" size="6277" author="he yongqiang" created="Wed, 22 Apr 2009 08:32:12 +0000"/>
                            <attachment id="12406112" name="4-22 progress.txt" size="2608" author="he yongqiang" created="Wed, 22 Apr 2009 08:32:12 +0000"/>
                            <attachment id="12403860" name="HIve-352-draft-2009-03-28.patch" size="51913" author="he yongqiang" created="Sat, 28 Mar 2009 05:22:31 +0000"/>
                            <attachment id="12404108" name="Hive-352-draft-2009-03-30.patch" size="61726" author="he yongqiang" created="Mon, 30 Mar 2009 06:50:41 +0000"/>
                            <attachment id="12405491" name="hive-352-2009-4-15.patch" size="129171" author="he yongqiang" created="Wed, 15 Apr 2009 04:18:24 +0000"/>
                            <attachment id="12405655" name="hive-352-2009-4-16.patch" size="108889" author="he yongqiang" created="Thu, 16 Apr 2009 14:35:02 +0000"/>
                            <attachment id="12405734" name="hive-352-2009-4-17.patch" size="111532" author="he yongqiang" created="Fri, 17 Apr 2009 06:28:08 +0000"/>
                            <attachment id="12405863" name="hive-352-2009-4-19.patch" size="131845" author="he yongqiang" created="Sun, 19 Apr 2009 07:43:07 +0000"/>
                            <attachment id="12406132" name="hive-352-2009-4-22-2.patch" size="141649" author="he yongqiang" created="Wed, 22 Apr 2009 12:44:41 +0000"/>
                            <attachment id="12406111" name="hive-352-2009-4-22.patch" size="140955" author="he yongqiang" created="Wed, 22 Apr 2009 08:32:12 +0000"/>
                            <attachment id="12406204" name="hive-352-2009-4-23.patch" size="141921" author="he yongqiang" created="Thu, 23 Apr 2009 09:04:02 +0000"/>
                            <attachment id="12406514" name="hive-352-2009-4-27.patch" size="145278" author="he yongqiang" created="Mon, 27 Apr 2009 11:50:01 +0000"/>
                            <attachment id="12406813" name="hive-352-2009-4-30-2.patch" size="149228" author="he yongqiang" created="Wed, 29 Apr 2009 18:32:51 +0000"/>
                            <attachment id="12406844" name="hive-352-2009-4-30-3.patch" size="149086" author="he yongqiang" created="Wed, 29 Apr 2009 21:57:09 +0000"/>
                            <attachment id="12406878" name="hive-352-2009-4-30-4.patch" size="159995" author="he yongqiang" created="Thu, 30 Apr 2009 06:32:09 +0000"/>
                            <attachment id="12406958" name="hive-352-2009-5-1-3.patch" size="162250" author="he yongqiang" created="Thu, 30 Apr 2009 20:50:30 +0000"/>
                            <attachment id="12406941" name="hive-352-2009-5-1.patch" size="158359" author="he yongqiang" created="Thu, 30 Apr 2009 19:10:29 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>18.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 17 Mar 2009 16:41:29 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>43010</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 24 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l9br:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122161</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-352&quot; title=&quot;Make Hive support column based storage&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-352&quot;&gt;&lt;strike&gt;HIVE-352&lt;/strike&gt;&lt;/a&gt;. Column-based storage format RCFile. (Yongqiang He via zshao)</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-353] Comments can&apos;t have semi-colons</title>
                <link>https://issues.apache.org/jira/browse/HIVE-353</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>
&lt;p&gt;hive&amp;gt; CREATE TABLE tmp_foo(foo DOUBLE COMMENT &apos;;&apos;);&lt;br/&gt;
FAILED: Parse Error: line 2:7 mismatched input &apos;TABLE&apos; expecting TEMPORARY in create function statement&lt;/p&gt;

&lt;p&gt;hive&amp;gt; CREATE TABLE tmp_foo(foo DOUBLE);        &lt;br/&gt;
OK&lt;/p&gt;</description>
                <environment></environment>
        <key id="12417104">HIVE-353</key>
            <summary>Comments can&apos;t have semi-colons</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="2">Won&apos;t Fix</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="sasmith">S. Alex Smith</reporter>
                        <labels>
                    </labels>
                <created>Tue, 17 Mar 2009 23:08:59 +0000</created>
                <updated>Thu, 17 Dec 2009 23:28:51 +0000</updated>
                            <resolved>Thu, 17 Dec 2009 23:28:51 +0000</resolved>
                                                                    <component>Query Processor</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12792219" author="zshao" created="Thu, 17 Dec 2009 23:28:39 +0000"  >&lt;p&gt;Please escape &quot;;&quot; by saying:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;hive&amp;gt; CREATE TABLE tmp_foo(foo DOUBLE COMMENT &lt;span class=&quot;code-quote&quot;&gt;&apos;\;&apos;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 17 Dec 2009 23:28:39 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73592</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 6 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l9bz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122162</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>
</channel>
</rss>
