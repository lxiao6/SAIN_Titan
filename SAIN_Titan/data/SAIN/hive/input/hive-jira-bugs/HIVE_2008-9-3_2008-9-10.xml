<!--
RSS generated by JIRA (7.6.3#76005-sha1:8a4e38d34af948780dbf52044e7aafb13a7cae58) at Tue Jan 22 15:11:57 UTC 2019

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<!-- If you wish to do custom client-side styling of RSS, uncomment this:
<?xml-stylesheet href="https://issues.apache.org/jira/styles/jiraxml2html.xsl" type="text/xsl"?>
-->
<rss version="0.92">
    <channel>
        <title>ASF JIRA</title>
        <link>https://issues.apache.org/jira/issues/?jql=project+%3D+HIVE+AND+created+%3E%3D+2008-9-3+AND+created+%3C%3D+2008-9-10+ORDER+BY+key+ASC</link>
        <description>An XML representation of a search request</description>
                <language>en-uk</language>
                        <issue start="0" end="7" total="7"/>
                <build-info>
            <version>7.6.3</version>
            <build-number>76005</build-number>
            <build-date>09-01-2018</build-date>
        </build-info>

<item>
            <title>[HIVE-4] internationalization support and sort order (ascedning/descending) support in create table</title>
                <link>https://issues.apache.org/jira/browse/HIVE-4</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;User cannot specify utf8 strings in the query, both for selection and filtering. Mysql syntax should be followed: &lt;/p&gt;

&lt;p&gt;select _utf8 &apos;string&apos; from &amp;lt;TableName&amp;gt;&lt;br/&gt;
select &amp;lt;selectExpr&amp;gt; from &amp;lt;TableName&amp;gt; where col = _utf8 0x&amp;lt;HexValue&amp;gt;&lt;/p&gt;


&lt;p&gt;To start with, utf8 strings should be supported. Support for other character sets can be added in the future on demand.&lt;/p&gt;

&lt;p&gt;The identifiers (table name/column name etc.) cannot be utf8 strings, it is only for the data values.&lt;/p&gt;

&lt;p&gt;Although, in create table, the user has the option of specifying sorted columns, he does not have the option of specifying whether they are ascending or descending.&lt;br/&gt;
Create Table syntax should be enhanced to support that.&lt;/p&gt;

</description>
                <environment></environment>
        <key id="12403771">HIVE-4</key>
            <summary>internationalization support and sort order (ascedning/descending) support in create table</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="namit">Namit Jain</reporter>
                        <labels>
                    </labels>
                <created>Fri, 5 Sep 2008 17:41:28 +0000</created>
                <updated>Sat, 17 Dec 2011 00:08:33 +0000</updated>
                            <resolved>Mon, 1 Dec 2008 17:01:11 +0000</resolved>
                                                    <fixVersion>0.3.0</fixVersion>
                                    <component>Metastore</component>
                    <component>Query Processor</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12628685" author="namit" created="Fri, 5 Sep 2008 18:10:21 +0000"  >&lt;p&gt;The change fixes the mentioned problems. Tests have been added.&lt;br/&gt;
Can someone from Hive please comment on the changes ?&lt;/p&gt;</comment>
                            <comment id="12628843" author="athusoo" created="Sat, 6 Sep 2008 07:00:59 +0000"  >&lt;p&gt;Comments are below. The most major one is about how we are treating character set name in the grammar. Ideally we would want this to an identifier instead of token (similar to table name identifiers). With that approach we would be able to support any kinds of character sets very easily.&lt;/p&gt;

&lt;p&gt;Inline Comments:&lt;br/&gt;
cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:85: nitpick - Can we follow the convention of having the opening brace on the same line as the code.&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g:781: Instead of having fixed tokens per character set in the grammar, we should define a character-set identifier and pass that across to the java calls. That is much more scalable and would get us to seamlessly be able to support any character sets supported by the java run time.&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;http://java.sun.com/j2se/1.4.2/docs/api/java/nio/charset/Charset.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://java.sun.com/j2se/1.4.2/docs/api/java/nio/charset/Charset.html&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;has information on what can be grammar rules to determine the character set name and how new charactersets can be added to the JVM by CharactersetProvider. &lt;br/&gt;
So the rule for the character set could look something like&lt;/p&gt;

&lt;p&gt; charSetStringLiteral : charSetIdentifier StringLiteral charSetIdentifier can be defined in terms of the rules mentioned in the link above.&lt;/p&gt;

&lt;p&gt;ql/src/test/queries/clientpositive/inputddl4.q:0: Lets put a brief comment in this describing what this actually tests.&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:157: nitpick - maybe we should call this PREFIX and not SAME&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:143: Should this not check across all sort columns instead of bucket columns? Is this a bug?&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:384: This function hardcodes the terminating character and the field delimiters while in the current code these are parameterized which is better as later we want to drive them through session level properties.&lt;/p&gt;</comment>
                            <comment id="12628928" author="namit" created="Sun, 7 Sep 2008 00:22:47 +0000"  >&lt;p&gt;I will take care of the above - but I was not sure of the charset names. Shouldn&apos;t we initially support only the 7-8 charsets &lt;br/&gt;
specified as must in CharSet and not let the user add any charset. Because, we will be using that encoding to convert&lt;br/&gt;
into a valid String later on. I agree that instead of hardcoding UTF-8, it should be something like &lt;/p&gt;

&lt;p&gt;charSetName:  can be a list of UTF-8, UNICODE etc..&lt;/p&gt;


&lt;p&gt;but we should not allow any identifier out there, because we have no way of knowing whether it is a valid encoding or not, and&lt;br/&gt;
we will be forced to throw errors at the Semantic Analyzer . Are you proposing that - instead of catching bad character set names &lt;br/&gt;
at the parser, throw an error in the Semnatic Analyzer if is not valid. &lt;/p&gt;</comment>
                            <comment id="12628944" author="athusoo" created="Sun, 7 Sep 2008 08:05:54 +0000"  >&lt;p&gt;Yes. we should be checking for valid character sets at SemanitcAnalysis and not at parse. By encoding charSetName as a list of char sets we are gauranteeing that in future we will have to change the parse code as we add more characterset support. That does not scale. In my opinion, parse should just encode rules on what can be construed as a valid characterset name (valid by construction). Checking on the actual list of names that we support should be done at semantic analysis time.&lt;/p&gt;</comment>
                            <comment id="12629643" author="namit" created="Tue, 9 Sep 2008 22:36:56 +0000"  >&lt;p&gt;comments incorporated&lt;/p&gt;</comment>
                            <comment id="12631778" author="athusoo" created="Wed, 17 Sep 2008 14:00:36 +0000"  >&lt;p&gt;Lets cleanup the System.outs that we have (I presume that this debugging code that did not get removed when you submitted the patch).&lt;/p&gt;

&lt;p&gt;Also can you comment on the second one - is that code ok?&lt;/p&gt;

&lt;p&gt;Inline Comments:&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/exec/CompositeHiveObject.java:92: We should avoid System.outs - use logging instead.&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:143: Don&apos;t we have to look at all the sort columns here?&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java:40: No System.out !!&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/exec/LabeledCompositeHiveObject.java:40: No System.out !!&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Ashish&lt;/p&gt;</comment>
                            <comment id="12631969" author="namit" created="Wed, 17 Sep 2008 21:48:22 +0000"  >&lt;p&gt;some old code for debugging was left - removed that.&lt;/p&gt;

&lt;p&gt;the ddltask change is intentional. we are only comparing the bucketcols size, since we are looking for a superset (sortCols &amp;gt;= bucketCols) &lt;br/&gt;
adding a new patch&lt;/p&gt;</comment>
                            <comment id="12652058" author="athusoo" created="Mon, 1 Dec 2008 17:01:11 +0000"  >&lt;p&gt;This was checked in a while back.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12389572" name="patch1" size="38803" author="namit" created="Fri, 5 Sep 2008 18:10:21 +0000"/>
                            <attachment id="12389784" name="patch2" size="39861" author="namit" created="Tue, 9 Sep 2008 22:36:56 +0000"/>
                            <attachment id="12390311" name="patch3.txt" size="42259" author="namit" created="Wed, 17 Sep 2008 21:54:10 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 6 Sep 2008 07:00:59 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73838</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 9 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0iujz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>108044</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-20] Hive should be able to create tables over binary flat files</title>
                <link>https://issues.apache.org/jira/browse/HIVE-20</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Currently Hive only supports binary data embedded inside SequenceFiles. This is a request to enable this over flat files (for example a flat file containings sets of serialized binary records).&lt;/p&gt;

&lt;p&gt;It is understood that this file cannot be split and that the user had to ensure that there is enough file level parallelism. maybe related to &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-3787&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-3787&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Hive also does not play nice with generic hadoop serialization mechanism (1986) - however that requires much more comprehensive change to SerDe interfaces.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12403645">HIVE-20</key>
            <summary>Hive should be able to create tables over binary flat files</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="jsensarma">Joydeep Sen Sarma</reporter>
                        <labels>
                    </labels>
                <created>Wed, 3 Sep 2008 22:48:33 +0000</created>
                <updated>Mon, 1 Dec 2008 17:04:40 +0000</updated>
                                                                            <component>Serializers/Deserializers</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                <comments>
                            <comment id="12629029" author="wyckoff" created="Mon, 8 Sep 2008 00:51:51 +0000"  >&lt;p&gt;I assume we don&apos;t want to build this until we can read from flat files.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12403646">HIVE-24</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 8 Sep 2008 00:51:51 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>42922</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 21 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0iuif:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>108037</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>


<item>
            <title>[HIVE-24] support for reading binary data from flat files</title>
                <link>https://issues.apache.org/jira/browse/HIVE-24</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;like textinputformat - looking for a concrete implementation to read binary records from a flat file (that may be compressed).&lt;/p&gt;

&lt;p&gt;it&apos;s assumed that hadoop can&apos;t split such a file. so the inputformat can set splittable to false.&lt;/p&gt;

&lt;p&gt;tricky aspects are:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;how to know what class the file contains (has to be in a configuration somewhere).&lt;/li&gt;
	&lt;li&gt;how to determine EOF (would be nice if hadoop can determine EOF and not have the deserializer throw an exception  (which is hard to distinguish from a exception due to corruptions?)). this is easy for non-compressed streams - for compressed streams - DecompressorStream has a useful looking getAvailable() call - except the class is marked package private.&lt;/li&gt;
&lt;/ul&gt;

</description>
                <environment></environment>
        <key id="12403646">HIVE-24</key>
            <summary>support for reading binary data from flat files</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="jsensarma">Joydeep Sen Sarma</reporter>
                        <labels>
                    </labels>
                <created>Wed, 3 Sep 2008 22:55:33 +0000</created>
                <updated>Sat, 17 Dec 2011 00:09:05 +0000</updated>
                            <resolved>Mon, 17 Nov 2008 01:30:48 +0000</resolved>
                                                    <fixVersion>0.3.0</fixVersion>
                                    <component>Serializers/Deserializers</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                <comments>
                            <comment id="12628192" author="jsensarma" created="Wed, 3 Sep 2008 22:56:09 +0000"  >&lt;p&gt;see &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-4065&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-4065&lt;/a&gt; as well.&lt;/p&gt;</comment>
                            <comment id="12628484" author="owen.omalley" created="Thu, 4 Sep 2008 22:18:02 +0000"  >&lt;p&gt;How are you going to define/find record boundaries? Is it going to be key/value or single blob? How is it different from KeyValueInputFormat?&lt;/p&gt;</comment>
                            <comment id="12628526" author="wyckoff" created="Thu, 4 Sep 2008 23:51:42 +0000"  >&lt;p&gt;I think what he means is that file format and record boundary finding could be decoupled, and the latter made into some kind of interface that may be related to the deserializer. He&apos;s talking about binary data for which only really the deserializer can figure out record boundaries. &lt;/p&gt;</comment>
                            <comment id="12628530" author="wyckoff" created="Fri, 5 Sep 2008 01:22:02 +0000"  >&lt;p&gt;It would be nice to also take care of the case where the file is self describing like sequence files. If we had a FlatFileRecordReader(conf, split, serializerFactory) where the serializerFactory could be initialized with the conf, split and the input stream, it could optionally read the self describing data from the inputstream. Then regardless, it could be used to implement getKey and getValue which it could do from the self described data or as you said based on the path. Or it could just instantiate the factory from a conf variable.&lt;/p&gt;

&lt;p&gt;Then the user is free to implement the serializer lookup however they want much like most of the rest of the system.&lt;/p&gt;

&lt;p&gt;This means the serializer lookup is very low in the stack, but since one must implement next and as Joy points out, you can&apos;t do that without the serializer??&lt;/p&gt;

&lt;p&gt;This solves this case, but also the case of self describing thrift TRecordStream since the serializer class info would be in the header itself.&lt;/p&gt;

&lt;p&gt;It would also be nice if the underlying input stream could actually be an interface because sometimes there&apos;s a flat file, but other times it may be compressed or some other format, but that format is capable of producing a stream of bytes. &lt;/p&gt;

&lt;p&gt;So, I guess I&apos;m advocating for flexibility in defining the serializer lookup logic as well as how to read from the file.&lt;/p&gt;
</comment>
                            <comment id="12628658" author="owen.omalley" created="Fri, 5 Sep 2008 15:50:46 +0000"  >&lt;p&gt;Ok, a dispatching FileInputFormat could make sense. Where it looked at the&lt;br/&gt;
filenames or header of the files and picked the appropriate reader. At that&lt;br/&gt;
point, it isn&apos;t about binary files, because you&apos;d want it to work for text&lt;br/&gt;
files also. What would be the approach? Filenames like we do with the&lt;br/&gt;
compression of text files? Or sampling the first 80 bytes looking for a&lt;br/&gt;
header?&lt;/p&gt;

&lt;p&gt;&amp;#8211; Owen&lt;/p&gt;</comment>
                            <comment id="12628679" author="wyckoff" created="Fri, 5 Sep 2008 17:29:55 +0000"  >&lt;p&gt;I think in Joy&apos;s case, it would be the filename and/or with some configuration info in the jobconf.  In the TRecordStream case, we would need to use some code from TFixedFrameTransport to read the frame headers - TFixedFrameTransport is splittable. (TRS is a thin layer on top of TFFT).&lt;/p&gt;

&lt;p&gt;Joy or I can post a proposed API.&lt;/p&gt;

&lt;p&gt;good point, we should make it general and not just for binary.&lt;/p&gt;

&lt;p&gt;pete&lt;/p&gt;
</comment>
                            <comment id="12628806" author="wyckoff" created="Sat, 6 Sep 2008 00:45:43 +0000"  >&lt;p&gt;I propose we re-use the code from SequenceFileRecordReader by making it depend on a SplittableTypedFile interace (below) which conveniently is already implemented by SequenceFile.  Then we&apos;re basically done. &lt;/p&gt;

&lt;p&gt;I am not super-familiar with this code and the devil is probably in the details, but looking at SequenceFileRecordReader, there is basically only about 5 methods it uses from SequenceFile and those are all well defined and seem needed for any implementation of a self describing file that is splittable.&lt;/p&gt;

&lt;p&gt;We could also not touch SequenceFileRecordReader, but it seems we&apos;d just be duplicating all of its code.&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;TypedFile Interfaces&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
                                                                                                                                                                                      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; TypedFile  {                                                                                                                                                         
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void initialize(Configuration conf, InputStream in);                                                                                                                         
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt; getKeyClass();                                                                                                                                                         
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt; getValueClass();                                                                                                                                                       
                                                                                                                                                                                      
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; next(Writable key);                                                                                                                                                  
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; next(Writable key, Writable value);                                                                                                                                  
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Writable getCurrentValue();                                                                                                                                                  
}                                                                                                                                                                                     
                                                                                                                                                                                      
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; SplittableTypedFile &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; TypedFile {                                                                                                                           
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; syncSeen(); &lt;span class=&quot;code-comment&quot;&gt;// i.e., atEOF()                                                                                                                                         
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; sync(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;); &lt;span class=&quot;code-comment&quot;&gt;// skip to past last frame boundary                                                                                                                      
&lt;/span&gt;}                                                                                                                                                                                     
                                                                                                                                                                                      
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;TypedSplittableRecordReader&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// This is almost a complete cut-n-paste of existing SequenceFileRecordReader - which would be removed
&lt;/span&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;TypedSplittableRecordReader&amp;lt;K, V&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; RecordReader&amp;lt;K,V&amp;gt; {                                                                                                         
  SplittableTypedFile in;                                                                                                                                                             
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; TypedRecordReader(Configuration conf, FileSplit split, SplittableTypedFileFactory&amp;lt;K,V&amp;gt; fileFactory) {                                                                        
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.in = fileFactory.getFileReader(fs, path, conf);                                                                                                                              
  }                                                                                                                                                                                   
  &lt;span class=&quot;code-comment&quot;&gt;// the &lt;span class=&quot;code-keyword&quot;&gt;rest&lt;/span&gt; is exactly like the current sequence file implementation basically.                                                                                                     
&lt;/span&gt;}                                                                                                                                                                                     
                                                                                                                                                                                      
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;SequenceFile&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
-&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;SequenceFile {
+&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;SequenceFile &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; SplittableTypedFile {                                                                                                                                  
                                                                                                                                                                                      
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;SequenceFileInputFormat&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;                                                                                                                                                                                      

&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;SequenceFileInputFormat&amp;lt;K, V&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; FileInputFormat&amp;lt;K, V&amp;gt; {                                                                                                            
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; RecordReader&amp;lt;K,V&amp;gt; getRecordReader() {                                                                                                                                        
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; TypedSplittableRecordReader&amp;lt;K, V&amp;gt;(job, split, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SequenceFileFactory&amp;lt;K,V&amp;gt;());                                                                                             
  }                                                                                                                                                                                   
}                            

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;SelfDescribingFileExample&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;TFixedFrameTransportInputFormat &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; SplittableTypedFile {                                                                                                         
  &lt;span class=&quot;code-comment&quot;&gt;// implementing all the above should be straightforward                                                                                                                             
&lt;/span&gt;}                                                                                                                                                                                     

&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;TFixedFrameTransportFileInputFormat&amp;lt;K, V&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; FileInputFormat&amp;lt;K, V&amp;gt; {                                                                                                
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; RecordReader&amp;lt;K,V&amp;gt; getRecordReader() {                                                                                                                                        
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; TypedSplittableRecordReader&amp;lt;K, V&amp;gt;(job, split, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TFixedFrameFileFactory&amp;lt;K,V&amp;gt;());                                                                                          
  }                                                                                                                                                                                   
}                                                                                                                                                                                     

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One problem is for non-splittable files, I have to create another record reader with almost the same code. Maybe better to put everything in one interface and add boolean isSplittable and have sync just do a seek(0) and syncSeen just look at EOF.&lt;/p&gt;

</comment>
                            <comment id="12629296" author="wyckoff" created="Mon, 8 Sep 2008 21:22:44 +0000"  >&lt;p&gt;I just wanted to post pseudo-code for this design that actually addresses this JIRA &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; and not only self describing files like SequenceFile and Thrift&apos;s TRecordStream.&lt;/p&gt;

&lt;p&gt;In the case for this JIRA, the file&apos;s metadata is stored in some external store or dictionary or something.  The only way to lookup the file would be through the filename/path, so I think it&apos;s fair that on job submission, the mapping is put in the JobConf.&lt;/p&gt;

&lt;p&gt;Given this use case, and looking at line 43 of SequenceFileRecordReader (   this.in = new SequenceFile.Reader(fs, path, conf); ), the TypeFile        interface should be changed:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void initialize(Configuration conf, InputStream in);&lt;br/&gt;
+ public void initialize(FileSystem, Path, Configuration);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Obviously it has top open the inputstream anyway ( &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ).  &lt;/p&gt;

&lt;p&gt;And a typo SequenceFile would not implement SplittableTypedFile, SequenceFile.Reader would.&lt;/p&gt;



</comment>
                            <comment id="12629298" author="mahadev" created="Mon, 8 Sep 2008 21:32:10 +0000"  >&lt;p&gt;we at yahoo have been working on similar kind of files where data is just stored as binary data and is splittable. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://issues.apache.org/jira/browse/HADOOP-3315&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/browse/HADOOP-3315&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;the  spec is old and needs to be updated. TFile is meant to be a sequence file replacement.&lt;/p&gt;


&lt;p&gt;  A TFile is a container of key-value pairs. Both keys and values are type-less&lt;br/&gt;
  byte arrays. Keys can be up to 64KB, value length is not restricted. TFile&lt;br/&gt;
  further provides the following features:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Block Compression.&lt;/li&gt;
	&lt;li&gt;Named meta data blocks.&lt;/li&gt;
	&lt;li&gt;Sorted or unsorted keys.&lt;/li&gt;
	&lt;li&gt;Seek by key or by file offset.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We will update the specs on &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-3315&quot; title=&quot;New binary file format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-3315&quot;&gt;&lt;del&gt;HADOOP-3315&lt;/del&gt;&lt;/a&gt; by the end of this week. &lt;/p&gt;
</comment>
                            <comment id="12629305" author="wyckoff" created="Mon, 8 Sep 2008 21:46:41 +0000"  >&lt;p&gt;Nice. Will the TFileRecordReader fit into this paradigm?  How were you going to implement the TFileRecordReader?&lt;/p&gt;

&lt;p&gt;TFile is very similar to TRecordStream - but more full featured - sorted and seek by key. But, TRecordStream is meant to be readable/writable in many languages (first cut c++, java, python and perl). It&apos;s primary use case is non-hadoop - just a robust way of logging data, but secondarily, there&apos;s no reason not to enable directly reading/writing to one from Hadoop as it&apos;s a waste to open one, read it and write it out as a TFile or SF if one doesn&apos;t need the richer functionality that sequence file and tfile support.&lt;/p&gt;



&lt;p&gt;&amp;#8211; pete&lt;/p&gt;
</comment>
                            <comment id="12629315" author="mahadev" created="Mon, 8 Sep 2008 22:22:00 +0000"  >&lt;p&gt;the tfilerecordreader would just return raw bytes for keys and values and its up to the application to convert the raw bytes to types. Though as far as our implementation goes right now &amp;#8211; we dont have a map reduce interface for Tfiles. We intend to provide one.&lt;/p&gt;

&lt;p&gt;The TFile is also supposed to be readable/writable in  other languages with the spec being clear enough on how to read and write Tfiles. We intend to provide just java implementation of Tfiles though. &lt;/p&gt;</comment>
                            <comment id="12629339" author="wyckoff" created="Mon, 8 Sep 2008 23:56:19 +0000"  >&lt;p&gt;Actual interface&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;TypedSplittableFile.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; TypedSplittableFile {                                                                                                                                                                         
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void initialize(FileSystem fileSys, Path path, Configuration conf) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException;                                                                                                                
                                                                                                                                                                                                               
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt; getKeyClass() ;                                                                                                                                                                                 
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt; getValueClass();                                                                                                                                                                                
                                                                                                                                                                                                               
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; next(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; key) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException;                                                                                                                                                           
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; getCurrentValue(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; val) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException ;                                                                                                                                               
                                                                                                                                                                                                               
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; syncSeen(); &lt;span class=&quot;code-comment&quot;&gt;// i.e., atEOF()                                                                                                                                                                  
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void sync(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; position) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException; &lt;span class=&quot;code-comment&quot;&gt;// skip to past last frame boundary                                                                                                                      
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; getPosition() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException;                                                                                                                                                                
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void seek(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; position) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException;                                                                                                                                                          
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void close() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException;                                                                                                                                                                      
                                                                                                                                                                                                               
}    
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12629342" author="wyckoff" created="Tue, 9 Sep 2008 00:06:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;The TFile is also supposed to be readable/writable in other languages with the spec being clear enough on how to read and write Tfiles. We intend to provide just java implementation of Tfiles though.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Fair enough. If we had TFile in a number of languages it could probably be put in as a thrift transport. TRS is much simpler though and thus much easier to write in other languages.&lt;/p&gt;</comment>
                            <comment id="12629359" author="wyckoff" created="Tue, 9 Sep 2008 01:35:34 +0000"  >&lt;p&gt;This is what the proposal would look like. It:&lt;/p&gt;

&lt;p&gt;1. adds the TypedSplittableFile interface&lt;br/&gt;
2. changes SequenceFile.Reader to implement TypedSplittableFile and adds an initialize method and an empty constructor&lt;br/&gt;
3. implements TypedSplittableFileRecordReader - just copied the code from SequenceFileRecordReader and changed the constructor only&lt;br/&gt;
4. change SequenceFileRecordReader to extend TypedSplittableFileRecordReader and have its constructor just construct the parent class.&lt;/p&gt;

&lt;p&gt;Still a work in progress, but this kind of shows the API and the changes to SequenceFileRecordReader.&lt;/p&gt;
</comment>
                            <comment id="12629360" author="wyckoff" created="Tue, 9 Sep 2008 01:42:37 +0000"  >&lt;p&gt;psuedo-esque code that implements Thrift in FlatFiles where the filename is used as the key to get the thrift class from the configuration.  Implements &amp;lt;LongWritable, ThriftWritable&amp;gt;;&lt;/p&gt;

&lt;p&gt;Of course, it may be that we want to use the serialization classes.&lt;/p&gt;</comment>
                            <comment id="12629362" author="jsensarma" created="Tue, 9 Sep 2008 01:46:08 +0000"  >&lt;p&gt;@Pete - i am still trying to understand the proposed interface. in my case - the data is not splitable (so maybe there&apos;s a different base class - UnsplittableFileInputFormat). The factory approach for getting a record reader sounds interesting - but on second thoughts - since the getRR() already takes in a Configuration object - we don&apos;t need a new interface for this i think. (Meaning that we could write a ConfigurableRecordReader that could look at the config and instantiate the right record reader and then redirect all RR api calls to the contained record reader.)&lt;/p&gt;

&lt;p&gt;the main motivation i had for filing this bug was to extract out the common parts for dealing with compressed, non-splitable binary data into a base class (and most of this functionality is in the record reader) and make it easy to handle new kinds of binary data with minimal code. Binary files don&apos;t have keys and values - they just have rows of data. So one part is to supply some default key (like record number). The remaining part is to get the class of the row and the deserialization method. Appropriately - it would be nice to have a deserializerFactory that takes in a Configuration object (which is i think one of the key missing parts of hadoop-1986). that way - the deserializer can be configured by application - instead of hadoop maintaining a mapping from class -&amp;gt; Deserializer.&lt;/p&gt;

&lt;p&gt;so my proposal would be something like this (admittedly - i am not being very ambitious here - just want to cover the issue mentioned in this jira):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;/**
  * forced to make &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;since maprunner tries to use same object &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; all next() calls.
  * This way we can swap out the actual &lt;span class=&quot;code-quote&quot;&gt;&apos;row&apos;&lt;/span&gt; object on each call to next()
  */
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;RowContainer {
   &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; row;
}

/**
 * Application can &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; right deserializer based on configuration
 */
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; RowSource {
   &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Deserializer&amp;lt;?&amp;gt; getDeserializer(Configuration conf) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException;
   &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;&amp;lt;?&amp;gt; getClass();
}

/**
 * Reads a non-splitable binary flat file.
 */
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;RowSourceFileInputFormat &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; FileInputFormat&amp;lt;LongWritable, RowContainer&amp;gt; {
  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; isSplitable(FileSystem fs, Path file) { &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;; }
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; RecordReader&amp;lt;LongWritable, RowContainer&amp;gt; getRecordReader(InputSplit genericSplit, JobConf job,
                                                                      Reporter reporter)
    &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    reporter.setStatus(genericSplit.toString());
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; RowSourceRecordReader(job, (FileSplit) genericSplit);
  }
}

/**
 * Reads one row at a time. The key is the row number and the actual row is returned inside the RowContainer
 */
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;RowSourceRecordReader &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; RecordReader&amp;lt;LongWritable, RowContainer&amp;gt; {
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; rnum;
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; DataInput in;
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; DecompressorStream dcin;
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; FSDataInputStream fsin;
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; end;
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Deserializer deserializer;

    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; RowSourceRecordReader(Configuration job,
                                FileSplit split) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {

      &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Path file = split.getPath();
      CompressionCodecFactory compressionCodecs = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CompressionCodecFactory(job);
      &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; CompressionCodec codec = compressionCodecs.getCodec(file);
      FileSystem fs = file.getFileSystem(job);
      fsin = fs.open(split.getPath());

      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(codec != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
        dcin = (DecompressorStream)codec.createInputStream(fsin);
        in = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DataInputStream(dcin);
      } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
        dcin = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
        in = fsin;
      }
      rnum = 0;
      end = split.getLength();

      deserializer=(ReflectionUtils.newInstance(job.getClass(&lt;span class=&quot;code-quote&quot;&gt;&quot;mapred.input.rowsource&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, RowSource.class)).getDeserializer(job);
      deserializer.open(in);
    }

    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; LongWritable createKey() {
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; LongWritable();
    }

    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; RowContainer createValue() {
       &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; RowContainer();
    }

    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; next(LongWritable key, RowContainer value) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(dcin != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (dcin.available() == 0) {
          &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
        }
      } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(fsin.getPos() &amp;gt;= end) {
          &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
        }
      }
      key.set(rnum++);
      &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; row = deserializer.deserialize(value.row);
      value.row = row;
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
    }

    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt; getProgress() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
      &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; assumes no splitting                                                                                               
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (end == 0) {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; 0.0f;
      } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
        &lt;span class=&quot;code-comment&quot;&gt;// gives progress over uncompressed stream                                                                               
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Math&lt;/span&gt;.min(1.0f, fsin.getPos()/(&lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt;)(end));


    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; getPos() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
      &lt;span class=&quot;code-comment&quot;&gt;// position over uncompressed stream. not sure what                                                                        
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// effect &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; has on stats about job                                                                                      
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; fsin.getPos();
    }

    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; void close() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
       &lt;span class=&quot;code-comment&quot;&gt;// assuming that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; closes the underlying streams
&lt;/span&gt;       deserializer.close();
    }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12629366" author="jsensarma" created="Tue, 9 Sep 2008 02:05:51 +0000"  >&lt;p&gt;oh - and my code does not work without a change to DecompressorStream class to make that a public class (it&apos;s marked package private right now). Truthfully - that forced me to file this bug &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12629398" author="wyckoff" created="Tue, 9 Sep 2008 05:44:45 +0000"  >&lt;p&gt;It looks like we posted complimentary patches &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I think we should keep my RecordReader, but use your implementation of the TypedFileTransport as it&apos;s more general since it uses the serialization framework.&lt;/p&gt;

&lt;p&gt;This would address the use case of files whose serializers are set in the config file and self describing files like sequence files and TRecordStream and I would guess, TFile but I haven&apos;t looked at it.&lt;/p&gt;

&lt;p&gt;&amp;#8211; pete&lt;/p&gt;</comment>
                            <comment id="12629652" author="owen.omalley" created="Tue, 9 Sep 2008 23:12:56 +0000"  >&lt;p&gt;I think this is too complicated. What is the justification for these new interfaces? We already have RecordReader that already &lt;br/&gt;
expresses these concepts.&lt;/p&gt;

&lt;p&gt;Once the TFile stuff is ready, I think it would make a lot of sense to build an ObjectFile that uses the pluggable serializer&lt;br/&gt;
framework to save any objects. At that point, it becomes a potential replacement for SequenceFile. By using the serializer&lt;br/&gt;
framework, it should work fine with Java serialization, Thrift, or Protocol Buffers. I don&apos;t think having a Thrift file format is very&lt;br/&gt;
compelling at that point.&lt;/p&gt;</comment>
                            <comment id="12629654" author="wyckoff" created="Tue, 9 Sep 2008 23:19:16 +0000"  >&lt;p&gt;Yes, good point.&lt;/p&gt;

&lt;p&gt;I will change it to DeserializerTypedFile.&lt;/p&gt;

&lt;p&gt;But, the SequenceFileRecordReader is re-usable for all these.  From the reader of a file that does its own deserializing of its types, it&apos;s all the same.&lt;/p&gt;

&lt;p&gt;With this interface, the SequenceFileRecordReader can read SequenceFiles, DeserializerTypedFiles (thrift, proto buffers, record io whatever) and any other self describing typed files; sequencefile&apos;s being one example of these.&lt;/p&gt;

&lt;p&gt;Otherwise, I don&apos;t see how not to be re-implementing the current SequenceFileRecordReader functionality for all these use cases??&lt;/p&gt;

&lt;p&gt;&amp;#8211; pete&lt;/p&gt;</comment>
                            <comment id="12629659" author="wyckoff" created="Tue, 9 Sep 2008 23:42:24 +0000"  >&lt;p&gt;I will submit a patch with its own RecordReader so we don&apos;t need to change SequenceFileRR. I&lt;/p&gt;</comment>
                            <comment id="12629661" author="cutting" created="Tue, 9 Sep 2008 23:50:53 +0000"  >&lt;p&gt;&amp;gt; With this interface, the SequenceFileRecordReader can read SequenceFiles, DeserializerTypedFiles (thrift, proto buffers, record io whatever) and any other self describing typed files; sequencefile&apos;s being one example of these.&lt;/p&gt;

&lt;p&gt;I&apos;m all for reducing code duplication.  So if SequenceFileRecordReader can mostly be replaced with code that&apos;s shared by other file format&apos;s that&apos;d be great.  But we need those file formats to exist before we perform this factoring.  Is there a splittable thrift or protocol-buffer input file format implementation yet that can share code with SequenceFileInputFormat?  Let&apos;s not refactor until we have these.&lt;/p&gt;</comment>
                            <comment id="12631184" author="wyckoff" created="Mon, 15 Sep 2008 23:22:36 +0000"  >&lt;p&gt;This patch implements: FlatFileDeserializerRecordReader (and input format), which reads rows of any kind of data using a Deserializer that is given in the JobConf.&lt;/p&gt;

&lt;p&gt;For the current test case, I created a simple deserializer for \n separated plain text as a thin wrapper around LineRecordReader.LineReader to show what LineRecordReader would look like using FlatFileDeserializerRecordReader.&lt;/p&gt;

&lt;p&gt;One thing is this is my first foray into Java generics, so I would appreciate an experienced generics person looking at the code.&lt;/p&gt;

&lt;p&gt;&amp;#8211; pete&lt;/p&gt;

&lt;p&gt;I want to also implement a thrift or record io test case.&lt;/p&gt;</comment>
                            <comment id="12631187" author="wyckoff" created="Mon, 15 Sep 2008 23:26:38 +0000"  >
&lt;p&gt;Should also mention we could put this in contrib as it is self contained.&lt;/p&gt;</comment>
                            <comment id="12631544" author="cutting" created="Tue, 16 Sep 2008 20:52:58 +0000"  >&lt;p&gt;Please don&apos;t edit descriptions.  It&apos;s very difficult to tell what&apos;s changed.  The description should describe the problem.  The discussion below should present solutions.  Editing descriptions and comments makes it very hard to follow an issue.  This is discussed in the &quot;Jira Guidlines&quot; section of &lt;a href=&quot;http://wiki.apache.org/hadoop/HowToContribute&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/hadoop/HowToContribute&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12631555" author="wyckoff" created="Tue, 16 Sep 2008 21:18:28 +0000"  >&lt;p&gt;reverting to original description &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12631913" author="wyckoff" created="Wed, 17 Sep 2008 20:12:16 +0000"  >&lt;p&gt;proposal (which does not touch any existing code):&lt;/p&gt;

&lt;p&gt;1. extend Deserializer with an interface that requires returning the actual type being deserialized&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;ParameterizedDeserializer.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; ParameterizedDeserializer&amp;lt;T&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; Deserializer&amp;lt;T&amp;gt; {
  &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;&amp;lt;? &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; T&amp;gt; getRealClass() ;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;2. create a Serialization implementation which gets the Serializer/Deserializer from the JobConf - e.g.,&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;   &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; ParameterizedDeserializer&amp;lt;R&amp;gt; getDeserializer(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;&amp;lt;R&amp;gt; c) {                                                                                                                                        
     &lt;span class=&quot;code-comment&quot;&gt;// ignore c. doesn&apos;t matter, it is coming from the configuration                                                                                                                                        
&lt;/span&gt;      &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;&amp;lt;? &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; ParameterizedDeserializer&amp;gt; t = conf.getClass(&lt;span class=&quot;code-quote&quot;&gt;&quot;mapred.input.io.deserializer&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, ParameterizedDeserializer.class);                                                                      
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; ReflectionUtils.newInstance(t, conf);                                                                                                                                                           
    } 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;3. Parameterized deserializers will (typically) get the specific class (? extends T - e.g., T= Record) they are implementing from the JobConf. e.g.,&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.recordClass = conf.getClass(&lt;span class=&quot;code-quote&quot;&gt;&quot;mapred.input.io.record_class&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, Record.class);                                                                                                                   
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4. RecordReader.getValueClass looks like:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; R createValue() {                                                                                                                                                                     
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; (R)ReflectionUtils.newInstance(deserializer.getRealClass(),conf);                                                                                                                                              
  }                    
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;5. Setting up the JobConf:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;      job.setClass(&lt;span class=&quot;code-quote&quot;&gt;&quot;mapred.input.io.deserializer&quot;&lt;/span&gt;, .serializer.RecordIOSerialization.RecordIODeserializer.class, serializer.ParameterizedDeserializer.class);          
                                                                                                                                                                                                             
      &lt;span class=&quot;code-comment&quot;&gt;// Set &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; so the RecordIO Deserializer knows the specific Record &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;in the file                                                                                                                   
&lt;/span&gt;      job.setClass(&lt;span class=&quot;code-quote&quot;&gt;&quot;mapred.input.io.recordio_class&quot;&lt;/span&gt;, FlatFileDeserializerTestObj.class, record.Record.class);             
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These classes could all go into a contrib directory devoted to contributing Serialization implementations.  Or it could be in core and mapred.&lt;/p&gt;

&lt;p&gt;Issues - why not get the  deserialization specific class in Serialization.getDeserializer (and implement getRealClass in this Serialization implementation - no need for any new interface) and pass that in to any normal Deserializer?  This would make things more uniform and also mean the Deserializer is any old deserializer.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;&amp;lt;? &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; R&amp;gt; realClass = conf.getClass(&lt;span class=&quot;code-quote&quot;&gt;&quot;mapred.input.io.deserializer.class&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;&amp;lt;R&amp;gt;);                                                                                                                   
&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; ReflectionUtils.newInstance(conf.getClass(&lt;span class=&quot;code-quote&quot;&gt;&quot;mapred.input.deserializer&quot;&lt;/span&gt;,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;,Deserializer.class));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And then we can construct an instance of realClass and pass that into any Deserializer.deserialize(T t).&lt;/p&gt;

&lt;p&gt;I am just learning generics and it doesn&apos;t seem conf.getClass(&quot;mapred.input.io.deserializer.class&quot;, null, Class&amp;lt;R&amp;gt;);  is possible because it doesn&apos;t accept Class&amp;lt;R&amp;gt;. It wants something more like Record.class, forcing us into doing this in the deserizlier.&lt;/p&gt;
</comment>
                            <comment id="12631938" author="owen.omalley" created="Wed, 17 Sep 2008 20:42:35 +0000"  >&lt;p&gt;I&apos;ve lost the motivation for this. It complicates the public interfaces a lot and doesn&apos;t have any payback.&lt;/p&gt;

&lt;p&gt;In our experience, given a file format, the code is pretty independent, but it is tied to the fragment splitting. &lt;/p&gt;

&lt;p&gt;Is the goal of this jira to:&lt;br/&gt;
  1. Make a generic / self-detecting format?&lt;br/&gt;
  2. A generic file format?&lt;/p&gt;

&lt;p&gt;In either case, changes to the serialization framework seems like serious overkill.&lt;/p&gt;</comment>
                            <comment id="12632058" author="jsensarma" created="Thu, 18 Sep 2008 02:11:17 +0000"  >&lt;p&gt;Hi Owen - the motivation was based on different families of binary (or even non binary) data embedded within flat files (by which i mean they are unsplittable and not self-describing (except for compression).&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;We should be able to write one concrete implementation that covers no-splits and compression related code&lt;/li&gt;
	&lt;li&gt;One should be able to plug in different deserializers for different binary formats&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The desire was that once this is written out - different deserializers can be plugged in easily. In that sense - this does not follow the general pattern that you observed of having to write custom code to deal with splitting (since there&apos;s no splitting here). Existing interfaces should not have to be changed (although things got pretty complicated in the intermediate discussion) - and i don&apos;t think they are although i am going to send back feedback on the code separately. The code should be really simple i would think.&lt;/p&gt;

&lt;p&gt;Do you think this is a reasonable thing to add?&lt;/p&gt;</comment>
                            <comment id="12632067" author="wyckoff" created="Thu, 18 Sep 2008 02:50:30 +0000"  >&lt;blockquote&gt;&lt;p&gt;In either case, changes to the serialization framework seems like serious overkill.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;you are right - I don&apos;t know why I went that direction. It should be exactly the oppostite of the way I coded it up.&lt;/p&gt;

&lt;p&gt;All one needs is some way of getting SerializationContext information (to instantiate the right Serialization Object and then the actual subclass we want to deserialize; e.g., Record/MyRecordObj). &lt;span class=&quot;error&quot;&gt;&amp;#91;this was called RowSource in joy&amp;#39;s code example&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;And then a simple record reader that uses that info to instantiate a deserializer and done.&lt;/p&gt;

&lt;p&gt;No changes to the serialization framework.&lt;/p&gt;

&lt;p&gt;I actually have that with unit tests and just need to clean up the documentation and such.&lt;/p&gt;

&lt;p&gt;&amp;#8211; pete&lt;/p&gt;</comment>
                            <comment id="12632072" author="wyckoff" created="Thu, 18 Sep 2008 03:09:37 +0000"  >&lt;p&gt;this is all this is - don&apos;t know why i went the other way &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12632089" author="wyckoff" created="Thu, 18 Sep 2008 04:46:22 +0000"  >&lt;p&gt;Here&apos;s the simple patch and in contrib/serialization/readers. No build file with this yet as will coordinate with Tom White on that.&lt;/p&gt;

&lt;p&gt;Although the unit test is only for Java now, I did have it with Thrift and RecordIO, but since neither of those are checked in yet, i didn&apos;t include those tests.&lt;/p&gt;
</comment>
                            <comment id="12632217" author="tomwhite" created="Thu, 18 Sep 2008 14:03:19 +0000"  >&lt;p&gt;A few comments:&lt;/p&gt;

&lt;p&gt;Could the types be called FlatFileInputFormat and FlatFileRecordReader?&lt;/p&gt;

&lt;p&gt;Is a SerializationContext class needed? The Serialization can be got from the SerializationFactory. It just needs to know the base class (Writable, TBase etc). A second configuration parameter is needed to specify the concrete class, but I don&apos;t see why the FlatFileDeserializerRecordReader can&apos;t just get these two classes from the Configuration itself.&lt;/p&gt;

&lt;p&gt;Can the classes go in the org.apache.hadoop.contrib.serialization.mapred package to echo the main mapred package? When &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-1230&quot; title=&quot;Replace parameters with context objects in Mapper, Reducer, Partitioner, InputFormat, and OutputFormat classes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-1230&quot;&gt;&lt;del&gt;HADOOP-1230&lt;/del&gt;&lt;/a&gt; is done an equivalent could then go in the mapreduce package.&lt;/p&gt;

&lt;p&gt;I agree it would be good to have tests for Writable, Java Serialization and Thrift to test the abstraction.&lt;/p&gt;

&lt;p&gt;Shouldn&apos;t keys be file offsets, similar to TextInputFormat? The row numbers you have are actually the row number within the split, which might be confusing (and they&apos;re not unique per file).&lt;/p&gt;</comment>
                            <comment id="12632309" author="wyckoff" created="Thu, 18 Sep 2008 17:43:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;Could the types be called FlatFileInputFormat and FlatFileRecordReader?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, better names.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is a SerializationContext class needed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If the Serialization is in contrib, one would need to use ReflectionUtils to instantiate it and it wouldn&apos;t be in any Factory, would it?  So, in this case, it needs to know the name of the Class to instantiate it, no?  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;an&apos;t just get these two classes from the Configuration itself.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;wanted to make it extensible so it could some from the configuration or maybe some place else - the name of the file or some external store or something depending on the application. Of course, in that case, one could argue a higher level is setting that up anyway, so why don&apos;t they just do the lookup and store the info in the configuration. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Shouldn&apos;t keys be file offsets, similar to TextInputFormat? The row numbers you have are actually the row number within the split, which might be confusing (and they&apos;re not unique per file).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Are the file offsets useful anywhere?  Maybe we should just always return the same instance of some dummy Writable for performance if the key isn&apos;t used anyway??&lt;/p&gt;</comment>
                            <comment id="12632451" author="jsensarma" created="Thu, 18 Sep 2008 23:41:10 +0000"  >&lt;p&gt;couple of comments on the code:&lt;/p&gt;

&lt;p&gt;SerializationContext&amp;lt;R&amp;gt; sinfo = (SerializationContext&amp;lt;R&amp;gt;)ReflectionUtils.newInstance(sinfoClass, conf);&lt;br/&gt;
sinfo.setConf(conf);&lt;/p&gt;

&lt;p&gt;the setConf call is redundant since SerializationContext is configurable&lt;/p&gt;

&lt;p&gt;key.set(rnum++);&lt;br/&gt;
if (key == null)&lt;br/&gt;
    key = createKey();&lt;/p&gt;

&lt;p&gt;switch order? (or maybe the createKey()/createValue() is not required?)&lt;/p&gt;

&lt;p&gt;otherwise looks good.&lt;/p&gt;

&lt;p&gt;wrt some of Tom&apos;s comments:&lt;/p&gt;

&lt;p&gt;&amp;gt; The row numbers you have are actually the row number within the split, which might be confusing&lt;br/&gt;
the inputformat is not splittable - so we are safe here&lt;/p&gt;

&lt;p&gt;&amp;gt; Is a SerializationContext class needed? &lt;/p&gt;

&lt;p&gt;Very much so. Let me walk through the Hive use case:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Hive knows the deserialization class for each file. However - it knows this through metadata about the &lt;em&gt;file&lt;/em&gt;.  (The file belongs to a table that has some metadata). This metadata is passed to mappers through the configuration.&lt;/li&gt;
	&lt;li&gt;In this case the mapping is not from a class -&amp;gt; deserializer but from a file -&amp;gt; deserializer - and the ability to bootstrap the serialization factory from the configuration is critical (the configuration has both the file name and the metadata about the file name)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This also seems to be the hadoop style of doing things (all implementations can be configurable) - and i think if it covers the hive case - it would help others as well. In fact - i think we should try to make this (configurable serialization factory pattern) a more fundamental part of the infrastructure. it seems more general than the class-&amp;gt;serialization way of bootstrapping (de)serialization.&lt;/p&gt;





</comment>
                            <comment id="12632475" author="wyckoff" created="Fri, 19 Sep 2008 00:54:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/MAPREDUCE-376&quot; title=&quot;Add serialization for Thrift&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAPREDUCE-376&quot;&gt;&lt;del&gt;HADOOP-3787&lt;/del&gt;&lt;/a&gt; includes the build file changes and such for creating src/contrib/serialization, so will need to wait for its commit.&lt;/p&gt;
</comment>
                            <comment id="12632501" author="owen.omalley" created="Fri, 19 Sep 2008 04:06:56 +0000"  >&lt;p&gt;I&apos;m still not convinced about the utility of this class outside of Hive. What is the advantage of storing the data this way?&lt;br/&gt;
If you put it in a sequence file or t-file, a single bug in the serialization code for the application type doesn&apos;t destroy&lt;br/&gt;
your entire file. With this format, that is exactly what will happen. Furthermore, since the types have to be configured,&lt;br/&gt;
you can&apos;t use multiple ones in different contexts.&lt;/p&gt;

&lt;p&gt;Maybe we should just put this into Hive?&lt;/p&gt;</comment>
                            <comment id="12632529" author="jsensarma" created="Fri, 19 Sep 2008 05:59:33 +0000"  >&lt;p&gt;yes - given that this has no dependency on core hadoop now - i really don&apos;t care - we can put this into Hive. The generic ThriftDeserializer is trivial - we could duplicate the code for now and then remove it once 3787 provides those classes as well.&lt;/p&gt;

&lt;p&gt;btw - we also don&apos;t store data in this manner. agree with all your observations. however &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this requested originated from outside Hive/Facebook. I get the impression (perhaps wrong) that quite a few people just dump thrift logs into a flat file (just like people dump apache logs into a flat file). This is also because Thrift does not have (so far) a good framed file format.&lt;/li&gt;
	&lt;li&gt;the same counter argument can be made for TextFileInputFormat. The general observation is that data originates outside the hadoop ecosystem and the general format it originates in is flat files. We should strive the easiest way to absorb this data and transform it into a better one (like Sequencefile).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;That is the general effort with Hive at least. We expect users to create temporary tables by pointing to flat files. And then quickly do some transformations (using sql and potentially scripts) and load it into tables in sequencefile (like) format (for longer term storage).  Being able to point to thrift flat files(and potentially other binary files)  is part of the data integration story.&lt;/p&gt;

&lt;p&gt;&amp;gt; Furthermore, since the types have to be configured, you can&apos;t use multiple ones in different contexts. &lt;/p&gt;

&lt;p&gt;not sure what u mean - but this is not true. the deserializer is obtained from a combination of file name and file name-&amp;gt;deserializer metadata from an external source. Different files can be read using different deserializers and then operated on in the same map-reduce program (the application logic has logic to deal with different classes based on the file name).  we will only be too happy to demonstrate a join of two different thrift classes (in different files/tables) using Hive and a generic flat file reader like this.&lt;/p&gt;</comment>
                            <comment id="12632752" author="wyckoff" created="Fri, 19 Sep 2008 17:36:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m still not convinced about the utility of this class outside of Hive. What is the advantage of storing the data this way?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;1. You don&apos;t need a loader.  &lt;br/&gt;
2. Tools outside of hadoop can use the data - python, perl, c++, ...&lt;br/&gt;
3. There are other file formats that are splittable and self or non self-describing. Hadoop is generally pretty pluggable, but not at the file level. Would be nice to have generic file interfaces that one can implement to get &lt;b&gt;First Class&lt;/b&gt; hadoop treatment for any file format.&lt;/p&gt;

&lt;p&gt;To be clear, Hive writes and reads binary data to sequence files only now. We load all binary data into sequence files.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;i really don&apos;t care - we can put this into Hive. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;-1&lt;/p&gt;

&lt;p&gt;This is a general FlatFileRecordReader - &lt;a href=&quot;https://issues.apache.org/jira/browse/MAPREDUCE-252&quot; title=&quot;Create an InputFormat for reading lines of text as Java Strings&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAPREDUCE-252&quot;&gt;&lt;del&gt;HADOOP-3566&lt;/del&gt;&lt;/a&gt; seems to be a non-general version of this? (with the issue of that being &amp;lt;String, Void&amp;gt;)&lt;/p&gt;

&lt;p&gt;And note my intention is to put this in contrib/serialization&lt;/p&gt;



</comment>
                            <comment id="12632760" author="wyckoff" created="Fri, 19 Sep 2008 18:28:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;And note my intention is to put this in contrib/serialization&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry, I meant. my intention was (not is) to put this in contrib/serialization, but if there is objection, i can change the patch to contrib/hive.&lt;/p&gt;
</comment>
                            <comment id="12632806" author="wyckoff" created="Fri, 19 Sep 2008 20:26:13 +0000"  >&lt;p&gt;RE: FlatFileRecordReader&apos;s signature.&lt;/p&gt;

&lt;p&gt;What would the implications be of changing the signature to &amp;lt;T, Void&amp;gt; ? Owen points out on &lt;a href=&quot;https://issues.apache.org/jira/browse/MAPREDUCE-252&quot; title=&quot;Create an InputFormat for reading lines of text as Java Strings&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAPREDUCE-252&quot;&gt;&lt;del&gt;HADOOP-3566&lt;/del&gt;&lt;/a&gt; there can be benefits to this viz sorting but that JIRA is for Strings, whereas here T could be anything.&lt;/p&gt;

&lt;p&gt;(Assuming for a moment that &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-1230&quot; title=&quot;Replace parameters with context objects in Mapper, Reducer, Partitioner, InputFormat, and OutputFormat classes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-1230&quot;&gt;&lt;del&gt;HADOOP-1230&lt;/del&gt;&lt;/a&gt; is implemented. Now it would be &amp;lt;RowContainer&amp;lt;T&amp;gt;, Void&amp;gt;)&lt;/p&gt;

&lt;p&gt;thanks, pete&lt;/p&gt;</comment>
                            <comment id="12633082" author="wyckoff" created="Sun, 21 Sep 2008 17:33:28 +0000"  >&lt;p&gt;if there are implications, why not make the signature &amp;lt;Void, T&amp;gt; and when someone wants the sorting, have them use the InverseMapper instead of the IdentityMapper?&lt;/p&gt;</comment>
                            <comment id="12633566" author="wyckoff" created="Mon, 22 Sep 2008 23:46:13 +0000"  >&lt;p&gt;This is what &lt;a href=&quot;https://issues.apache.org/jira/browse/MAPREDUCE-252&quot; title=&quot;Create an InputFormat for reading lines of text as Java Strings&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAPREDUCE-252&quot;&gt;&lt;del&gt;HADOOP-3566&lt;/del&gt;&lt;/a&gt; looks like as an instance of a FlatFileRecordReader (with signature &amp;lt;Void, String&amp;gt;, not &amp;lt;String, Void&amp;gt;).  This assumes there is a StringSerialization implementation (based on LineRecordReader) and that &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-1230&quot; title=&quot;Replace parameters with context objects in Mapper, Reducer, Partitioner, InputFormat, and OutputFormat classes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-1230&quot;&gt;&lt;del&gt;HADOOP-1230&lt;/del&gt;&lt;/a&gt; is implemented. But, it should hopefully demonstrate that FlatFileRecordReader can be used for non binary records.  Although,  without this, it can still be be used for anything that implements the Serialization interface.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;StringInputFormat.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;StringInputFormat &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; FileInputFormat&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Void&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; JobConfigurable {                                                     
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; CompressionCodecFactory compressionCodecs = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;                                                                                           
                                                                                                                                                      
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void configure(JobConf conf) {                                                                                                               
    compressionCodecs = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CompressionCodecFactory(conf);                                                                                            
  }                                                                                                                                                   
                                                                                                                                                      
  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; isSplittable(FileSystem fs, Path file) {                                                                                          
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; compressionCodecs.getCodec(file) == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;                                                                     
  }                                                                                                                                                   
                                                                                                                                                      
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; RecordReader&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Void&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; getRecordReader(InputSplit split,                                                                                 
                                                    JobConf job, Reporter reporter)                                                                   
    &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {                                                                                                                              
                                                                                                                                                      
    reporter.setStatus(split.toString());                                                                                                             
                                                                                                                                                      
    &lt;span class=&quot;code-comment&quot;&gt;//                                                                                                                                                
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// Set &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; so the SerializerFromConf can lookup our deserializer.                                                                                
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;//                                                                                                                                                
&lt;/span&gt;    job.setClass(FlatFileRecordReader.SerializationContextFromConf.SerializationImplKey,                                                              
                 org.apache.hadoop.contrib.serialization.string.StringSerialization.class,                                                            
                 org.apache.hadoop.io.Serialization.class);                                                                                           
                                                                                                                                                      
    job.setClass(FlatFileRecordReader.SerializationContextFromConf.SerializationSubclassKey,                                                          
                 java.lang.&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;.class, java.lang.&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;.class);                                                                                     
                                                                                                                                                      
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; FlatFileRecordReader&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt;(job, (FileSplit) split);                                                                                  
  }                                                                                                                                                   
}                                                                                                                                                     
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12633822" author="cutting" created="Tue, 23 Sep 2008 17:47:15 +0000"  >&lt;p&gt;&amp;gt; my intention was to put this in contrib/serialization, but if there is objection, i can change the patch to contrib/hive.&lt;/p&gt;

&lt;p&gt;+1 I&apos;d rather not have contrib/serialization just become a grab-bag of io-related stuff.  If this is needed by Hive only, then it belongs in contrib/hive.  If we decide (subsequently, perhaps) that it has wide utility as a generic API for access to files in a variety of formats for a variety of applications, then perhaps it could be moved to mapred.  But that doesn&apos;t yet sound like the consensus, so contrib/hive is probably best for now.&lt;/p&gt;</comment>
                            <comment id="12633949" author="wyckoff" created="Tue, 23 Sep 2008 22:45:03 +0000"  >&lt;p&gt;namespaced into hive and also brought back the RowContainer so it will work without &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-1230&quot; title=&quot;Replace parameters with context objects in Mapper, Reducer, Partitioner, InputFormat, and OutputFormat classes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-1230&quot;&gt;&lt;del&gt;HADOOP-1230&lt;/del&gt;&lt;/a&gt; being fixed.  &lt;/p&gt;

&lt;p&gt;The testcase uses JavaSerialization, WritableSerialization (Record) and ThriftSerialization.&lt;/p&gt;</comment>
                            <comment id="12634030" author="hadoopqa" created="Wed, 24 Sep 2008 05:54:01 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12390798/HADOOP-4065.2.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12390798/HADOOP-4065.2.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision 698385.&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 9 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    +1 findbugs.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    +1 core tests.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    -1 contrib tests.  The patch failed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3358/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3358/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3358/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3358/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3358/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3358/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3358/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3358/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12634337" author="wyckoff" created="Wed, 24 Sep 2008 23:03:52 +0000"  >&lt;p&gt;hudson -1 contrib i think this wasn&apos;t due to this patch.&lt;/p&gt;</comment>
                            <comment id="12634338" author="wyckoff" created="Wed, 24 Sep 2008 23:04:08 +0000"  >&lt;p&gt;re-submitting for hudson.&lt;/p&gt;</comment>
                            <comment id="12634394" author="hadoopqa" created="Thu, 25 Sep 2008 07:20:14 +0000"  >&lt;p&gt;+1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12390798/HADOOP-4065.2.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12390798/HADOOP-4065.2.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision 698721.&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 9 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    +1 findbugs.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.&lt;/p&gt;

&lt;p&gt;    +1 core tests.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    +1 contrib tests.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3367/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3367/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3367/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3367/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3367/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3367/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3367/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3367/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12642527" author="jsensarma" created="Fri, 24 Oct 2008 19:37:16 +0000"  >&lt;p&gt;+1. &lt;/p&gt;</comment>
                            <comment id="12647312" author="athusoo" created="Thu, 13 Nov 2008 15:01:20 +0000"  >&lt;p&gt;I think this patch is already commited. Is there anything else needed on this. Can we mark this as resolved?&lt;/p&gt;</comment>
                            <comment id="12648059" author="wyckoff" created="Mon, 17 Nov 2008 01:30:47 +0000"  >&lt;p&gt;yes, already committed.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12403645">HIVE-20</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12404460">HADOOP-4192</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12400531">MAPREDUCE-376</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12390331" name="FlatFileReader.java" size="10334" author="wyckoff" created="Thu, 18 Sep 2008 03:09:37 +0000"/>
                            <attachment id="12389715" name="HADOOP-4065.0.txt" size="13182" author="wyckoff" created="Tue, 9 Sep 2008 01:35:34 +0000"/>
                            <attachment id="12390340" name="HADOOP-4065.1.txt" size="18429" author="wyckoff" created="Thu, 18 Sep 2008 04:46:22 +0000"/>
                            <attachment id="12390145" name="HADOOP-4065.1.txt" size="16198" author="wyckoff" created="Mon, 15 Sep 2008 23:22:36 +0000"/>
                            <attachment id="12390798" name="HADOOP-4065.2.txt" size="34840" author="wyckoff" created="Tue, 23 Sep 2008 22:45:03 +0000"/>
                            <attachment id="12389716" name="ThriftFlatFile.java" size="2515" author="wyckoff" created="Tue, 9 Sep 2008 01:42:37 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 4 Sep 2008 22:18:02 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73827</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 11 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0iuin:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>108038</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-35] Fix some of the out dated syntax in the hive tutorial</title>
                <link>https://issues.apache.org/jira/browse/HIVE-35</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Some of the users have reported inconsistencies in the hive tutorial and the actual supported syntax. Some of these issues are as follows:&lt;/p&gt;

&lt;p&gt;1. Tutorial refers to SMALLINT which is not supported currently in the Query Language.&lt;br/&gt;
2. References to INSERT INTO instead of INSERT OVERWRITE in the section on udfs.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12403776">HIVE-35</key>
            <summary>Fix some of the out dated syntax in the hive tutorial</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="athusoo">Ashish Thusoo</reporter>
                        <labels>
                    </labels>
                <created>Fri, 5 Sep 2008 18:02:04 +0000</created>
                <updated>Sat, 17 Dec 2011 00:08:26 +0000</updated>
                            <resolved>Tue, 10 Mar 2009 22:37:42 +0000</resolved>
                                                    <fixVersion>0.3.0</fixVersion>
                                    <component>Documentation</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12650837" author="besquared" created="Wed, 26 Nov 2008 01:48:19 +0000"  >&lt;p&gt;Also I don&apos;t think the \xxx syntax works for specifying ASCII characters in the delimiter specification sections of CREATE TABLE. The tutorial has them unquoted but I think the literals need to go in single quotes or in some other format.&lt;/p&gt;

&lt;p&gt;Also the LOAD DATA syntax is now different than described in the tutorial&lt;/p&gt;</comment>
                            <comment id="12680664" author="prasadc" created="Tue, 10 Mar 2009 22:37:42 +0000"  >&lt;p&gt;smallint is supported right now&lt;br/&gt;
removed references to \xxx in the tutorial&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 26 Nov 2008 01:48:19 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73822</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 46 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0iuk7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>108045</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-42] Enable findbugs and hive tests from hadoop build.xml</title>
                <link>https://issues.apache.org/jira/browse/HIVE-42</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Enable findbugs on hive code and also enable hive tests to be run as part of hadoop tests.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12403713">HIVE-42</key>
            <summary>Enable findbugs and hive tests from hadoop build.xml</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="6">Invalid</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="athusoo">Ashish Thusoo</reporter>
                        <labels>
                    </labels>
                <created>Thu, 4 Sep 2008 21:16:15 +0000</created>
                <updated>Thu, 7 Aug 2014 09:03:05 +0000</updated>
                            <resolved>Thu, 7 Aug 2014 09:03:05 +0000</resolved>
                                                                    <component>Build Infrastructure</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="14086815" author="lars_francke" created="Tue, 5 Aug 2014 21:32:02 +0000"  >&lt;p&gt;I think this can be closed now. I&apos;ll do so if no one objects in the next few days.&lt;/p&gt;</comment>
                            <comment id="14089032" author="lars_francke" created="Thu, 7 Aug 2014 09:03:05 +0000"  >&lt;p&gt;Invalid at least since the switch to Maven&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 5 Aug 2014 21:32:02 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>42914</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 24 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0iuj3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>108040</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-44] Add Hive source directories to hadoop&apos;s eclipse project .classpath files</title>
                <link>https://issues.apache.org/jira/browse/HIVE-44</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;bring in hive source directories and dependent libs into Hadoop&apos;s eclipse project&lt;/p&gt;</description>
                <environment></environment>
        <key id="12403794">HIVE-44</key>
            <summary>Add Hive source directories to hadoop&apos;s eclipse project .classpath files</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="3">Duplicate</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="prasadc">Prasad Chakka</reporter>
                        <labels>
                    </labels>
                <created>Fri, 5 Sep 2008 21:26:50 +0000</created>
                <updated>Sat, 17 Dec 2011 00:08:38 +0000</updated>
                            <resolved>Tue, 6 Jan 2009 01:50:12 +0000</resolved>
                                                    <fixVersion>0.3.0</fixVersion>
                                    <component>Build Infrastructure</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12629571" author="prasadc" created="Tue, 9 Sep 2008 18:46:25 +0000"  >&lt;p&gt;added generated test code path as well&lt;/p&gt;</comment>
                            <comment id="12629853" author="athusoo" created="Wed, 10 Sep 2008 16:22:49 +0000"  >&lt;p&gt;+1&lt;br/&gt;
Looks good.&lt;/p&gt;


&lt;p&gt;How do we add the test setup?&lt;/p&gt;</comment>
                            <comment id="12630418" author="prasadc" created="Thu, 11 Sep 2008 23:16:02 +0000"  >&lt;p&gt;since this is eclipse template file, there isn&apos;t any needed for tests&lt;/p&gt;</comment>
                            <comment id="12630549" author="hadoopqa" created="Fri, 12 Sep 2008 11:29:00 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12389767/hadoop-4092-2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12389767/hadoop-4092-2.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision 694562.&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 7 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    +1 findbugs.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    +1 core tests.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    -1 contrib tests.  The patch failed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3258/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3258/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3258/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3258/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3258/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3258/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3258/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3258/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12631092" author="dhruba" created="Mon, 15 Sep 2008 18:17:52 +0000"  >&lt;p&gt;I just committed this. Thanks Prasad!&lt;/p&gt;</comment>
                            <comment id="12631106" author="nidaley" created="Mon, 15 Sep 2008 19:10:40 +0000"  >&lt;p&gt;Dhruba, as you can see, contrib got a -1.  If you look at the console it&apos;s because the eclipse jars in the eclipse classpath are different from what test-patch.sh see&apos;s in the lib directory.&lt;/p&gt;

&lt;p&gt;Please revert this.  The patch should also include a fix to test-patch.sh.&lt;/p&gt;</comment>
                            <comment id="12631110" author="nidaley" created="Mon, 15 Sep 2008 19:18:00 +0000"  >&lt;p&gt;FYI, until this patch is reverted, all patches will get a -1 in the contrib component.&lt;/p&gt;
</comment>
                            <comment id="12631111" author="dhruba" created="Mon, 15 Sep 2008 19:21:50 +0000"  >&lt;p&gt;Oops, I looked at the test results and saw that all tests were passing. Did not look at the console output. &lt;/p&gt;</comment>
                            <comment id="12631126" author="dhruba" created="Mon, 15 Sep 2008 20:15:16 +0000"  >&lt;p&gt;Change reverted.&lt;/p&gt;</comment>
                            <comment id="12631386" author="hudson" created="Tue, 16 Sep 2008 13:09:26 +0000"  >&lt;p&gt;Integrated in Hadoop-trunk #605 (See &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/605/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/605/&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="12636041" author="devaraj" created="Wed, 1 Oct 2008 14:06:56 +0000"  >&lt;p&gt;Please mark it as a blocker for 0.19 if required&lt;/p&gt;</comment>
                            <comment id="12661029" author="rsm" created="Tue, 6 Jan 2009 01:50:12 +0000"  >&lt;p&gt;See hive-203&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12389765" name="hadoop-4092-1.patch" size="3008" author="prasadc" created="Tue, 9 Sep 2008 18:12:57 +0000"/>
                            <attachment id="12389767" name="hadoop-4092-2.patch" size="3077" author="prasadc" created="Tue, 9 Sep 2008 18:46:25 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 10 Sep 2008 16:22:49 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73818</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0iukf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>108046</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>


<item>
            <title>[HIVE-48] Support JDBC connections for interoperability between Hive and RDBMS</title>
                <link>https://issues.apache.org/jira/browse/HIVE-48</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;In many DW and BI systems, the data are stored in RDBMS for now such as oracle, mysql, postgresql ... for reporting, charting and etc.&lt;br/&gt;
It would be useful to be able to import data from RDBMS and export data to RDBMS using JDBC connections.&lt;br/&gt;
If Hive support JDBC connections, It wll be much easier to use 3rd party DW/BI tools.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12403853">HIVE-48</key>
            <summary>Support JDBC connections for interoperability between Hive and RDBMS</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rsm">Raghotham Murthy</assignee>
                                    <reporter username="warwithin">YoungWoo Kim</reporter>
                        <labels>
                    </labels>
                <created>Sun, 7 Sep 2008 15:40:50 +0000</created>
                <updated>Sat, 17 Dec 2011 00:09:04 +0000</updated>
                            <resolved>Tue, 6 Jan 2009 02:07:16 +0000</resolved>
                                                    <fixVersion>0.3.0</fixVersion>
                                    <component>JDBC</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>8</watches>
                                                                <comments>
                            <comment id="12629267" author="athusoo" created="Mon, 8 Sep 2008 19:33:44 +0000"  >&lt;p&gt;completely agree on this. With a jdbc driver the front end integration would be much easier.&lt;/p&gt;</comment>
                            <comment id="12629269" author="athusoo" created="Mon, 8 Sep 2008 19:44:10 +0000"  >&lt;p&gt;Also I wanted to add that we have tried to structure the Driver code in such a way that we follow the execute/fetch paradgm that is followed by JDBC drivers - though admittedly the metadata part of jdbc is harder than the data part.&lt;/p&gt;

&lt;p&gt;Also Raghu was looking into creating a simple jdbc driver for hive. We should add that to the hive roadmap wiki.&lt;/p&gt;</comment>
                            <comment id="12629280" author="rsm" created="Mon, 8 Sep 2008 20:28:12 +0000"  >&lt;p&gt;I had already added it to the roadmap. Regarding the simple jdbc driver, I will submit a patch next week.&lt;/p&gt;</comment>
                            <comment id="12642581" author="hammer" created="Sat, 25 Oct 2008 00:57:39 +0000"  >&lt;p&gt;Raghu, any progress on the JDBC driver?&lt;/p&gt;</comment>
                            <comment id="12642860" author="rsm" created="Mon, 27 Oct 2008 06:34:39 +0000"  >&lt;p&gt;I had a preliminary set of classes. I didnt get a chance to finish working on them though. Michi has now taken those classes and I believe he has something working now. I&apos;ll let him post a patch.&lt;/p&gt;</comment>
                            <comment id="12642863" author="michim" created="Mon, 27 Oct 2008 06:47:48 +0000"  >&lt;p&gt;Added a JDBC driver for hive. Look at src/contrib/hive/ql/src/test/org/apache/hadoop/hive/ql/jdbc/TestHiveDriver.java for example.&lt;/p&gt;

&lt;p&gt;Next steps:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;provide a hive standalone server&lt;/li&gt;
	&lt;li&gt;integrate with hive metastore (e.g. support different types)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12642869" author="hammer" created="Mon, 27 Oct 2008 07:17:17 +0000"  >&lt;p&gt;Nice, Michi! Will poke at this tomorrow.&lt;/p&gt;</comment>
                            <comment id="12642983" author="namit" created="Mon, 27 Oct 2008 17:59:43 +0000"  >&lt;p&gt;Michi, did you consider having a client-server approach for the JDBC server ? There is nothing wrong with this approach - infact, this way, the server does not become a single point of failure. &lt;br/&gt;
The client does become thicker, which may be acceptable. I just wanted to know did you consider the pros-cons of that approach.&lt;/p&gt;</comment>
                            <comment id="12642994" author="namit" created="Mon, 27 Oct 2008 18:14:30 +0000"  >&lt;p&gt;The Driver API has changed - it is now integrated with the serde and returns a vector&amp;lt;string&amp;gt; instead of vector&amp;lt;vector&amp;lt;string&amp;gt;&amp;gt; wrongly.&lt;br/&gt;
That needs to be changed also.&lt;/p&gt;</comment>
                            <comment id="12643000" author="jsensarma" created="Mon, 27 Oct 2008 18:30:12 +0000"  >&lt;p&gt;how are we planning on picking up the hadoop and hive configuration file? (the cli picks them up through the classpath). the same concern applies to jar files (there&apos;s configuration in the cli shell script to set it up to include jars in auxlib).&lt;/p&gt;

&lt;p&gt;We will need a client-server model. the cli does not, for example, run on cygwin/windows and there are all manner of pathing issues that we would need to fix to make that work. within facebook - we won&apos;t even be able to access hdfs directly from windows agents that are outside the secure zone (only http ports are available i believe). i verified from Dhruba that this is the case in yahoo as well. so - we just can&apos;t run queries directly from windows machines without a server side that is within the secure zone.&lt;/p&gt;</comment>
                            <comment id="12643014" author="jsensarma" created="Mon, 27 Oct 2008 18:53:42 +0000"  >&lt;p&gt;ok - Ashish just walked us through a couple of scenarios:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;BI tool has server side. in this case the approach in this patch might work - but the concern about setting up classpaths and the suitability of running hadoop code setting classloaders and stuff on the same JVM as the BI server is suspect. At the minimum this has significant integration issues for each BI server.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;BI tool does not have a server side - only a client. I think this is a very common scenario and something which we should try to cover (since the whole premise of hadoop/hive is to avoid spending a lot of money - which is what BI tools with server side will require). In this case - the approach in this patch will be hard to make work because of firewalling issues that i had mentioned in the previous post (even if all the technical issues like hive treatment of windows paths are resolved).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;hopefully this captures the issues more accurately.&lt;/p&gt;</comment>
                            <comment id="12643036" author="prasadc" created="Mon, 27 Oct 2008 20:04:33 +0000"  >&lt;p&gt;There is already a MetaStore server (HiveMetaStore.java). It is a thrift service so I am not sure it would fit requirements for JDBC server. If it does, we should add JDBC functionality to this server. &lt;/p&gt;</comment>
                            <comment id="12643400" author="michim" created="Wed, 29 Oct 2008 03:09:22 +0000"  >&lt;p&gt;I talked about this with Ragho.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The next step is to separate client from the server.&lt;/li&gt;
	&lt;li&gt;I&apos;ll check if we can use thrift to implement JDBC server/client.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;--Michi&lt;/p&gt;</comment>
                            <comment id="12644812" author="namit" created="Mon, 3 Nov 2008 21:29:49 +0000"  >&lt;p&gt;Hi Michi, Any updates on this. If you want to meet to discuss in more detail, we can also meet&lt;/p&gt;</comment>
                            <comment id="12644829" author="rsm" created="Mon, 3 Nov 2008 22:20:23 +0000"  >&lt;p&gt;Michi and I were discussing this over the weekend. Here&apos;s our current thinking about the design. Michi, pls confirm.&lt;/p&gt;

&lt;p&gt;1. implement a thrift client/server for hive. for now, the interface consists only of execute and fetch_row. we were able to setup the framework with a thrift server and a java client which talks to the server. next step is to get the server to run the queries. &lt;br/&gt;
notes: we looked at the metastore code and thought it might be simpler to first implement a separate thrift client/server before merging it with the metastore. some installations might want to have separate instances of metastore and hive server. and, its easier to test a smaller interface where we understand the code. also, metastore code seems to have classes which arent being used at all and the scripts to start/stop metastore dont really work in non-facebook installations (need to file separate jiras for those).&lt;/p&gt;

&lt;p&gt;2. build a jdbc interface which makes calls to the generated java thrift client. we could also have python and perl dbi interfaces which can be make calls to the generated thrift client code in those languages. so, the thrift interface is a generic interface which is not specific to any particular standard (jdbc/dbi etc).&lt;/p&gt;

&lt;p&gt;3. the directory structure in the code would be as follows in src/contrib/hive. it follows a similar model to metastore.&lt;/p&gt;

&lt;p&gt;service/if/hive_service.thrift&lt;br/&gt;
service/include/&amp;lt;headers from thrift&amp;gt;&lt;br/&gt;
service/fb303/&amp;lt;scripts for service_ctrl to manage server&amp;gt;&lt;br/&gt;
service/src/gen-javabean/&amp;lt;generated java code&amp;gt;&lt;br/&gt;
service/src/gen-php/&amp;lt;generated php&amp;gt;&lt;br/&gt;
service/src/gen-py/&amp;lt;generated python&amp;gt;&lt;br/&gt;
service/src/gen-perl/&amp;lt;generated perl&amp;gt;&lt;br/&gt;
service/src/scripts/&amp;lt;ctrl scripts for server&amp;gt;&lt;br/&gt;
service/src/java/org/apache/hadoop/hive/service/HiveServer.java&lt;br/&gt;
service/src/java/org/apache/hadoop/hive/service/HiveClient.java&lt;br/&gt;
jdbc/src/java/org/apache/hadoop/hive/jdbc/&amp;lt;whatever is in current jdbc patch&amp;gt;&lt;br/&gt;
dbi/&amp;lt;perl dbi interface calling service/src/gen-perl&amp;gt;&lt;br/&gt;
cli/&amp;lt;changed to use HiveClient or HiveJdbc&amp;gt;&lt;/p&gt;

&lt;p&gt;4. next steps&lt;br/&gt;
a. get server to run queries and return results to client.&lt;br/&gt;
b. move ql/Driver.java to service since the actual running of the query is not really part of the query language.&lt;br/&gt;
c. change cli to use the service&lt;br/&gt;
d. verify which parts of the metastore interface are needed by jdbc and move/copy over parts to hive_service - i dont think it makes sense to do it the other way around i.e. put the hive service into metastore since metastore is not the right abstraction to actually run queries.&lt;br/&gt;
e. there is common thrift code in metastore and service. we should either move it to a seprate thrift directory or make metastore use stuff from service.&lt;/p&gt;

&lt;p&gt;It will be good to meet up to discuss them in more detail. I&apos;ll let Michi provide a patch for the hive server/client and jdbc wrappers for the hive client.&lt;/p&gt;</comment>
                            <comment id="12644835" author="prasadc" created="Mon, 3 Nov 2008 22:31:28 +0000"  >&lt;p&gt;Regarding unused files in metastore, these are the files that got carried over hive prototype which used file based metastore. We left them there in case some one wants to use file based metastore. So in a sense they are useful and there are tests.&lt;/p&gt;

&lt;p&gt;I think we should combine the servers now. It will be difficult and time consuming to merge them later. Advanced users can still have two installations of the same server but direct metadata calls to one server and data calls to another server. But the default case, there will be only one server and easier for maintenance.&lt;/p&gt;

&lt;p&gt;Only issue I see is that metastore code is independent of ql/cli code. So it might be better to build JDBC server on top of metastore server (ie extend metastore server) and import metastore thrift IDL into service thrift IDL. So the JDBC service would be a superset of metastore functionality.&lt;/p&gt;

&lt;p&gt;What do you guys think?&lt;/p&gt;</comment>
                            <comment id="12644837" author="michim" created="Mon, 3 Nov 2008 22:39:17 +0000"  >&lt;p&gt;Ragho: I confirm.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I should be able to finish implementing HiveServer.java/HiveClient.java by the end of this week (maybe by Sunday). As Ragho said, right now we have only 2 methods: void execute(String query) and list&amp;lt;String&amp;gt; fetch_row().&lt;/li&gt;
	&lt;li&gt;After that, I will modify the JDBC driver to use HiveClient.&lt;/li&gt;
	&lt;li&gt;Command line interface can use either HiveClient or JDBC driver.&lt;/li&gt;
	&lt;li&gt;I&apos;m usually available after 7 on tue-fri.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;--Michi&lt;/p&gt;</comment>
                            <comment id="12645110" author="athusoo" created="Tue, 4 Nov 2008 23:38:25 +0000"  >&lt;p&gt;How are you planning to implement the metadata calls. There is a lot of inheritance in the JDBC metadata calls and from what I understand, thrift does not support inheritance.&lt;/p&gt;

&lt;p&gt;Also, if you do go the thrift route, it may be better to share the server container code between the metastore and the JDBC driver, the apis I think should be independent and should be kept separate. While reorganizing the code, it may be worthwhile to put the server portion of it in common and then share it between the metastore and service..&lt;/p&gt;</comment>
                            <comment id="12645292" author="michim" created="Wed, 5 Nov 2008 18:55:03 +0000"  >&lt;p&gt;I was thinking the JDBC driver will be of type 4:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/JDBC_driver#Type_4_Driver_-_Native-Protocol_Driver&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/JDBC_driver#Type_4_Driver_-_Native-Protocol_Driver&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;which means there is a server &amp;lt;--&amp;gt; client api that is independent of JDBC, and JDBC driver uses the client api. &lt;/p&gt;

&lt;p&gt;We should meet up to make sure we are all on the same page. Ragho, can you set up a meeting?&lt;/p&gt;

&lt;p&gt;--Michi&lt;/p&gt;</comment>
                            <comment id="12646202" author="rsm" created="Mon, 10 Nov 2008 08:34:56 +0000"  >&lt;p&gt;I am not sure I understand what Ashish meant by &apos;inheritance in JDBC metadata calls&apos;. The plan is to include metastore.thrift in hive_service.thrift and then hive_service will just forward metadata calls to the metastore code. I guess with inheritance we wouldnt have to implement the forwarding functions. Is this what you mean Ashish?&lt;/p&gt;

&lt;p&gt;And yes, we should have a single implementation of the thrift server container for HiveServer and Metastore. JDBC would then be a wrapper on top of the thrift hive client.&lt;/p&gt;

&lt;p&gt;update: step 4a above has been completed - can now issue queries via HiveClient and retrieve results. HiveServer - a thrift server - actually runs the queries via ql/Driver.  I am attaching the patch with the code for the thrift server/client.&lt;/p&gt;

&lt;p&gt;We should meet up to figure out what the plan is for the JDBC client.&lt;/p&gt;</comment>
                            <comment id="12646323" author="athusoo" created="Mon, 10 Nov 2008 18:45:24 +0000"  >&lt;p&gt;Type 4 should work I guess.&lt;/p&gt;

&lt;p&gt;I guess if you use that then you can sidestep the inheritance stuff that I was alluding to. Basically my concern was that if the server APIs mimicked the javax.sql APIs then inheritance would be a problem.&lt;/p&gt;

&lt;p&gt;Can you guys come over tomorrow sometime in the afternoon?&lt;/p&gt;</comment>
                            <comment id="12646326" author="michim" created="Mon, 10 Nov 2008 18:59:40 +0000"  >&lt;p&gt;7pm?&lt;/p&gt;

&lt;p&gt;--Michi&lt;/p&gt;</comment>
                            <comment id="12646348" author="namit" created="Mon, 10 Nov 2008 20:13:47 +0000"  >&lt;p&gt;7pm is fine with me &lt;/p&gt;</comment>
                            <comment id="12646623" author="athusoo" created="Tue, 11 Nov 2008 18:47:39 +0000"  >&lt;p&gt;yes just come over at 7pm.&lt;/p&gt;

&lt;p&gt;Also hive mailing list have changed to&lt;/p&gt;

&lt;p&gt;hive-user@hadoop.apache.org&lt;br/&gt;
hive-dev@hadoop.apache.org&lt;br/&gt;
hive-commits@hadoop.apache.org&lt;/p&gt;

&lt;p&gt;So you may want to subscribe to those (this is because hive is in the process of becoming a subproject under hadoop).&lt;/p&gt;</comment>
                            <comment id="12652734" author="michim" created="Wed, 3 Dec 2008 10:40:22 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Attached hadoop-4101.3.patch&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;A temporary patch for jdbc support. You need to apply the patch from &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-73&quot; title=&quot;Thrift Server and Client for Hive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-73&quot;&gt;&lt;del&gt;HIVE-73&lt;/del&gt;&lt;/a&gt; before using this patch. See jdbc/src/test/org/apache/hadoop/hive/jdbc/TestHiveDriver.java for supported methods. &lt;/p&gt;</comment>
                            <comment id="12652937" author="rsm" created="Wed, 3 Dec 2008 20:00:31 +0000"  >&lt;p&gt;This patch doesnt seem to work. guess you need to upload a patch which adds the jdbc directory. also, can you generate a patch by running &apos;git diff --no-prefix&apos;. That will allow us to apply the patch with patch -p0.&lt;/p&gt;</comment>
                            <comment id="12652995" author="michim" created="Wed, 3 Dec 2008 22:04:51 +0000"  >&lt;p&gt;Previous patch was not working. Giving another try.&lt;/p&gt;</comment>
                            <comment id="12653387" author="appodictic" created="Thu, 4 Dec 2008 18:00:53 +0000"  >&lt;p&gt;A few people developing servers Thrift/JDBC need to modify the bin/hive script. &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-107&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;HIVE-107&lt;/a&gt; what you think of this idea and if it works for you.&lt;/p&gt;</comment>
                            <comment id="12658006" author="rsm" created="Fri, 19 Dec 2008 04:45:53 +0000"  >&lt;p&gt;Fixed url parsing. Also added standalone server option for testing.&lt;/p&gt;</comment>
                            <comment id="12660282" author="rsm" created="Fri, 2 Jan 2009 09:24:40 +0000"  >&lt;p&gt;I have added functionality to getByte, getBoolean, getDouble, getFloat, getInt, getLong, getObject, getShort, and changed getString to do a toString.&lt;/p&gt;

&lt;p&gt;Also added a single test to test getInt. Right now, anything other than select * uses MetaDataTypedColumnSetSerDe - so, the result schema has all columns as strings. Will add more tests to jdbc once we start using DynamicSerDe for all queries.&lt;/p&gt;</comment>
                            <comment id="12660774" author="athusoo" created="Mon, 5 Jan 2009 14:49:08 +0000"  >&lt;p&gt;A few questions:&lt;br/&gt;
1. What is the motivation for Driver.java:87 change?&lt;br/&gt;
2. In JdbcSessionState.java what are execString and fileName variables used for?&lt;br/&gt;
3. Shouldn&apos;t HiveResultSetMetadata.java be doing something?&lt;br/&gt;
4. In HiveResultSet.java:436 the getDouble function is missing TODO comment? Same is true for getFloat and getInt.&lt;br/&gt;
5. For the non implemented functions should we through a non implemented run time exception instead of just returning 0.&lt;/p&gt;

</comment>
                            <comment id="12660854" author="rsm" created="Mon, 5 Jan 2009 18:55:22 +0000"  >&lt;p&gt;&amp;gt; 1. What is the motivation for Driver.java:87 change?&lt;/p&gt;

&lt;p&gt;The reason was that there is currently no way to retrieve the result table name in the server. Also, the driver is always returning the result of queries, so, &apos;result&apos; seems to be a reasonable name for the table. Ideally, we should have a function which converts a schema DDL to a Schema object and we should be able to query the schema object. Right now we just pass the schema string as is to the DynamicSerDe.&lt;/p&gt;

&lt;p&gt;&amp;gt; 2. In JdbcSessionState.java what are execString and fileName variables used for?&lt;/p&gt;

&lt;p&gt;Right now JdbcSessionState is a dummy class. Its not being used for anything. The plan is to use it later on. I just copied over the class from Cli. I can make it an empty class. &lt;/p&gt;

&lt;p&gt;&amp;gt; 3. Shouldn&apos;t HiveResultSetMetadata.java be doing something?&lt;/p&gt;

&lt;p&gt;The plan was to stage the JDBC implementation. There are a bunch of auto-generated classes which will be used later on.&lt;/p&gt;

&lt;p&gt;&amp;gt; 4. In HiveResultSet.java:436 the getDouble function is missing TODO comment? Same is true for getFloat and getInt.&lt;/p&gt;

&lt;p&gt;The TODO comment is inside the function body. Isnt that enough?&lt;/p&gt;

&lt;p&gt;&amp;gt; 5. For the non implemented functions should we throw a non implemented run time exception instead of just returning 0. &lt;/p&gt;

&lt;p&gt;return 0 was auto-generated. I&apos;ll change them to throw SQLException instead.&lt;/p&gt;</comment>
                            <comment id="12660941" author="rsm" created="Mon, 5 Jan 2009 21:45:45 +0000"  >&lt;p&gt;Two changes:&lt;br/&gt;
1. made JdbcSessionState a dummy class&lt;br/&gt;
2. now throwing SQLException for unimplemented functions.&lt;/p&gt;</comment>
                            <comment id="12660983" author="athusoo" created="Mon, 5 Jan 2009 23:15:52 +0000"  >&lt;p&gt;+1.&lt;/p&gt;

&lt;p&gt;Looks good to me.&lt;/p&gt;</comment>
                            <comment id="12661019" author="dhruba" created="Tue, 6 Jan 2009 01:30:05 +0000"  >&lt;p&gt;I get compilation problems:&lt;/p&gt;

&lt;p&gt;core-compile:&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; Compiling 10 source files to /mnt/vol/devrs004.snc1/dhruba/commithive/build/jdbc/classes&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; /mnt/vol/devrs004.snc1/dhruba/commithive/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveConnection.java:452: unreported exception java.sql.SQLException; must be caught or declared to be thrown&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;     throw new SQLException(&quot;Method not supported&quot;);&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;     ^&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt; /mnt/vol/devrs004.snc1/dhruba/commithive/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveConnection.java:462: unreported exception java.sql.SQLException; must be caught or declared to be thrown&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;     throw new SQLException(&quot;Method not supported&quot;);&lt;/p&gt;</comment>
                            <comment id="12661028" author="rsm" created="Tue, 6 Jan 2009 01:48:13 +0000"  >&lt;p&gt;Oops. My method of generating diffs seems to be broken with git - this happened twice today &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; switching back to svn. Uploaded fixed patch.&lt;/p&gt;</comment>
                            <comment id="12661032" author="dhruba" created="Tue, 6 Jan 2009 02:07:16 +0000"  >&lt;p&gt;I just committed this. Thanks Raghu and Michi.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12408847">HIVE-73</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12471478">HIVE-1536</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12392852" name="hadoop-4101.1.patch" size="201766" author="michim" created="Mon, 27 Oct 2008 06:47:48 +0000"/>
                            <attachment id="12393621" name="hadoop-4101.2.patch" size="519780" author="rsm" created="Mon, 10 Nov 2008 08:34:56 +0000"/>
                            <attachment id="12395175" name="hadoop-4101.3.patch" size="9235" author="michim" created="Wed, 3 Dec 2008 10:40:21 +0000"/>
                            <attachment id="12395218" name="hadoop-4101.4.patch" size="204997" author="michim" created="Wed, 3 Dec 2008 22:04:51 +0000"/>
                            <attachment id="12396464" name="hive-48.5.patch" size="206879" author="rsm" created="Fri, 19 Dec 2008 04:45:53 +0000"/>
                            <attachment id="12397014" name="hive-48.6.patch" size="201279" author="rsm" created="Fri, 2 Jan 2009 09:24:40 +0000"/>
                            <attachment id="12397149" name="hive-48.7.patch" size="235595" author="rsm" created="Mon, 5 Jan 2009 21:45:45 +0000"/>
                            <attachment id="12397168" name="hive-48.8.patch" size="235589" author="rsm" created="Tue, 6 Jan 2009 01:48:13 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 8 Sep 2008 19:33:44 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73815</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0iukv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>108048</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>JDBC Driver</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                    </customfields>
    </item>
</channel>
</rss>
